2025-08-29 13:10:44.742087: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1756473044.943315      95 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1756473044.999793      95 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
create dataset:  cifar100
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Creating model..
, number of params: 27599012
SGD!!!
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Train: 0 [   0/390 (  0%)]  Loss:  4.619664 (4.6197)  Time: 3.485s,   36.73/s  (3.485s,   36.73/s)  LR: 1.000e-02  Data: 1.577 (1.577)
Train: 0 [  50/390 ( 13%)]  Loss:  4.561179 (4.5904)  Time: 0.749s,  170.94/s  (0.802s,  159.56/s)  LR: 1.000e-02  Data: 0.000 (0.031)
Train: 0 [ 100/390 ( 26%)]  Loss:  4.465921 (4.5489)  Time: 0.748s,  171.23/s  (0.775s,  165.06/s)  LR: 1.000e-02  Data: 0.000 (0.016)
Train: 0 [ 150/390 ( 39%)]  Loss:  4.394899 (4.5104)  Time: 0.750s,  170.75/s  (0.766s,  167.01/s)  LR: 1.000e-02  Data: 0.000 (0.011)
Train: 0 [ 200/390 ( 51%)]  Loss:  4.336814 (4.4757)  Time: 0.748s,  171.03/s  (0.764s,  167.58/s)  LR: 1.000e-02  Data: 0.000 (0.008)
Train: 0 [ 250/390 ( 64%)]  Loss:  4.319393 (4.4496)  Time: 0.748s,  171.10/s  (0.761s,  168.26/s)  LR: 1.000e-02  Data: 0.000 (0.007)
Train: 0 [ 300/390 ( 77%)]  Loss:  4.215168 (4.4161)  Time: 0.748s,  171.15/s  (0.759s,  168.73/s)  LR: 1.000e-02  Data: 0.000 (0.006)
Train: 0 [ 350/390 ( 90%)]  Loss:  4.158026 (4.3839)  Time: 0.751s,  170.46/s  (0.757s,  169.06/s)  LR: 1.000e-02  Data: 0.003 (0.005)
Train: 0 [ 389/390 (100%)]  Loss:  4.084802 (4.3507)  Time: 0.747s,  171.30/s  (0.756s,  169.27/s)  LR: 1.000e-02  Data: 0.000 (0.004)
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Test: [   0/78]  Time: 1.275 (1.275)  DataTime: 0.594 (0.594)  Loss:  3.7168 (3.7168)  Acc@1: 18.7500 (18.7500)  Acc@5: 46.8750 (46.8750)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.012)  Loss:  3.8633 (3.8328)  Acc@1: 14.0625 (13.0208)  Acc@5: 39.8438 (40.1501)
Test: [  78/78]  Time: 0.041 (0.269)  DataTime: 0.000 (0.008)  Loss:  3.9238 (3.8427)  Acc@1:  6.2500 (12.5900)  Acc@5: 43.7500 (39.6300)
tb
loss  0 : 4.5617625 0
loss  1 : 3.6714875 0
loss  2 : 3.923684375 0
loss  3 : 3.69060625 0
{'loss_different_0': 4.5617625, 'loss_different_1': 3.6714875, 'loss_different_2': 3.923684375, 'loss_different_3': 3.69060625}
Top1:  12.59
Saving model to ./output/train/20250829-131105-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_4/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_4_0.pt
Train: 1 [   0/390 (  0%)]  Loss:  4.369576 (4.3696)  Time: 1.125s,  113.80/s  (1.125s,  113.80/s)  LR: 9.999e-03  Data: 0.319 (0.319)
Train: 1 [  50/390 ( 13%)]  Loss:  4.008358 (4.1890)  Time: 0.748s,  171.11/s  (0.755s,  169.46/s)  LR: 9.999e-03  Data: 0.000 (0.007)
Train: 1 [ 100/390 ( 26%)]  Loss:  3.966896 (4.1149)  Time: 0.748s,  171.22/s  (0.752s,  170.29/s)  LR: 9.999e-03  Data: 0.000 (0.004)
Train: 1 [ 150/390 ( 39%)]  Loss:  4.010895 (4.0889)  Time: 0.748s,  171.10/s  (0.750s,  170.57/s)  LR: 9.999e-03  Data: 0.000 (0.002)
Train: 1 [ 200/390 ( 51%)]  Loss:  3.738237 (4.0188)  Time: 0.747s,  171.25/s  (0.750s,  170.70/s)  LR: 9.999e-03  Data: 0.000 (0.002)
Train: 1 [ 250/390 ( 64%)]  Loss:  4.074936 (4.0281)  Time: 0.748s,  171.20/s  (0.749s,  170.78/s)  LR: 9.999e-03  Data: 0.000 (0.002)
Train: 1 [ 300/390 ( 77%)]  Loss:  4.228341 (4.0567)  Time: 0.747s,  171.28/s  (0.749s,  170.84/s)  LR: 9.999e-03  Data: 0.000 (0.001)
Train: 1 [ 350/390 ( 90%)]  Loss:  3.687092 (4.0105)  Time: 0.750s,  170.65/s  (0.749s,  170.88/s)  LR: 9.999e-03  Data: 0.002 (0.001)
Train: 1 [ 389/390 (100%)]  Loss:  3.758960 (3.9826)  Time: 0.748s,  171.14/s  (0.749s,  170.91/s)  LR: 9.999e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.220 (1.220)  DataTime: 0.949 (0.949)  Loss:  3.1133 (3.1133)  Acc@1: 25.7812 (25.7812)  Acc@5: 57.8125 (57.8125)
Test: [  50/78]  Time: 0.259 (0.278)  DataTime: 0.000 (0.019)  Loss:  3.3320 (3.2454)  Acc@1: 19.5312 (21.1091)  Acc@5: 50.0000 (56.2347)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  3.4004 (3.2561)  Acc@1:  0.0000 (20.7000)  Acc@5: 56.2500 (55.9400)
tb
loss  0 : 4.36155625 1
loss  1 : 3.06909375 1
loss  2 : 3.522375 1
loss  3 : 3.122878125 1
{'loss_different_0': 4.36155625, 'loss_different_1': 3.06909375, 'loss_different_2': 3.522375, 'loss_different_3': 3.122878125}
Top1:  20.7
Train: 2 [   0/390 (  0%)]  Loss:  3.684150 (3.6841)  Time: 1.641s,   77.99/s  (1.641s,   77.99/s)  LR: 9.998e-03  Data: 0.892 (0.892)
Train: 2 [  50/390 ( 13%)]  Loss:  3.550110 (3.6171)  Time: 0.748s,  171.02/s  (0.770s,  166.19/s)  LR: 9.998e-03  Data: 0.000 (0.018)
Train: 2 [ 100/390 ( 26%)]  Loss:  3.634883 (3.6230)  Time: 0.748s,  171.11/s  (0.759s,  168.60/s)  LR: 9.998e-03  Data: 0.000 (0.009)
Train: 2 [ 150/390 ( 39%)]  Loss:  3.550316 (3.6049)  Time: 0.748s,  171.18/s  (0.756s,  169.42/s)  LR: 9.998e-03  Data: 0.000 (0.006)
Train: 2 [ 200/390 ( 51%)]  Loss:  3.710890 (3.6261)  Time: 0.748s,  171.23/s  (0.754s,  169.84/s)  LR: 9.998e-03  Data: 0.000 (0.005)
Train: 2 [ 250/390 ( 64%)]  Loss:  3.464976 (3.5992)  Time: 0.749s,  170.99/s  (0.752s,  170.10/s)  LR: 9.998e-03  Data: 0.000 (0.004)
Train: 2 [ 300/390 ( 77%)]  Loss:  3.757524 (3.6218)  Time: 0.748s,  171.13/s  (0.752s,  170.27/s)  LR: 9.998e-03  Data: 0.000 (0.003)
Train: 2 [ 350/390 ( 90%)]  Loss:  3.475945 (3.6036)  Time: 0.750s,  170.68/s  (0.751s,  170.39/s)  LR: 9.998e-03  Data: 0.003 (0.003)
Train: 2 [ 389/390 (100%)]  Loss:  3.679163 (3.6120)  Time: 0.747s,  171.27/s  (0.751s,  170.47/s)  LR: 9.998e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.266 (1.266)  DataTime: 0.984 (0.984)  Loss:  2.7559 (2.7559)  Acc@1: 30.4688 (30.4688)  Acc@5: 67.1875 (67.1875)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.020)  Loss:  2.9863 (2.8878)  Acc@1: 28.1250 (29.0288)  Acc@5: 60.9375 (64.5987)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.013)  Loss:  3.0449 (2.8931)  Acc@1: 31.2500 (28.4700)  Acc@5: 68.7500 (64.4400)
tb
loss  0 : 3.97669375 2
loss  1 : 2.72385 2
loss  2 : 3.249503125 2
loss  3 : 2.83354375 2
{'loss_different_0': 3.97669375, 'loss_different_1': 2.72385, 'loss_different_2': 3.249503125, 'loss_different_3': 2.83354375}
Top1:  28.47
Train: 3 [   0/390 (  0%)]  Loss:  4.225515 (4.2255)  Time: 1.590s,   80.49/s  (1.590s,   80.49/s)  LR: 9.994e-03  Data: 0.832 (0.832)
Train: 3 [  50/390 ( 13%)]  Loss:  3.329900 (3.7777)  Time: 0.748s,  171.01/s  (0.765s,  167.42/s)  LR: 9.994e-03  Data: 0.000 (0.017)
Train: 3 [ 100/390 ( 26%)]  Loss:  3.331811 (3.6291)  Time: 0.748s,  171.12/s  (0.756s,  169.23/s)  LR: 9.994e-03  Data: 0.000 (0.009)
Train: 3 [ 150/390 ( 39%)]  Loss:  3.517298 (3.6011)  Time: 0.748s,  171.10/s  (0.756s,  169.37/s)  LR: 9.994e-03  Data: 0.000 (0.006)
Train: 3 [ 200/390 ( 51%)]  Loss:  3.287768 (3.5385)  Time: 0.748s,  171.14/s  (0.754s,  169.80/s)  LR: 9.994e-03  Data: 0.000 (0.005)
Train: 3 [ 250/390 ( 64%)]  Loss:  4.008576 (3.6168)  Time: 0.747s,  171.24/s  (0.753s,  170.07/s)  LR: 9.994e-03  Data: 0.000 (0.004)
Train: 3 [ 300/390 ( 77%)]  Loss:  3.192623 (3.5562)  Time: 0.748s,  171.06/s  (0.752s,  170.24/s)  LR: 9.994e-03  Data: 0.000 (0.003)
Train: 3 [ 350/390 ( 90%)]  Loss:  3.311477 (3.5256)  Time: 0.750s,  170.59/s  (0.751s,  170.37/s)  LR: 9.994e-03  Data: 0.003 (0.003)
Train: 3 [ 389/390 (100%)]  Loss:  4.051761 (3.5841)  Time: 0.748s,  171.10/s  (0.751s,  170.45/s)  LR: 9.994e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.931 (0.931)  DataTime: 0.667 (0.667)  Loss:  2.4824 (2.4824)  Acc@1: 37.5000 (37.5000)  Acc@5: 77.3438 (77.3438)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.014)  Loss:  2.6875 (2.6219)  Acc@1: 34.3750 (33.3180)  Acc@5: 70.3125 (71.0172)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  2.7773 (2.6246)  Acc@1: 31.2500 (33.2900)  Acc@5: 50.0000 (71.0200)
tb
loss  0 : 3.571478125 3
loss  1 : 2.48165625 3
loss  2 : 2.986503125 3
loss  3 : 2.6248875 3
{'loss_different_0': 3.571478125, 'loss_different_1': 2.48165625, 'loss_different_2': 2.986503125, 'loss_different_3': 2.6248875}
Top1:  33.29
Train: 4 [   0/390 (  0%)]  Loss:  3.265740 (3.2657)  Time: 0.870s,  147.12/s  (0.870s,  147.12/s)  LR: 9.990e-03  Data: 0.104 (0.104)
Train: 4 [  50/390 ( 13%)]  Loss:  4.100854 (3.6833)  Time: 0.748s,  171.07/s  (0.751s,  170.51/s)  LR: 9.990e-03  Data: 0.000 (0.002)
Train: 4 [ 100/390 ( 26%)]  Loss:  3.125383 (3.4973)  Time: 0.748s,  171.13/s  (0.749s,  170.83/s)  LR: 9.990e-03  Data: 0.000 (0.001)
Train: 4 [ 150/390 ( 39%)]  Loss:  3.384242 (3.4691)  Time: 0.748s,  171.15/s  (0.749s,  170.94/s)  LR: 9.990e-03  Data: 0.000 (0.001)
Train: 4 [ 200/390 ( 51%)]  Loss:  3.121543 (3.3996)  Time: 0.748s,  171.16/s  (0.749s,  170.98/s)  LR: 9.990e-03  Data: 0.000 (0.001)
Train: 4 [ 250/390 ( 64%)]  Loss:  3.141644 (3.3566)  Time: 0.749s,  171.00/s  (0.748s,  171.01/s)  LR: 9.990e-03  Data: 0.000 (0.001)
Train: 4 [ 300/390 ( 77%)]  Loss:  3.165454 (3.3293)  Time: 0.748s,  171.19/s  (0.748s,  171.04/s)  LR: 9.990e-03  Data: 0.000 (0.001)
Train: 4 [ 350/390 ( 90%)]  Loss:  3.147251 (3.3065)  Time: 0.750s,  170.64/s  (0.748s,  171.05/s)  LR: 9.990e-03  Data: 0.003 (0.001)
Train: 4 [ 389/390 (100%)]  Loss:  3.190831 (3.2937)  Time: 0.747s,  171.25/s  (0.748s,  171.07/s)  LR: 9.990e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.282 (1.282)  DataTime: 1.016 (1.016)  Loss:  2.3242 (2.3242)  Acc@1: 36.7188 (36.7188)  Acc@5: 78.1250 (78.1250)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.020)  Loss:  2.5391 (2.4537)  Acc@1: 40.6250 (38.3732)  Acc@5: 71.8750 (74.4485)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.013)  Loss:  2.5391 (2.4573)  Acc@1: 31.2500 (38.2200)  Acc@5: 81.2500 (74.3300)
tb
loss  0 : 3.32306875 4
loss  1 : 2.328984375 4
loss  2 : 2.858378125 4
loss  3 : 2.50185625 4
{'loss_different_0': 3.32306875, 'loss_different_1': 2.328984375, 'loss_different_2': 2.858378125, 'loss_different_3': 2.50185625}
Top1:  38.22
Train: 5 [   0/390 (  0%)]  Loss:  4.124227 (4.1242)  Time: 1.639s,   78.11/s  (1.639s,   78.11/s)  LR: 9.985e-03  Data: 0.889 (0.889)
Train: 5 [  50/390 ( 13%)]  Loss:  3.053872 (3.5890)  Time: 0.748s,  171.22/s  (0.765s,  167.25/s)  LR: 9.985e-03  Data: 0.001 (0.018)
Train: 5 [ 100/390 ( 26%)]  Loss:  2.954022 (3.3774)  Time: 0.748s,  171.19/s  (0.759s,  168.56/s)  LR: 9.985e-03  Data: 0.000 (0.009)
Train: 5 [ 150/390 ( 39%)]  Loss:  3.043876 (3.2940)  Time: 0.748s,  171.14/s  (0.756s,  169.40/s)  LR: 9.985e-03  Data: 0.000 (0.006)
Train: 5 [ 200/390 ( 51%)]  Loss:  3.216492 (3.2785)  Time: 0.747s,  171.28/s  (0.754s,  169.84/s)  LR: 9.985e-03  Data: 0.000 (0.005)
Train: 5 [ 250/390 ( 64%)]  Loss:  3.406279 (3.2998)  Time: 0.748s,  171.23/s  (0.753s,  170.10/s)  LR: 9.985e-03  Data: 0.000 (0.004)
Train: 5 [ 300/390 ( 77%)]  Loss:  3.260484 (3.2942)  Time: 0.748s,  171.05/s  (0.752s,  170.27/s)  LR: 9.985e-03  Data: 0.000 (0.003)
Train: 5 [ 350/390 ( 90%)]  Loss:  2.960761 (3.2525)  Time: 0.750s,  170.68/s  (0.751s,  170.39/s)  LR: 9.985e-03  Data: 0.003 (0.003)
Train: 5 [ 389/390 (100%)]  Loss:  3.635583 (3.2951)  Time: 0.747s,  171.25/s  (0.751s,  170.48/s)  LR: 9.985e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.923 (0.923)  DataTime: 0.612 (0.612)  Loss:  2.1270 (2.1270)  Acc@1: 50.0000 (50.0000)  Acc@5: 81.2500 (81.2500)
Test: [  50/78]  Time: 0.259 (0.272)  DataTime: 0.000 (0.012)  Loss:  2.3301 (2.2671)  Acc@1: 41.4062 (43.7960)  Acc@5: 71.0938 (77.4969)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.008)  Loss:  2.4785 (2.2718)  Acc@1: 43.7500 (43.3000)  Acc@5: 68.7500 (77.5100)
tb
loss  0 : 3.1332 5
loss  1 : 2.171275 5
loss  2 : 2.6917375 5
loss  3 : 2.354646875 5
{'loss_different_0': 3.1332, 'loss_different_1': 2.171275, 'loss_different_2': 2.6917375, 'loss_different_3': 2.354646875}
Top1:  43.3
Train: 6 [   0/390 (  0%)]  Loss:  3.067694 (3.0677)  Time: 1.528s,   83.78/s  (1.528s,   83.78/s)  LR: 9.978e-03  Data: 0.755 (0.755)
Train: 6 [  50/390 ( 13%)]  Loss:  4.077540 (3.5726)  Time: 0.748s,  171.12/s  (0.763s,  167.71/s)  LR: 9.978e-03  Data: 0.000 (0.015)
Train: 6 [ 100/390 ( 26%)]  Loss:  3.053788 (3.3997)  Time: 0.748s,  171.13/s  (0.756s,  169.40/s)  LR: 9.978e-03  Data: 0.000 (0.008)
Train: 6 [ 150/390 ( 39%)]  Loss:  3.003070 (3.3005)  Time: 0.748s,  171.13/s  (0.753s,  169.96/s)  LR: 9.978e-03  Data: 0.000 (0.005)
Train: 6 [ 200/390 ( 51%)]  Loss:  3.045191 (3.2495)  Time: 0.748s,  171.19/s  (0.752s,  170.26/s)  LR: 9.978e-03  Data: 0.000 (0.004)
Train: 6 [ 250/390 ( 64%)]  Loss:  3.106205 (3.2256)  Time: 0.749s,  171.00/s  (0.752s,  170.14/s)  LR: 9.978e-03  Data: 0.000 (0.003)
Train: 6 [ 300/390 ( 77%)]  Loss:  4.007449 (3.3373)  Time: 0.748s,  171.03/s  (0.752s,  170.31/s)  LR: 9.978e-03  Data: 0.000 (0.003)
Train: 6 [ 350/390 ( 90%)]  Loss:  3.574564 (3.3669)  Time: 0.751s,  170.54/s  (0.751s,  170.43/s)  LR: 9.978e-03  Data: 0.003 (0.003)
Train: 6 [ 389/390 (100%)]  Loss:  3.429667 (3.3739)  Time: 0.747s,  171.34/s  (0.751s,  170.50/s)  LR: 9.978e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.207 (1.207)  DataTime: 0.910 (0.910)  Loss:  2.0996 (2.0996)  Acc@1: 46.0938 (46.0938)  Acc@5: 82.0312 (82.0312)
Test: [  50/78]  Time: 0.259 (0.278)  DataTime: 0.000 (0.018)  Loss:  2.1816 (2.1497)  Acc@1: 47.6562 (46.1397)  Acc@5: 75.0000 (79.5190)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  2.4453 (2.1528)  Acc@1: 43.7500 (45.8800)  Acc@5: 68.7500 (79.7300)
tb
loss  0 : 2.9278 6
loss  1 : 2.088075 6
loss  2 : 2.554671875 6
loss  3 : 2.292753125 6
{'loss_different_0': 2.9278, 'loss_different_1': 2.088075, 'loss_different_2': 2.554671875, 'loss_different_3': 2.292753125}
Top1:  45.88
Train: 7 [   0/390 (  0%)]  Loss:  3.207685 (3.2077)  Time: 1.644s,   77.86/s  (1.644s,   77.86/s)  LR: 9.970e-03  Data: 0.894 (0.894)
Train: 7 [  50/390 ( 13%)]  Loss:  2.906043 (3.0569)  Time: 0.748s,  171.06/s  (0.766s,  167.17/s)  LR: 9.970e-03  Data: 0.000 (0.018)
Train: 7 [ 100/390 ( 26%)]  Loss:  2.977349 (3.0304)  Time: 0.748s,  171.13/s  (0.757s,  169.08/s)  LR: 9.970e-03  Data: 0.000 (0.009)
Train: 7 [ 150/390 ( 39%)]  Loss:  2.887620 (2.9947)  Time: 0.748s,  171.13/s  (0.754s,  169.75/s)  LR: 9.970e-03  Data: 0.000 (0.006)
Train: 7 [ 200/390 ( 51%)]  Loss:  2.737646 (2.9433)  Time: 0.748s,  171.11/s  (0.753s,  170.08/s)  LR: 9.970e-03  Data: 0.000 (0.005)
Train: 7 [ 250/390 ( 64%)]  Loss:  2.803517 (2.9200)  Time: 0.748s,  171.18/s  (0.752s,  170.29/s)  LR: 9.970e-03  Data: 0.000 (0.004)
Train: 7 [ 300/390 ( 77%)]  Loss:  2.746965 (2.8953)  Time: 0.747s,  171.25/s  (0.751s,  170.42/s)  LR: 9.970e-03  Data: 0.000 (0.003)
Train: 7 [ 350/390 ( 90%)]  Loss:  2.642375 (2.8636)  Time: 0.750s,  170.64/s  (0.751s,  170.52/s)  LR: 9.970e-03  Data: 0.003 (0.003)
Train: 7 [ 389/390 (100%)]  Loss:  4.046134 (2.9950)  Time: 0.748s,  171.10/s  (0.750s,  170.58/s)  LR: 9.970e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.154 (1.154)  DataTime: 0.879 (0.879)  Loss:  1.9170 (1.9170)  Acc@1: 53.9062 (53.9062)  Acc@5: 84.3750 (84.3750)
Test: [  50/78]  Time: 0.258 (0.277)  DataTime: 0.000 (0.018)  Loss:  2.1562 (2.0334)  Acc@1: 44.5312 (48.1464)  Acc@5: 77.3438 (81.1275)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  2.5352 (2.0430)  Acc@1: 25.0000 (47.6800)  Acc@5: 62.5000 (81.1400)
tb
loss  0 : 2.722290625 7
loss  1 : 2.04529375 7
loss  2 : 2.43233125 7
loss  3 : 2.2198125 7
{'loss_different_0': 2.722290625, 'loss_different_1': 2.04529375, 'loss_different_2': 2.43233125, 'loss_different_3': 2.2198125}
Top1:  47.68
Train: 8 [   0/390 (  0%)]  Loss:  2.783748 (2.7837)  Time: 1.177s,  108.74/s  (1.177s,  108.74/s)  LR: 9.961e-03  Data: 0.410 (0.410)
Train: 8 [  50/390 ( 13%)]  Loss:  2.773685 (2.7787)  Time: 0.749s,  170.90/s  (0.757s,  169.19/s)  LR: 9.961e-03  Data: 0.000 (0.008)
Train: 8 [ 100/390 ( 26%)]  Loss:  2.721419 (2.7596)  Time: 0.748s,  171.07/s  (0.752s,  170.14/s)  LR: 9.961e-03  Data: 0.000 (0.004)
Train: 8 [ 150/390 ( 39%)]  Loss:  2.759830 (2.7597)  Time: 0.747s,  171.37/s  (0.752s,  170.10/s)  LR: 9.961e-03  Data: 0.000 (0.003)
Train: 8 [ 200/390 ( 51%)]  Loss:  2.549844 (2.7177)  Time: 0.748s,  171.14/s  (0.751s,  170.35/s)  LR: 9.961e-03  Data: 0.000 (0.002)
Train: 8 [ 250/390 ( 64%)]  Loss:  2.704875 (2.7156)  Time: 0.749s,  170.98/s  (0.751s,  170.49/s)  LR: 9.961e-03  Data: 0.000 (0.002)
Train: 8 [ 300/390 ( 77%)]  Loss:  2.675738 (2.7099)  Time: 0.748s,  171.08/s  (0.750s,  170.60/s)  LR: 9.961e-03  Data: 0.000 (0.002)
Train: 8 [ 350/390 ( 90%)]  Loss:  2.752672 (2.7152)  Time: 0.750s,  170.74/s  (0.750s,  170.67/s)  LR: 9.961e-03  Data: 0.003 (0.002)
Train: 8 [ 389/390 (100%)]  Loss:  2.932906 (2.7394)  Time: 0.748s,  171.18/s  (0.750s,  170.72/s)  LR: 9.961e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.205 (1.205)  DataTime: 0.916 (0.916)  Loss:  1.8721 (1.8721)  Acc@1: 54.6875 (54.6875)  Acc@5: 82.0312 (82.0312)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.018)  Loss:  1.9873 (1.9499)  Acc@1: 50.0000 (50.4749)  Acc@5: 79.6875 (81.8781)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  2.2109 (1.9527)  Acc@1: 43.7500 (50.3000)  Acc@5: 75.0000 (82.0300)
tb
loss  0 : 2.664384375 8
loss  1 : 1.947634375 8
loss  2 : 2.43716875 8
loss  3 : 2.150778125 8
{'loss_different_0': 2.664384375, 'loss_different_1': 1.947634375, 'loss_different_2': 2.43716875, 'loss_different_3': 2.150778125}
Top1:  50.3
Train: 9 [   0/390 (  0%)]  Loss:  3.714505 (3.7145)  Time: 1.601s,   79.93/s  (1.601s,   79.93/s)  LR: 9.950e-03  Data: 0.848 (0.848)
Train: 9 [  50/390 ( 13%)]  Loss:  2.558132 (3.1363)  Time: 0.748s,  171.08/s  (0.765s,  167.43/s)  LR: 9.950e-03  Data: 0.000 (0.017)
Train: 9 [ 100/390 ( 26%)]  Loss:  2.586691 (2.9531)  Time: 0.748s,  171.18/s  (0.756s,  169.26/s)  LR: 9.950e-03  Data: 0.000 (0.009)
Train: 9 [ 150/390 ( 39%)]  Loss:  4.036095 (3.2239)  Time: 0.748s,  171.17/s  (0.753s,  169.89/s)  LR: 9.950e-03  Data: 0.000 (0.006)
Train: 9 [ 200/390 ( 51%)]  Loss:  3.628231 (3.3047)  Time: 0.748s,  171.12/s  (0.752s,  170.20/s)  LR: 9.950e-03  Data: 0.000 (0.005)
Train: 9 [ 250/390 ( 64%)]  Loss:  2.631063 (3.1925)  Time: 0.748s,  171.17/s  (0.751s,  170.39/s)  LR: 9.950e-03  Data: 0.000 (0.004)
Train: 9 [ 300/390 ( 77%)]  Loss:  3.410138 (3.2236)  Time: 0.748s,  171.04/s  (0.752s,  170.26/s)  LR: 9.950e-03  Data: 0.000 (0.003)
Train: 9 [ 350/390 ( 90%)]  Loss:  2.678103 (3.1554)  Time: 0.750s,  170.60/s  (0.751s,  170.39/s)  LR: 9.950e-03  Data: 0.003 (0.003)
Train: 9 [ 389/390 (100%)]  Loss:  2.419112 (3.0736)  Time: 0.748s,  171.14/s  (0.751s,  170.47/s)  LR: 9.950e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.170 (1.170)  DataTime: 0.807 (0.807)  Loss:  1.8350 (1.8350)  Acc@1: 50.7812 (50.7812)  Acc@5: 85.1562 (85.1562)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.016)  Loss:  1.9160 (1.8757)  Acc@1: 49.2188 (53.7071)  Acc@5: 83.5938 (83.9308)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.011)  Loss:  2.2656 (1.8778)  Acc@1: 43.7500 (53.9000)  Acc@5: 75.0000 (84.0100)
tb
loss  0 : 2.58171875 9
loss  1 : 1.8745125 9
loss  2 : 2.283590625 9
loss  3 : 2.0909375 9
{'loss_different_0': 2.58171875, 'loss_different_1': 1.8745125, 'loss_different_2': 2.283590625, 'loss_different_3': 2.0909375}
Top1:  53.9
Train: 10 [   0/390 (  0%)]  Loss:  2.562003 (2.5620)  Time: 1.196s,  107.03/s  (1.196s,  107.03/s)  LR: 9.938e-03  Data: 0.364 (0.364)
Train: 10 [  50/390 ( 13%)]  Loss:  2.570103 (2.5661)  Time: 0.748s,  171.13/s  (0.756s,  169.20/s)  LR: 9.938e-03  Data: 0.000 (0.008)
Train: 10 [ 100/390 ( 26%)]  Loss:  2.630959 (2.5877)  Time: 0.749s,  170.98/s  (0.752s,  170.17/s)  LR: 9.938e-03  Data: 0.000 (0.004)
Train: 10 [ 150/390 ( 39%)]  Loss:  2.583168 (2.5866)  Time: 0.748s,  171.10/s  (0.751s,  170.51/s)  LR: 9.938e-03  Data: 0.000 (0.003)
Train: 10 [ 200/390 ( 51%)]  Loss:  2.577524 (2.5848)  Time: 0.747s,  171.25/s  (0.750s,  170.67/s)  LR: 9.938e-03  Data: 0.000 (0.002)
Train: 10 [ 250/390 ( 64%)]  Loss:  2.507740 (2.5719)  Time: 0.748s,  171.20/s  (0.750s,  170.77/s)  LR: 9.938e-03  Data: 0.000 (0.002)
Train: 10 [ 300/390 ( 77%)]  Loss:  2.394139 (2.5465)  Time: 0.749s,  170.99/s  (0.749s,  170.84/s)  LR: 9.938e-03  Data: 0.000 (0.002)
Train: 10 [ 350/390 ( 90%)]  Loss:  2.826262 (2.5815)  Time: 0.750s,  170.68/s  (0.749s,  170.89/s)  LR: 9.938e-03  Data: 0.003 (0.001)
Train: 10 [ 389/390 (100%)]  Loss:  2.637165 (2.5877)  Time: 0.748s,  171.20/s  (0.749s,  170.92/s)  LR: 9.938e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.249 (1.249)  DataTime: 0.979 (0.979)  Loss:  1.7520 (1.7520)  Acc@1: 63.2812 (63.2812)  Acc@5: 85.9375 (85.9375)
Test: [  50/78]  Time: 0.259 (0.278)  DataTime: 0.000 (0.020)  Loss:  1.8438 (1.8024)  Acc@1: 53.9062 (53.9216)  Acc@5: 86.7188 (85.3554)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.013)  Loss:  2.1328 (1.8113)  Acc@1: 31.2500 (54.0700)  Acc@5: 81.2500 (85.3600)
tb
loss  0 : 2.476371875 10
loss  1 : 1.866765625 10
loss  2 : 2.17203125 10
loss  3 : 2.0617625 10
{'loss_different_0': 2.476371875, 'loss_different_1': 1.866765625, 'loss_different_2': 2.17203125, 'loss_different_3': 2.0617625}
Top1:  54.07
Saving model to ./output/train/20250829-131105-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_4/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_4_10.pt
Train: 11 [   0/390 (  0%)]  Loss:  2.569355 (2.5694)  Time: 1.604s,   79.81/s  (1.604s,   79.81/s)  LR: 9.926e-03  Data: 0.854 (0.854)
Train: 11 [  50/390 ( 13%)]  Loss:  2.489590 (2.5295)  Time: 0.748s,  171.09/s  (0.765s,  167.41/s)  LR: 9.926e-03  Data: 0.000 (0.017)
Train: 11 [ 100/390 ( 26%)]  Loss:  3.000384 (2.6864)  Time: 0.748s,  171.13/s  (0.756s,  169.24/s)  LR: 9.926e-03  Data: 0.000 (0.009)
Train: 11 [ 150/390 ( 39%)]  Loss:  2.706339 (2.6914)  Time: 0.748s,  171.13/s  (0.753s,  169.88/s)  LR: 9.926e-03  Data: 0.000 (0.006)
Train: 11 [ 200/390 ( 51%)]  Loss:  2.939948 (2.7411)  Time: 0.748s,  171.11/s  (0.753s,  169.92/s)  LR: 9.926e-03  Data: 0.000 (0.005)
Train: 11 [ 250/390 ( 64%)]  Loss:  2.650604 (2.7260)  Time: 0.748s,  171.13/s  (0.752s,  170.17/s)  LR: 9.926e-03  Data: 0.000 (0.004)
Train: 11 [ 300/390 ( 77%)]  Loss:  3.241741 (2.7997)  Time: 0.748s,  171.16/s  (0.751s,  170.34/s)  LR: 9.926e-03  Data: 0.000 (0.003)
Train: 11 [ 350/390 ( 90%)]  Loss:  2.585479 (2.7729)  Time: 0.750s,  170.63/s  (0.751s,  170.46/s)  LR: 9.926e-03  Data: 0.003 (0.003)
Train: 11 [ 389/390 (100%)]  Loss:  2.679075 (2.7625)  Time: 0.747s,  171.37/s  (0.751s,  170.54/s)  LR: 9.926e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.988 (0.988)  DataTime: 0.720 (0.720)  Loss:  1.6699 (1.6699)  Acc@1: 58.5938 (58.5938)  Acc@5: 87.5000 (87.5000)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.015)  Loss:  1.7832 (1.7458)  Acc@1: 57.0312 (56.7249)  Acc@5: 86.7188 (85.9375)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.010)  Loss:  1.8848 (1.7505)  Acc@1: 50.0000 (56.4800)  Acc@5: 75.0000 (85.9500)
tb
loss  0 : 2.402203125 11
loss  1 : 1.787553125 11
loss  2 : 2.131021875 11
loss  3 : 2.005296875 11
{'loss_different_0': 2.402203125, 'loss_different_1': 1.787553125, 'loss_different_2': 2.131021875, 'loss_different_3': 2.005296875}
Top1:  56.48
Train: 12 [   0/390 (  0%)]  Loss:  2.762220 (2.7622)  Time: 1.598s,   80.12/s  (1.598s,   80.12/s)  LR: 9.911e-03  Data: 0.848 (0.848)
Train: 12 [  50/390 ( 13%)]  Loss:  3.847472 (3.3048)  Time: 0.748s,  171.11/s  (0.765s,  167.40/s)  LR: 9.911e-03  Data: 0.000 (0.017)
Train: 12 [ 100/390 ( 26%)]  Loss:  2.840666 (3.1501)  Time: 0.748s,  171.10/s  (0.756s,  169.23/s)  LR: 9.911e-03  Data: 0.000 (0.009)
Train: 12 [ 150/390 ( 39%)]  Loss:  2.363071 (2.9534)  Time: 0.747s,  171.24/s  (0.754s,  169.85/s)  LR: 9.911e-03  Data: 0.000 (0.006)
Train: 12 [ 200/390 ( 51%)]  Loss:  2.400829 (2.8429)  Time: 0.748s,  171.14/s  (0.752s,  170.18/s)  LR: 9.911e-03  Data: 0.000 (0.005)
Train: 12 [ 250/390 ( 64%)]  Loss:  2.457917 (2.7787)  Time: 0.749s,  170.95/s  (0.751s,  170.36/s)  LR: 9.911e-03  Data: 0.000 (0.004)
Train: 12 [ 300/390 ( 77%)]  Loss:  4.055416 (2.9611)  Time: 0.748s,  171.21/s  (0.751s,  170.49/s)  LR: 9.911e-03  Data: 0.000 (0.003)
Train: 12 [ 350/390 ( 90%)]  Loss:  2.510158 (2.9047)  Time: 0.749s,  170.83/s  (0.751s,  170.37/s)  LR: 9.911e-03  Data: 0.003 (0.003)
Train: 12 [ 389/390 (100%)]  Loss:  2.369215 (2.8452)  Time: 0.748s,  171.03/s  (0.751s,  170.45/s)  LR: 9.911e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.222 (1.222)  DataTime: 0.952 (0.952)  Loss:  1.6035 (1.6035)  Acc@1: 63.2812 (63.2812)  Acc@5: 83.5938 (83.5938)
Test: [  50/78]  Time: 0.259 (0.278)  DataTime: 0.000 (0.019)  Loss:  1.7520 (1.7081)  Acc@1: 57.0312 (57.3223)  Acc@5: 84.3750 (86.0141)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.8662 (1.7106)  Acc@1: 56.2500 (57.4200)  Acc@5: 75.0000 (86.1600)
tb
loss  0 : 2.37975 12
loss  1 : 1.7571625 12
loss  2 : 2.12653125 12
loss  3 : 1.94684375 12
{'loss_different_0': 2.37975, 'loss_different_1': 1.7571625, 'loss_different_2': 2.12653125, 'loss_different_3': 1.94684375}
Top1:  57.42
Train: 13 [   0/390 (  0%)]  Loss:  3.100652 (3.1007)  Time: 1.585s,   80.77/s  (1.585s,   80.77/s)  LR: 9.896e-03  Data: 0.829 (0.829)
Train: 13 [  50/390 ( 13%)]  Loss:  2.440111 (2.7704)  Time: 0.748s,  171.08/s  (0.764s,  167.48/s)  LR: 9.896e-03  Data: 0.000 (0.017)
Train: 13 [ 100/390 ( 26%)]  Loss:  2.436116 (2.6590)  Time: 0.748s,  171.01/s  (0.756s,  169.26/s)  LR: 9.896e-03  Data: 0.000 (0.009)
Train: 13 [ 150/390 ( 39%)]  Loss:  3.357470 (2.8336)  Time: 0.749s,  170.99/s  (0.754s,  169.87/s)  LR: 9.896e-03  Data: 0.000 (0.006)
Train: 13 [ 200/390 ( 51%)]  Loss:  3.707158 (3.0083)  Time: 0.747s,  171.24/s  (0.752s,  170.18/s)  LR: 9.896e-03  Data: 0.000 (0.004)
Train: 13 [ 250/390 ( 64%)]  Loss:  2.456208 (2.9163)  Time: 0.748s,  171.11/s  (0.751s,  170.37/s)  LR: 9.896e-03  Data: 0.000 (0.004)
Train: 13 [ 300/390 ( 77%)]  Loss:  2.224047 (2.8174)  Time: 0.748s,  171.06/s  (0.751s,  170.49/s)  LR: 9.896e-03  Data: 0.000 (0.003)
Train: 13 [ 350/390 ( 90%)]  Loss:  2.451954 (2.7717)  Time: 0.750s,  170.68/s  (0.750s,  170.58/s)  LR: 9.896e-03  Data: 0.003 (0.003)
Train: 13 [ 389/390 (100%)]  Loss:  2.227294 (2.7112)  Time: 0.748s,  171.09/s  (0.750s,  170.65/s)  LR: 9.896e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.127 (1.127)  DataTime: 0.824 (0.824)  Loss:  1.4912 (1.4912)  Acc@1: 64.0625 (64.0625)  Acc@5: 87.5000 (87.5000)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.017)  Loss:  1.8184 (1.6548)  Acc@1: 57.8125 (58.0270)  Acc@5: 82.8125 (86.6728)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.7520 (1.6640)  Acc@1: 56.2500 (57.6600)  Acc@5: 81.2500 (86.7900)
tb
loss  0 : 2.307646875 13
loss  1 : 1.756334375 13
loss  2 : 2.046815625 13
loss  3 : 1.9122109375 13
{'loss_different_0': 2.307646875, 'loss_different_1': 1.756334375, 'loss_different_2': 2.046815625, 'loss_different_3': 1.9122109375}
Top1:  57.66
Train: 14 [   0/390 (  0%)]  Loss:  2.308774 (2.3088)  Time: 1.350s,   94.79/s  (1.350s,   94.79/s)  LR: 9.880e-03  Data: 0.571 (0.571)
Train: 14 [  50/390 ( 13%)]  Loss:  3.701455 (3.0051)  Time: 0.748s,  171.10/s  (0.760s,  168.49/s)  LR: 9.880e-03  Data: 0.000 (0.012)
Train: 14 [ 100/390 ( 26%)]  Loss:  2.452622 (2.8210)  Time: 0.747s,  171.32/s  (0.754s,  169.81/s)  LR: 9.880e-03  Data: 0.000 (0.006)
Train: 14 [ 150/390 ( 39%)]  Loss:  2.304017 (2.6917)  Time: 0.747s,  171.24/s  (0.754s,  169.76/s)  LR: 9.880e-03  Data: 0.000 (0.004)
Train: 14 [ 200/390 ( 51%)]  Loss:  2.317090 (2.6168)  Time: 0.747s,  171.35/s  (0.752s,  170.12/s)  LR: 9.880e-03  Data: 0.000 (0.003)
Train: 14 [ 250/390 ( 64%)]  Loss:  2.804485 (2.6481)  Time: 0.748s,  171.09/s  (0.752s,  170.32/s)  LR: 9.880e-03  Data: 0.001 (0.003)
Train: 14 [ 300/390 ( 77%)]  Loss:  2.207811 (2.5852)  Time: 0.748s,  171.03/s  (0.751s,  170.46/s)  LR: 9.880e-03  Data: 0.000 (0.002)
Train: 14 [ 350/390 ( 90%)]  Loss:  2.333773 (2.5538)  Time: 0.750s,  170.62/s  (0.750s,  170.56/s)  LR: 9.880e-03  Data: 0.003 (0.002)
Train: 14 [ 389/390 (100%)]  Loss:  3.678979 (2.6788)  Time: 0.747s,  171.30/s  (0.750s,  170.63/s)  LR: 9.880e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.382 (1.382)  DataTime: 1.111 (1.111)  Loss:  1.5322 (1.5322)  Acc@1: 64.8438 (64.8438)  Acc@5: 87.5000 (87.5000)
Test: [  50/78]  Time: 0.259 (0.281)  DataTime: 0.000 (0.022)  Loss:  1.7656 (1.6355)  Acc@1: 59.3750 (59.1759)  Acc@5: 84.3750 (87.3315)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.014)  Loss:  1.8438 (1.6340)  Acc@1: 50.0000 (59.2500)  Acc@5: 81.2500 (87.2100)
tb
loss  0 : 2.269871875 14
loss  1 : 1.738715625 14
loss  2 : 1.991446875 14
loss  3 : 1.917859375 14
{'loss_different_0': 2.269871875, 'loss_different_1': 1.738715625, 'loss_different_2': 1.991446875, 'loss_different_3': 1.917859375}
Top1:  59.25
Train: 15 [   0/390 (  0%)]  Loss:  2.495605 (2.4956)  Time: 1.551s,   82.52/s  (1.551s,   82.52/s)  LR: 9.862e-03  Data: 0.789 (0.789)
Train: 15 [  50/390 ( 13%)]  Loss:  2.496905 (2.4963)  Time: 0.748s,  171.16/s  (0.763s,  167.65/s)  LR: 9.862e-03  Data: 0.000 (0.016)
Train: 15 [ 100/390 ( 26%)]  Loss:  2.369695 (2.4541)  Time: 0.748s,  171.02/s  (0.756s,  169.39/s)  LR: 9.862e-03  Data: 0.000 (0.008)
Train: 15 [ 150/390 ( 39%)]  Loss:  2.654732 (2.5042)  Time: 0.749s,  170.97/s  (0.753s,  169.97/s)  LR: 9.862e-03  Data: 0.000 (0.006)
Train: 15 [ 200/390 ( 51%)]  Loss:  2.221215 (2.4476)  Time: 0.748s,  171.17/s  (0.752s,  170.27/s)  LR: 9.862e-03  Data: 0.000 (0.004)
Train: 15 [ 250/390 ( 64%)]  Loss:  2.352860 (2.4318)  Time: 0.748s,  171.19/s  (0.751s,  170.44/s)  LR: 9.862e-03  Data: 0.000 (0.004)
Train: 15 [ 300/390 ( 77%)]  Loss:  3.217782 (2.5441)  Time: 0.749s,  171.01/s  (0.750s,  170.56/s)  LR: 9.862e-03  Data: 0.000 (0.003)
Train: 15 [ 350/390 ( 90%)]  Loss:  2.357260 (2.5208)  Time: 0.750s,  170.59/s  (0.750s,  170.65/s)  LR: 9.862e-03  Data: 0.003 (0.003)
Train: 15 [ 389/390 (100%)]  Loss:  3.665543 (2.6480)  Time: 0.747s,  171.25/s  (0.750s,  170.71/s)  LR: 9.862e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.354 (1.354)  DataTime: 1.079 (1.079)  Loss:  1.5449 (1.5449)  Acc@1: 64.8438 (64.8438)  Acc@5: 84.3750 (84.3750)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.022)  Loss:  1.7100 (1.6120)  Acc@1: 58.5938 (60.7690)  Acc@5: 84.3750 (88.1281)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.014)  Loss:  1.7744 (1.6155)  Acc@1: 62.5000 (60.6900)  Acc@5: 75.0000 (87.8900)
tb
loss  0 : 2.2691 15
loss  1 : 1.690784375 15
loss  2 : 2.02260625 15
loss  3 : 1.86476875 15
{'loss_different_0': 2.2691, 'loss_different_1': 1.690784375, 'loss_different_2': 2.02260625, 'loss_different_3': 1.86476875}
Top1:  60.69
Train: 16 [   0/390 (  0%)]  Loss:  2.242071 (2.2421)  Time: 1.620s,   79.03/s  (1.620s,   79.03/s)  LR: 9.843e-03  Data: 0.870 (0.870)
Train: 16 [  50/390 ( 13%)]  Loss:  2.204340 (2.2232)  Time: 0.748s,  171.16/s  (0.769s,  166.36/s)  LR: 9.843e-03  Data: 0.000 (0.017)
Train: 16 [ 100/390 ( 26%)]  Loss:  2.120566 (2.1890)  Time: 0.747s,  171.31/s  (0.759s,  168.71/s)  LR: 9.843e-03  Data: 0.000 (0.009)
Train: 16 [ 150/390 ( 39%)]  Loss:  2.333974 (2.2252)  Time: 0.748s,  171.18/s  (0.755s,  169.51/s)  LR: 9.843e-03  Data: 0.000 (0.006)
Train: 16 [ 200/390 ( 51%)]  Loss:  3.948395 (2.5699)  Time: 0.747s,  171.34/s  (0.753s,  169.92/s)  LR: 9.843e-03  Data: 0.000 (0.005)
Train: 16 [ 250/390 ( 64%)]  Loss:  2.594380 (2.5740)  Time: 0.747s,  171.28/s  (0.752s,  170.17/s)  LR: 9.843e-03  Data: 0.000 (0.004)
Train: 16 [ 300/390 ( 77%)]  Loss:  3.571831 (2.7165)  Time: 0.748s,  171.20/s  (0.751s,  170.33/s)  LR: 9.843e-03  Data: 0.000 (0.003)
Train: 16 [ 350/390 ( 90%)]  Loss:  2.300040 (2.6644)  Time: 0.750s,  170.56/s  (0.751s,  170.45/s)  LR: 9.843e-03  Data: 0.003 (0.003)
Train: 16 [ 389/390 (100%)]  Loss:  2.262601 (2.6198)  Time: 0.747s,  171.26/s  (0.751s,  170.53/s)  LR: 9.843e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.226 (1.226)  DataTime: 0.923 (0.923)  Loss:  1.4160 (1.4160)  Acc@1: 67.1875 (67.1875)  Acc@5: 88.2812 (88.2812)
Test: [  50/78]  Time: 0.259 (0.278)  DataTime: 0.000 (0.018)  Loss:  1.6064 (1.5337)  Acc@1: 61.7188 (61.6728)  Acc@5: 86.7188 (87.9596)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.2568 (1.5401)  Acc@1: 62.5000 (61.4800)  Acc@5: 87.5000 (87.9700)
tb
loss  0 : 2.245921875 16
loss  1 : 1.6397328125 16
loss  2 : 1.908690625 16
loss  3 : 1.814190625 16
{'loss_different_0': 2.245921875, 'loss_different_1': 1.6397328125, 'loss_different_2': 1.908690625, 'loss_different_3': 1.814190625}
Top1:  61.48
Train: 17 [   0/390 (  0%)]  Loss:  2.164942 (2.1649)  Time: 1.654s,   77.39/s  (1.654s,   77.39/s)  LR: 9.823e-03  Data: 0.903 (0.903)
Train: 17 [  50/390 ( 13%)]  Loss:  2.350966 (2.2580)  Time: 0.748s,  171.20/s  (0.766s,  167.14/s)  LR: 9.823e-03  Data: 0.000 (0.018)
Train: 17 [ 100/390 ( 26%)]  Loss:  2.382849 (2.2996)  Time: 0.748s,  171.17/s  (0.757s,  169.09/s)  LR: 9.823e-03  Data: 0.000 (0.009)
Train: 17 [ 150/390 ( 39%)]  Loss:  2.235016 (2.2834)  Time: 0.748s,  171.19/s  (0.754s,  169.76/s)  LR: 9.823e-03  Data: 0.000 (0.006)
Train: 17 [ 200/390 ( 51%)]  Loss:  2.378075 (2.3024)  Time: 0.748s,  171.21/s  (0.754s,  169.72/s)  LR: 9.823e-03  Data: 0.000 (0.005)
Train: 17 [ 250/390 ( 64%)]  Loss:  2.197758 (2.2849)  Time: 0.748s,  171.09/s  (0.753s,  170.00/s)  LR: 9.823e-03  Data: 0.000 (0.004)
Train: 17 [ 300/390 ( 77%)]  Loss:  2.012358 (2.2460)  Time: 0.749s,  171.00/s  (0.752s,  170.18/s)  LR: 9.823e-03  Data: 0.000 (0.003)
Train: 17 [ 350/390 ( 90%)]  Loss:  2.359560 (2.2602)  Time: 0.751s,  170.47/s  (0.752s,  170.31/s)  LR: 9.823e-03  Data: 0.003 (0.003)
Train: 17 [ 389/390 (100%)]  Loss:  2.325555 (2.2675)  Time: 0.748s,  171.05/s  (0.751s,  170.39/s)  LR: 9.823e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.959 (0.959)  DataTime: 0.676 (0.676)  Loss:  1.3135 (1.3135)  Acc@1: 67.1875 (67.1875)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.014)  Loss:  1.5996 (1.5138)  Acc@1: 63.2812 (61.2439)  Acc@5: 85.1562 (88.3732)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  1.2080 (1.5096)  Acc@1: 75.0000 (61.5700)  Acc@5: 87.5000 (88.4800)
tb
loss  0 : 2.227215625 17
loss  1 : 1.6216265625 17
loss  2 : 1.9047703125 17
loss  3 : 1.7477140625 17
{'loss_different_0': 2.227215625, 'loss_different_1': 1.6216265625, 'loss_different_2': 1.9047703125, 'loss_different_3': 1.7477140625}
Top1:  61.57
Train: 18 [   0/390 (  0%)]  Loss:  3.747194 (3.7472)  Time: 1.454s,   88.03/s  (1.454s,   88.03/s)  LR: 9.801e-03  Data: 0.696 (0.696)
Train: 18 [  50/390 ( 13%)]  Loss:  2.354187 (3.0507)  Time: 0.748s,  171.17/s  (0.762s,  167.99/s)  LR: 9.801e-03  Data: 0.000 (0.014)
Train: 18 [ 100/390 ( 26%)]  Loss:  2.912071 (3.0045)  Time: 0.748s,  171.15/s  (0.755s,  169.52/s)  LR: 9.801e-03  Data: 0.000 (0.007)
Train: 18 [ 150/390 ( 39%)]  Loss:  3.304359 (3.0795)  Time: 0.748s,  171.09/s  (0.753s,  170.05/s)  LR: 9.801e-03  Data: 0.000 (0.005)
Train: 18 [ 200/390 ( 51%)]  Loss:  2.225277 (2.9086)  Time: 0.748s,  171.16/s  (0.752s,  170.32/s)  LR: 9.801e-03  Data: 0.000 (0.004)
Train: 18 [ 250/390 ( 64%)]  Loss:  2.565745 (2.8515)  Time: 0.748s,  171.05/s  (0.751s,  170.47/s)  LR: 9.801e-03  Data: 0.000 (0.003)
Train: 18 [ 300/390 ( 77%)]  Loss:  1.975315 (2.7263)  Time: 0.748s,  171.06/s  (0.750s,  170.58/s)  LR: 9.801e-03  Data: 0.000 (0.003)
Train: 18 [ 350/390 ( 90%)]  Loss:  2.832062 (2.7395)  Time: 0.750s,  170.64/s  (0.750s,  170.65/s)  LR: 9.801e-03  Data: 0.003 (0.002)
Train: 18 [ 389/390 (100%)]  Loss:  2.137374 (2.6726)  Time: 0.748s,  171.07/s  (0.750s,  170.70/s)  LR: 9.801e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.898 (0.898)  DataTime: 0.635 (0.635)  Loss:  1.3037 (1.3037)  Acc@1: 68.7500 (68.7500)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.015)  Loss:  1.5244 (1.4743)  Acc@1: 60.1562 (62.6532)  Acc@5: 86.7188 (88.9706)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.010)  Loss:  1.5732 (1.4799)  Acc@1: 62.5000 (62.8700)  Acc@5: 81.2500 (88.8200)
tb
loss  0 : 2.22114375 18
loss  1 : 1.5835234375 18
loss  2 : 1.9062296875 18
loss  3 : 1.7360015625 18
{'loss_different_0': 2.22114375, 'loss_different_1': 1.5835234375, 'loss_different_2': 1.9062296875, 'loss_different_3': 1.7360015625}
Top1:  62.87
Train: 19 [   0/390 (  0%)]  Loss:  2.189425 (2.1894)  Time: 1.618s,   79.09/s  (1.618s,   79.09/s)  LR: 9.779e-03  Data: 0.867 (0.867)
Train: 19 [  50/390 ( 13%)]  Loss:  2.046901 (2.1182)  Time: 0.748s,  171.14/s  (0.765s,  167.32/s)  LR: 9.779e-03  Data: 0.000 (0.017)
Train: 19 [ 100/390 ( 26%)]  Loss:  2.205107 (2.1471)  Time: 0.747s,  171.24/s  (0.759s,  168.70/s)  LR: 9.779e-03  Data: 0.000 (0.009)
Train: 19 [ 150/390 ( 39%)]  Loss:  2.003324 (2.1112)  Time: 0.747s,  171.25/s  (0.755s,  169.51/s)  LR: 9.779e-03  Data: 0.000 (0.006)
Train: 19 [ 200/390 ( 51%)]  Loss:  2.229674 (2.1349)  Time: 0.748s,  171.17/s  (0.753s,  169.91/s)  LR: 9.779e-03  Data: 0.000 (0.005)
Train: 19 [ 389/390 (100%)]  Loss:  2.612572 (2.4040)  Time: 0.747s,  171.36/s  (0.751s,  170.52/s)  LR: 9.779e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.933 (0.933)  DataTime: 0.655 (0.655)  Loss:  1.4121 (1.4121)  Acc@1: 63.2812 (63.2812)  Acc@5: 88.2812 (88.2812)
Test: [  50/78]  Time: 0.259 (0.272)  DataTime: 0.000 (0.013)  Loss:  1.5723 (1.5065)  Acc@1: 59.3750 (63.0055)  Acc@5: 84.3750 (89.4301)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  1.2500 (1.5081)  Acc@1: 68.7500 (63.0800)  Acc@5: 87.5000 (89.2200)
tb
loss  0 : 2.230921875 19
loss  1 : 1.614378125 19
loss  2 : 1.8964578125 19
loss  3 : 1.793084375 19
{'loss_different_0': 2.230921875, 'loss_different_1': 1.614378125, 'loss_different_2': 1.8964578125, 'loss_different_3': 1.793084375}
Top1:  63.08
Train: 20 [   0/390 (  0%)]  Loss:  2.304015 (2.3040)  Time: 1.223s,  104.70/s  (1.223s,  104.70/s)  LR: 9.755e-03  Data: 0.438 (0.438)
Train: 20 [  50/390 ( 13%)]  Loss:  2.054627 (2.1793)  Time: 0.748s,  171.23/s  (0.757s,  169.06/s)  LR: 9.755e-03  Data: 0.000 (0.009)
Train: 20 [ 100/390 ( 26%)]  Loss:  2.299869 (2.2195)  Time: 0.748s,  171.07/s  (0.753s,  170.09/s)  LR: 9.755e-03  Data: 0.000 (0.005)
Train: 20 [ 150/390 ( 39%)]  Loss:  2.128844 (2.1968)  Time: 0.748s,  171.16/s  (0.751s,  170.45/s)  LR: 9.755e-03  Data: 0.000 (0.003)
Train: 20 [ 200/390 ( 51%)]  Loss:  3.551545 (2.4678)  Time: 0.748s,  171.11/s  (0.750s,  170.63/s)  LR: 9.755e-03  Data: 0.001 (0.003)
Train: 20 [ 250/390 ( 64%)]  Loss:  2.351061 (2.4483)  Time: 0.748s,  171.04/s  (0.751s,  170.43/s)  LR: 9.755e-03  Data: 0.000 (0.002)
Train: 20 [ 300/390 ( 77%)]  Loss:  2.255500 (2.4208)  Time: 0.748s,  171.22/s  (0.751s,  170.55/s)  LR: 9.755e-03  Data: 0.000 (0.002)
Train: 20 [ 350/390 ( 90%)]  Loss:  2.125166 (2.3838)  Time: 0.750s,  170.70/s  (0.750s,  170.64/s)  LR: 9.755e-03  Data: 0.003 (0.002)
Train: 20 [ 389/390 (100%)]  Loss:  2.540694 (2.4013)  Time: 0.748s,  171.21/s  (0.750s,  170.70/s)  LR: 9.755e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.157 (1.157)  DataTime: 0.858 (0.858)  Loss:  1.2949 (1.2949)  Acc@1: 70.3125 (70.3125)  Acc@5: 90.6250 (90.6250)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.017)  Loss:  1.5078 (1.4669)  Acc@1: 60.9375 (63.0821)  Acc@5: 87.5000 (89.3995)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.4170 (1.4663)  Acc@1: 75.0000 (63.5100)  Acc@5: 87.5000 (89.3800)
tb
loss  0 : 2.15708125 20
loss  1 : 1.6215375 20
loss  2 : 1.8042421875 20
loss  3 : 1.785359375 20
{'loss_different_0': 2.15708125, 'loss_different_1': 1.6215375, 'loss_different_2': 1.8042421875, 'loss_different_3': 1.785359375}
Top1:  63.51
Saving model to ./output/train/20250829-131105-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_4/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_4_20.pt
Train: 21 [   0/390 (  0%)]  Loss:  2.167247 (2.1672)  Time: 1.606s,   79.72/s  (1.606s,   79.72/s)  LR: 9.730e-03  Data: 0.856 (0.856)
Train: 21 [  50/390 ( 13%)]  Loss:  3.343426 (2.7553)  Time: 0.748s,  171.22/s  (0.765s,  167.39/s)  LR: 9.730e-03  Data: 0.000 (0.017)
Train: 21 [ 100/390 ( 26%)]  Loss:  2.041407 (2.5174)  Time: 0.748s,  171.19/s  (0.756s,  169.24/s)  LR: 9.730e-03  Data: 0.000 (0.009)
Train: 21 [ 150/390 ( 39%)]  Loss:  3.759308 (2.8278)  Time: 0.748s,  171.19/s  (0.754s,  169.87/s)  LR: 9.730e-03  Data: 0.000 (0.006)
Train: 21 [ 200/390 ( 51%)]  Loss:  2.448579 (2.7520)  Time: 0.748s,  171.21/s  (0.752s,  170.18/s)  LR: 9.730e-03  Data: 0.000 (0.005)
Train: 21 [ 250/390 ( 64%)]  Loss:  2.783355 (2.7572)  Time: 0.748s,  171.21/s  (0.751s,  170.38/s)  LR: 9.730e-03  Data: 0.001 (0.004)
Train: 21 [ 300/390 ( 77%)]  Loss:  3.765975 (2.9013)  Time: 0.748s,  171.20/s  (0.751s,  170.51/s)  LR: 9.730e-03  Data: 0.000 (0.003)
Train: 21 [ 350/390 ( 90%)]  Loss:  2.469078 (2.8473)  Time: 0.750s,  170.59/s  (0.750s,  170.60/s)  LR: 9.730e-03  Data: 0.003 (0.003)
Train: 21 [ 389/390 (100%)]  Loss:  2.072163 (2.7612)  Time: 0.747s,  171.30/s  (0.750s,  170.66/s)  LR: 9.730e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.201 (1.201)  DataTime: 0.917 (0.917)  Loss:  1.2441 (1.2441)  Acc@1: 68.7500 (68.7500)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.018)  Loss:  1.4697 (1.4084)  Acc@1: 65.6250 (63.3732)  Acc@5: 87.5000 (90.3646)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.3975 (1.4048)  Acc@1: 68.7500 (63.9400)  Acc@5: 81.2500 (90.1900)
tb
loss  0 : 2.1461375 21
loss  1 : 1.5296375 21
loss  2 : 1.8098125 21
loss  3 : 1.6679125 21
{'loss_different_0': 2.1461375, 'loss_different_1': 1.5296375, 'loss_different_2': 1.8098125, 'loss_different_3': 1.6679125}
Top1:  63.94
Train: 22 [   0/390 (  0%)]  Loss:  1.974092 (1.9741)  Time: 1.326s,   96.50/s  (1.326s,   96.50/s)  LR: 9.704e-03  Data: 0.555 (0.555)
Train: 22 [  50/390 ( 13%)]  Loss:  2.324920 (2.1495)  Time: 0.747s,  171.27/s  (0.759s,  168.61/s)  LR: 9.704e-03  Data: 0.000 (0.011)
Train: 22 [ 100/390 ( 26%)]  Loss:  2.104864 (2.1346)  Time: 0.748s,  171.19/s  (0.754s,  169.87/s)  LR: 9.704e-03  Data: 0.000 (0.006)
Train: 22 [ 150/390 ( 39%)]  Loss:  2.271966 (2.1690)  Time: 0.747s,  171.28/s  (0.753s,  169.97/s)  LR: 9.704e-03  Data: 0.000 (0.004)
Train: 22 [ 200/390 ( 51%)]  Loss:  2.138987 (2.1630)  Time: 0.748s,  171.10/s  (0.752s,  170.27/s)  LR: 9.704e-03  Data: 0.000 (0.003)
Train: 22 [ 250/390 ( 64%)]  Loss:  2.677247 (2.2487)  Time: 0.748s,  171.15/s  (0.751s,  170.45/s)  LR: 9.704e-03  Data: 0.000 (0.003)
Train: 22 [ 300/390 ( 77%)]  Loss:  2.060331 (2.2218)  Time: 0.748s,  171.21/s  (0.750s,  170.57/s)  LR: 9.704e-03  Data: 0.000 (0.002)
Train: 22 [ 350/390 ( 90%)]  Loss:  2.111372 (2.2080)  Time: 0.751s,  170.45/s  (0.750s,  170.66/s)  LR: 9.704e-03  Data: 0.003 (0.002)
Train: 22 [ 389/390 (100%)]  Loss:  2.068606 (2.1925)  Time: 0.748s,  171.13/s  (0.750s,  170.72/s)  LR: 9.704e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.098 (1.098)  DataTime: 0.807 (0.807)  Loss:  1.3721 (1.3721)  Acc@1: 67.9688 (67.9688)  Acc@5: 89.8438 (89.8438)
Test: [  50/78]  Time: 0.259 (0.275)  DataTime: 0.000 (0.016)  Loss:  1.4619 (1.4512)  Acc@1: 66.4062 (64.9357)  Acc@5: 89.8438 (89.9510)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.5127 (1.4588)  Acc@1: 68.7500 (64.9100)  Acc@5: 81.2500 (89.9500)
tb
loss  0 : 2.13694375 22
loss  1 : 1.592540625 22
loss  2 : 1.787703125 22
loss  3 : 1.756953125 22
{'loss_different_0': 2.13694375, 'loss_different_1': 1.592540625, 'loss_different_2': 1.787703125, 'loss_different_3': 1.756953125}
Top1:  64.91
Train: 23 [   0/390 (  0%)]  Loss:  1.985994 (1.9860)  Time: 1.322s,   96.82/s  (1.322s,   96.82/s)  LR: 9.677e-03  Data: 0.553 (0.553)
Train: 23 [  50/390 ( 13%)]  Loss:  2.178228 (2.0821)  Time: 0.748s,  171.17/s  (0.759s,  168.64/s)  LR: 9.677e-03  Data: 0.000 (0.011)
Train: 23 [ 100/390 ( 26%)]  Loss:  2.085738 (2.0833)  Time: 0.747s,  171.27/s  (0.753s,  169.91/s)  LR: 9.677e-03  Data: 0.000 (0.006)
Train: 23 [ 150/390 ( 39%)]  Loss:  2.610781 (2.2152)  Time: 0.747s,  171.45/s  (0.751s,  170.33/s)  LR: 9.677e-03  Data: 0.000 (0.004)
Train: 23 [ 200/390 ( 51%)]  Loss:  2.091913 (2.1905)  Time: 0.748s,  171.09/s  (0.751s,  170.54/s)  LR: 9.677e-03  Data: 0.000 (0.003)
Train: 23 [ 250/390 ( 64%)]  Loss:  2.876991 (2.3049)  Time: 0.748s,  171.21/s  (0.750s,  170.67/s)  LR: 9.677e-03  Data: 0.000 (0.003)
Train: 23 [ 300/390 ( 77%)]  Loss:  2.060637 (2.2700)  Time: 0.747s,  171.25/s  (0.751s,  170.50/s)  LR: 9.677e-03  Data: 0.000 (0.002)
Train: 23 [ 350/390 ( 90%)]  Loss:  2.533011 (2.3029)  Time: 0.749s,  170.80/s  (0.750s,  170.60/s)  LR: 9.677e-03  Data: 0.002 (0.002)
Train: 23 [ 389/390 (100%)]  Loss:  2.277357 (2.3001)  Time: 0.747s,  171.28/s  (0.750s,  170.66/s)  LR: 9.677e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.191 (1.191)  DataTime: 0.916 (0.916)  Loss:  1.2490 (1.2490)  Acc@1: 65.6250 (65.6250)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.258 (0.277)  DataTime: 0.000 (0.018)  Loss:  1.5020 (1.3957)  Acc@1: 62.5000 (64.8284)  Acc@5: 88.2812 (89.8284)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.1309 (1.4058)  Acc@1: 68.7500 (64.4700)  Acc@5: 93.7500 (89.6900)
tb
loss  0 : 2.191684375 23
loss  1 : 1.5515390625 23
loss  2 : 1.8034921875 23
loss  3 : 1.70499375 23
{'loss_different_0': 2.191684375, 'loss_different_1': 1.5515390625, 'loss_different_2': 1.8034921875, 'loss_different_3': 1.70499375}
Top1:  64.47
Train: 24 [   0/390 (  0%)]  Loss:  2.090704 (2.0907)  Time: 1.193s,  107.31/s  (1.193s,  107.31/s)  LR: 9.649e-03  Data: 0.367 (0.367)
Train: 24 [  50/390 ( 13%)]  Loss:  2.098756 (2.0947)  Time: 0.748s,  171.03/s  (0.756s,  169.25/s)  LR: 9.649e-03  Data: 0.001 (0.008)
Train: 24 [ 100/390 ( 26%)]  Loss:  1.976379 (2.0553)  Time: 0.747s,  171.36/s  (0.752s,  170.23/s)  LR: 9.649e-03  Data: 0.000 (0.004)
Train: 24 [ 150/390 ( 39%)]  Loss:  2.015645 (2.0454)  Time: 0.748s,  171.16/s  (0.750s,  170.56/s)  LR: 9.649e-03  Data: 0.000 (0.003)
Train: 24 [ 200/390 ( 51%)]  Loss:  2.070276 (2.0504)  Time: 0.748s,  171.14/s  (0.750s,  170.72/s)  LR: 9.649e-03  Data: 0.000 (0.002)
Train: 24 [ 250/390 ( 64%)]  Loss:  1.993798 (2.0409)  Time: 0.747s,  171.30/s  (0.749s,  170.82/s)  LR: 9.649e-03  Data: 0.000 (0.002)
Train: 24 [ 300/390 ( 77%)]  Loss:  2.298017 (2.0777)  Time: 0.748s,  171.08/s  (0.749s,  170.89/s)  LR: 9.649e-03  Data: 0.000 (0.002)
Train: 24 [ 350/390 ( 90%)]  Loss:  2.138033 (2.0852)  Time: 0.750s,  170.63/s  (0.749s,  170.94/s)  LR: 9.649e-03  Data: 0.003 (0.001)
Train: 24 [ 389/390 (100%)]  Loss:  2.859215 (2.1712)  Time: 0.747s,  171.40/s  (0.749s,  170.97/s)  LR: 9.649e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.224 (1.224)  DataTime: 0.962 (0.962)  Loss:  1.2871 (1.2871)  Acc@1: 69.5312 (69.5312)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.019)  Loss:  1.4463 (1.3889)  Acc@1: 64.0625 (65.8241)  Acc@5: 88.2812 (89.7059)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.4453 (1.3809)  Acc@1: 62.5000 (66.0600)  Acc@5: 81.2500 (89.8700)
tb
loss  0 : 2.1632 24
loss  1 : 1.5341 24
loss  2 : 1.748590625 24
loss  3 : 1.6768125 24
{'loss_different_0': 2.1632, 'loss_different_1': 1.5341, 'loss_different_2': 1.748590625, 'loss_different_3': 1.6768125}
Top1:  66.06
Train: 25 [   0/390 (  0%)]  Loss:  1.912746 (1.9127)  Time: 1.113s,  114.99/s  (1.113s,  114.99/s)  LR: 9.619e-03  Data: 0.327 (0.327)
Train: 25 [  50/390 ( 13%)]  Loss:  2.023543 (1.9681)  Time: 0.747s,  171.24/s  (0.755s,  169.63/s)  LR: 9.619e-03  Data: 0.000 (0.007)
Train: 25 [ 100/390 ( 26%)]  Loss:  2.053736 (1.9967)  Time: 0.747s,  171.33/s  (0.751s,  170.42/s)  LR: 9.619e-03  Data: 0.000 (0.004)
Train: 25 [ 150/390 ( 39%)]  Loss:  2.584467 (2.1436)  Time: 0.748s,  171.17/s  (0.750s,  170.69/s)  LR: 9.619e-03  Data: 0.000 (0.003)
Train: 25 [ 200/390 ( 51%)]  Loss:  1.894008 (2.0937)  Time: 0.747s,  171.33/s  (0.749s,  170.83/s)  LR: 9.619e-03  Data: 0.000 (0.002)
Train: 25 [ 250/390 ( 64%)]  Loss:  2.199453 (2.1113)  Time: 0.748s,  171.21/s  (0.750s,  170.71/s)  LR: 9.619e-03  Data: 0.000 (0.002)
Train: 25 [ 300/390 ( 77%)]  Loss:  3.414864 (2.2975)  Time: 0.748s,  171.11/s  (0.749s,  170.79/s)  LR: 9.619e-03  Data: 0.000 (0.001)
Train: 25 [ 350/390 ( 90%)]  Loss:  3.464623 (2.4434)  Time: 0.750s,  170.76/s  (0.749s,  170.85/s)  LR: 9.619e-03  Data: 0.003 (0.001)
Train: 25 [ 389/390 (100%)]  Loss:  1.960513 (2.3898)  Time: 0.748s,  171.21/s  (0.749s,  170.90/s)  LR: 9.619e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 0.862 (0.862)  DataTime: 0.597 (0.597)  Loss:  1.2666 (1.2666)  Acc@1: 70.3125 (70.3125)  Acc@5: 89.0625 (89.0625)
Test: [  50/78]  Time: 0.258 (0.275)  DataTime: 0.000 (0.017)  Loss:  1.4170 (1.4025)  Acc@1: 67.1875 (66.1305)  Acc@5: 88.2812 (89.7518)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.011)  Loss:  1.6670 (1.4011)  Acc@1: 56.2500 (65.8800)  Acc@5: 81.2500 (89.5900)
tb
loss  0 : 2.1313875 25
loss  1 : 1.549790625 25
loss  2 : 1.75473125 25
loss  3 : 1.69798125 25
{'loss_different_0': 2.1313875, 'loss_different_1': 1.549790625, 'loss_different_2': 1.75473125, 'loss_different_3': 1.69798125}
Top1:  65.88
Train: 26 [   0/390 (  0%)]  Loss:  2.052903 (2.0529)  Time: 1.462s,   87.53/s  (1.462s,   87.53/s)  LR: 9.589e-03  Data: 0.645 (0.645)
Train: 26 [  50/390 ( 13%)]  Loss:  2.048146 (2.0505)  Time: 0.747s,  171.43/s  (0.762s,  168.07/s)  LR: 9.589e-03  Data: 0.000 (0.013)
Train: 26 [ 100/390 ( 26%)]  Loss:  2.035465 (2.0455)  Time: 0.747s,  171.26/s  (0.755s,  169.61/s)  LR: 9.589e-03  Data: 0.000 (0.007)
Train: 26 [ 150/390 ( 39%)]  Loss:  1.850669 (1.9968)  Time: 0.748s,  171.23/s  (0.752s,  170.14/s)  LR: 9.589e-03  Data: 0.000 (0.005)
Train: 26 [ 200/390 ( 51%)]  Loss:  2.056064 (2.0086)  Time: 0.747s,  171.27/s  (0.751s,  170.41/s)  LR: 9.589e-03  Data: 0.000 (0.004)
Train: 26 [ 250/390 ( 64%)]  Loss:  2.093166 (2.0227)  Time: 0.748s,  171.18/s  (0.750s,  170.57/s)  LR: 9.589e-03  Data: 0.000 (0.003)
Train: 26 [ 300/390 ( 77%)]  Loss:  2.147175 (2.0405)  Time: 0.747s,  171.25/s  (0.750s,  170.68/s)  LR: 9.589e-03  Data: 0.000 (0.003)
Train: 26 [ 350/390 ( 90%)]  Loss:  2.119982 (2.0504)  Time: 0.750s,  170.64/s  (0.750s,  170.76/s)  LR: 9.589e-03  Data: 0.003 (0.002)
Train: 26 [ 389/390 (100%)]  Loss:  2.883063 (2.1430)  Time: 0.747s,  171.40/s  (0.750s,  170.63/s)  LR: 9.589e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.011 (1.011)  DataTime: 0.740 (0.740)  Loss:  1.3662 (1.3662)  Acc@1: 66.4062 (66.4062)  Acc@5: 89.8438 (89.8438)
Test: [  50/78]  Time: 0.258 (0.273)  DataTime: 0.000 (0.015)  Loss:  1.4316 (1.3823)  Acc@1: 62.5000 (64.9969)  Acc@5: 89.8438 (90.1501)
Test: [  78/78]  Time: 0.034 (0.265)  DataTime: 0.000 (0.010)  Loss:  1.3418 (1.3795)  Acc@1: 68.7500 (65.2400)  Acc@5: 87.5000 (90.1500)
tb
loss  0 : 2.1683359375 26
loss  1 : 1.5388921875 26
loss  2 : 1.7907328125 26
loss  3 : 1.6687125 26
{'loss_different_0': 2.1683359375, 'loss_different_1': 1.5388921875, 'loss_different_2': 1.7907328125, 'loss_different_3': 1.6687125}
Top1:  65.24
Train: 27 [   0/390 (  0%)]  Loss:  2.349048 (2.3490)  Time: 1.387s,   92.30/s  (1.387s,   92.30/s)  LR: 9.557e-03  Data: 0.627 (0.627)
Train: 27 [  50/390 ( 13%)]  Loss:  2.455149 (2.4021)  Time: 0.747s,  171.39/s  (0.760s,  168.44/s)  LR: 9.557e-03  Data: 0.000 (0.013)
Train: 27 [ 100/390 ( 26%)]  Loss:  2.839734 (2.5480)  Time: 0.748s,  171.04/s  (0.754s,  169.80/s)  LR: 9.557e-03  Data: 0.000 (0.007)
Train: 27 [ 150/390 ( 39%)]  Loss:  2.075076 (2.4298)  Time: 0.747s,  171.24/s  (0.752s,  170.28/s)  LR: 9.557e-03  Data: 0.000 (0.005)
Train: 27 [ 200/390 ( 51%)]  Loss:  2.104070 (2.3646)  Time: 0.748s,  171.15/s  (0.751s,  170.51/s)  LR: 9.557e-03  Data: 0.001 (0.003)
Train: 27 [ 250/390 ( 64%)]  Loss:  2.971524 (2.4658)  Time: 0.747s,  171.28/s  (0.750s,  170.64/s)  LR: 9.557e-03  Data: 0.000 (0.003)
Train: 27 [ 300/390 ( 77%)]  Loss:  2.098887 (2.4134)  Time: 0.748s,  171.14/s  (0.750s,  170.73/s)  LR: 9.557e-03  Data: 0.000 (0.002)
Train: 27 [ 350/390 ( 90%)]  Loss:  2.164860 (2.3823)  Time: 0.750s,  170.73/s  (0.749s,  170.79/s)  LR: 9.557e-03  Data: 0.003 (0.002)
Train: 27 [ 389/390 (100%)]  Loss:  3.032011 (2.4545)  Time: 0.748s,  171.08/s  (0.749s,  170.83/s)  LR: 9.557e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.214 (1.214)  DataTime: 0.920 (0.920)  Loss:  1.2793 (1.2793)  Acc@1: 74.2188 (74.2188)  Acc@5: 89.8438 (89.8438)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.018)  Loss:  1.4229 (1.3761)  Acc@1: 67.1875 (66.3450)  Acc@5: 89.0625 (90.5484)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.2529 (1.3758)  Acc@1: 75.0000 (66.4800)  Acc@5: 81.2500 (90.5000)
tb
loss  0 : 2.21620625 27
loss  1 : 1.516103125 27
loss  2 : 1.785325 27
loss  3 : 1.6539765625 27
{'loss_different_0': 2.21620625, 'loss_different_1': 1.516103125, 'loss_different_2': 1.785325, 'loss_different_3': 1.6539765625}
Top1:  66.48
Train: 28 [   0/390 (  0%)]  Loss:  1.809628 (1.8096)  Time: 1.592s,   80.42/s  (1.592s,   80.42/s)  LR: 9.524e-03  Data: 0.843 (0.843)
Train: 28 [  50/390 ( 13%)]  Loss:  3.328516 (2.5691)  Time: 0.749s,  170.95/s  (0.764s,  167.50/s)  LR: 9.524e-03  Data: 0.000 (0.017)
Train: 28 [ 100/390 ( 26%)]  Loss:  1.915533 (2.3512)  Time: 0.748s,  171.17/s  (0.756s,  169.31/s)  LR: 9.524e-03  Data: 0.000 (0.009)
Train: 28 [ 150/390 ( 39%)]  Loss:  2.039979 (2.2734)  Time: 0.748s,  171.19/s  (0.755s,  169.43/s)  LR: 9.524e-03  Data: 0.000 (0.006)
Train: 28 [ 200/390 ( 51%)]  Loss:  1.870097 (2.1928)  Time: 0.748s,  171.23/s  (0.754s,  169.87/s)  LR: 9.524e-03  Data: 0.000 (0.005)
Train: 28 [ 250/390 ( 64%)]  Loss:  1.790983 (2.1258)  Time: 0.749s,  170.99/s  (0.752s,  170.13/s)  LR: 9.524e-03  Data: 0.001 (0.004)
Train: 28 [ 300/390 ( 77%)]  Loss:  3.786242 (2.3630)  Time: 0.748s,  171.14/s  (0.752s,  170.31/s)  LR: 9.524e-03  Data: 0.000 (0.003)
Train: 28 [ 350/390 ( 90%)]  Loss:  2.101973 (2.3304)  Time: 0.750s,  170.77/s  (0.751s,  170.43/s)  LR: 9.524e-03  Data: 0.003 (0.003)
Train: 28 [ 389/390 (100%)]  Loss:  2.963297 (2.4007)  Time: 0.748s,  171.22/s  (0.751s,  170.51/s)  LR: 9.524e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.104 (1.104)  DataTime: 0.756 (0.756)  Loss:  1.2305 (1.2305)  Acc@1: 74.2188 (74.2188)  Acc@5: 90.6250 (90.6250)
Test: [  50/78]  Time: 0.258 (0.275)  DataTime: 0.000 (0.015)  Loss:  1.5234 (1.3504)  Acc@1: 62.5000 (67.7696)  Acc@5: 89.0625 (90.9467)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.010)  Loss:  1.1914 (1.3439)  Acc@1: 75.0000 (67.6400)  Acc@5: 87.5000 (91.0400)
tb
loss  0 : 2.1348234375 28
loss  1 : 1.4946203125 28
loss  2 : 1.701275 28
loss  3 : 1.611178125 28
{'loss_different_0': 2.1348234375, 'loss_different_1': 1.4946203125, 'loss_different_2': 1.701275, 'loss_different_3': 1.611178125}
Top1:  67.64
Train: 29 [   0/390 (  0%)]  Loss:  1.989794 (1.9898)  Time: 1.110s,  115.29/s  (1.110s,  115.29/s)  LR: 9.490e-03  Data: 0.331 (0.331)
Train: 29 [  50/390 ( 13%)]  Loss:  1.969378 (1.9796)  Time: 0.747s,  171.32/s  (0.755s,  169.64/s)  LR: 9.490e-03  Data: 0.000 (0.007)
Train: 29 [ 100/390 ( 26%)]  Loss:  1.837821 (1.9323)  Time: 0.748s,  171.14/s  (0.751s,  170.43/s)  LR: 9.490e-03  Data: 0.000 (0.004)
Train: 29 [ 150/390 ( 39%)]  Loss:  2.056514 (1.9634)  Time: 0.748s,  171.19/s  (0.750s,  170.70/s)  LR: 9.490e-03  Data: 0.000 (0.003)
Train: 29 [ 200/390 ( 51%)]  Loss:  1.913664 (1.9534)  Time: 0.747s,  171.31/s  (0.749s,  170.83/s)  LR: 9.490e-03  Data: 0.000 (0.002)
Train: 29 [ 250/390 ( 64%)]  Loss:  1.861322 (1.9381)  Time: 0.747s,  171.34/s  (0.749s,  170.91/s)  LR: 9.490e-03  Data: 0.000 (0.002)
Train: 29 [ 300/390 ( 77%)]  Loss:  3.836514 (2.2093)  Time: 0.748s,  171.06/s  (0.749s,  170.97/s)  LR: 9.490e-03  Data: 0.000 (0.001)
Train: 29 [ 350/390 ( 90%)]  Loss:  1.963450 (2.1786)  Time: 0.749s,  170.80/s  (0.749s,  171.00/s)  LR: 9.490e-03  Data: 0.003 (0.001)
Train: 29 [ 389/390 (100%)]  Loss:  2.116478 (2.1717)  Time: 0.748s,  171.23/s  (0.748s,  171.03/s)  LR: 9.490e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 0.882 (0.882)  DataTime: 0.616 (0.616)  Loss:  1.2432 (1.2432)  Acc@1: 71.8750 (71.8750)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.258 (0.275)  DataTime: 0.000 (0.017)  Loss:  1.4619 (1.3794)  Acc@1: 67.1875 (66.5441)  Acc@5: 89.8438 (90.7782)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.5215 (1.3694)  Acc@1: 62.5000 (66.8600)  Acc@5: 87.5000 (90.7400)
tb
loss  0 : 2.16993125 29
loss  1 : 1.486509375 29
loss  2 : 1.75736875 29
loss  3 : 1.677946875 29
{'loss_different_0': 2.16993125, 'loss_different_1': 1.486509375, 'loss_different_2': 1.75736875, 'loss_different_3': 1.677946875}
Top1:  66.86
Train: 30 [   0/390 (  0%)]  Loss:  3.254188 (3.2542)  Time: 1.590s,   80.52/s  (1.590s,   80.52/s)  LR: 9.455e-03  Data: 0.841 (0.841)
Train: 30 [  50/390 ( 13%)]  Loss:  3.687166 (3.4707)  Time: 0.748s,  171.16/s  (0.768s,  166.59/s)  LR: 9.455e-03  Data: 0.000 (0.017)
Train: 30 [ 100/390 ( 26%)]  Loss:  1.903950 (2.9484)  Time: 0.748s,  171.20/s  (0.758s,  168.85/s)  LR: 9.455e-03  Data: 0.000 (0.009)
Train: 30 [ 150/390 ( 39%)]  Loss:  3.114096 (2.9898)  Time: 0.747s,  171.24/s  (0.755s,  169.63/s)  LR: 9.455e-03  Data: 0.000 (0.006)
Train: 30 [ 200/390 ( 51%)]  Loss:  1.983800 (2.7886)  Time: 0.748s,  171.13/s  (0.753s,  170.02/s)  LR: 9.455e-03  Data: 0.000 (0.005)
Train: 30 [ 250/390 ( 64%)]  Loss:  1.917492 (2.6434)  Time: 0.747s,  171.26/s  (0.752s,  170.26/s)  LR: 9.455e-03  Data: 0.001 (0.004)
Train: 30 [ 300/390 ( 77%)]  Loss:  1.868570 (2.5328)  Time: 0.747s,  171.27/s  (0.751s,  170.42/s)  LR: 9.455e-03  Data: 0.000 (0.003)
Train: 30 [ 350/390 ( 90%)]  Loss:  2.995498 (2.5906)  Time: 0.750s,  170.68/s  (0.751s,  170.53/s)  LR: 9.455e-03  Data: 0.003 (0.003)
Train: 30 [ 389/390 (100%)]  Loss:  2.163342 (2.5431)  Time: 0.748s,  171.16/s  (0.750s,  170.61/s)  LR: 9.455e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.056 (1.056)  DataTime: 0.788 (0.788)  Loss:  1.1211 (1.1211)  Acc@1: 71.0938 (71.0938)  Acc@5: 92.1875 (92.1875)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.016)  Loss:  1.4346 (1.3423)  Acc@1: 64.0625 (66.7279)  Acc@5: 92.1875 (90.3033)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.010)  Loss:  1.5654 (1.3376)  Acc@1: 62.5000 (66.8800)  Acc@5: 81.2500 (90.6100)
tb
loss  0 : 2.09840625 30
loss  1 : 1.4889140625 30
loss  2 : 1.697053125 30
loss  3 : 1.637540625 30
{'loss_different_0': 2.09840625, 'loss_different_1': 1.4889140625, 'loss_different_2': 1.697053125, 'loss_different_3': 1.637540625}
Top1:  66.88
Saving model to ./output/train/20250829-131105-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_4/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_4_30.pt
Train: 31 [   0/390 (  0%)]  Loss:  3.262084 (3.2621)  Time: 1.324s,   96.68/s  (1.324s,   96.68/s)  LR: 9.419e-03  Data: 0.567 (0.567)
Train: 31 [  50/390 ( 13%)]  Loss:  2.305046 (2.7836)  Time: 0.748s,  171.18/s  (0.759s,  168.67/s)  LR: 9.419e-03  Data: 0.000 (0.011)
Train: 31 [ 100/390 ( 26%)]  Loss:  1.920670 (2.4959)  Time: 0.748s,  171.20/s  (0.753s,  169.93/s)  LR: 9.419e-03  Data: 0.000 (0.006)
Train: 31 [ 150/390 ( 39%)]  Loss:  2.057277 (2.3863)  Time: 0.747s,  171.38/s  (0.751s,  170.36/s)  LR: 9.419e-03  Data: 0.000 (0.004)
Train: 31 [ 200/390 ( 51%)]  Loss:  3.011694 (2.5114)  Time: 0.747s,  171.32/s  (0.752s,  170.23/s)  LR: 9.419e-03  Data: 0.000 (0.003)
^C
Traceback (most recent call last):
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 233, in <module>
    main()
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 69, in main
    train_metrics = train_epoch(
                    ^^^^^^^^^^^^
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 127, in train_epoch
    loss_scaler(loss, optimizer)
  File "/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py", line 96, in __call__
    self._scaler.step(optimizer)
  File "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py", line 457, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
               ^^^^^^^^
KeyboardInterrupt