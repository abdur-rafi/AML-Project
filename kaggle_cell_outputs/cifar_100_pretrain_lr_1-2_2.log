2025-08-29 04:24:11.593383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1756441451.792566      98 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1756441451.850416      98 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
create dataset:  cifar100
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Creating model..
, number of params: 27599012
SGD!!!
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Train: 0 [   0/390 (  0%)]  Loss:  4.619816 (4.6198)  Time: 3.314s,   38.62/s  (3.314s,   38.62/s)  LR: 1.000e-02  Data: 1.225 (1.225)
Train: 0 [  50/390 ( 13%)]  Loss:  4.573690 (4.5968)  Time: 0.748s,  171.20/s  (0.798s,  160.32/s)  LR: 1.000e-02  Data: 0.000 (0.024)
Train: 0 [ 100/390 ( 26%)]  Loss:  4.446521 (4.5467)  Time: 0.748s,  171.23/s  (0.773s,  165.52/s)  LR: 1.000e-02  Data: 0.001 (0.013)
Train: 0 [ 150/390 ( 39%)]  Loss:  4.366542 (4.5016)  Time: 0.748s,  171.15/s  (0.765s,  167.37/s)  LR: 1.000e-02  Data: 0.000 (0.008)
Train: 0 [ 200/390 ( 51%)]  Loss:  4.337895 (4.4689)  Time: 0.746s,  171.51/s  (0.762s,  167.90/s)  LR: 1.000e-02  Data: 0.000 (0.006)
Train: 0 [ 250/390 ( 64%)]  Loss:  4.267788 (4.4354)  Time: 0.747s,  171.40/s  (0.759s,  168.56/s)  LR: 1.000e-02  Data: 0.000 (0.005)
Train: 0 [ 300/390 ( 77%)]  Loss:  4.134818 (4.3924)  Time: 0.747s,  171.25/s  (0.757s,  169.00/s)  LR: 1.000e-02  Data: 0.000 (0.004)
Train: 0 [ 350/390 ( 90%)]  Loss:  4.075500 (4.3528)  Time: 0.749s,  170.88/s  (0.756s,  169.32/s)  LR: 1.000e-02  Data: 0.003 (0.004)
Train: 0 [ 389/390 (100%)]  Loss:  3.989391 (4.3124)  Time: 0.748s,  171.21/s  (0.755s,  169.52/s)  LR: 1.000e-02  Data: 0.000 (0.004)
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Test: [   0/78]  Time: 1.327 (1.327)  DataTime: 0.840 (0.840)  Loss:  3.6289 (3.6289)  Acc@1: 20.3125 (20.3125)  Acc@5: 46.8750 (46.8750)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.017)  Loss:  3.7969 (3.7495)  Acc@1: 13.2812 (12.9289)  Acc@5: 40.6250 (41.2071)
Test: [  78/78]  Time: 0.040 (0.269)  DataTime: 0.000 (0.011)  Loss:  3.8594 (3.7597)  Acc@1:  6.2500 (12.7900)  Acc@5: 25.0000 (40.6800)
tb
loss  0 : 4.54575 0
loss  1 : 3.59774375 0
loss  2 : 3.877925 0
loss  3 : 3.60708125 0
{'loss_different_0': 4.54575, 'loss_different_1': 3.59774375, 'loss_different_2': 3.877925, 'loss_different_3': 3.60708125}
Top1:  12.79
Saving model to ./output/train/20250829-042433-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_2/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_2_0.pt
Train: 1 [   0/390 (  0%)]  Loss:  3.965521 (3.9655)  Time: 1.351s,   94.76/s  (1.351s,   94.76/s)  LR: 9.999e-03  Data: 0.597 (0.597)
Train: 1 [ 150/390 ( 39%)]  Loss:  3.890618 (3.9075)  Time: 0.748s,  171.16/s  (0.751s,  170.40/s)  LR: 9.999e-03  Data: 0.000 (0.004)
Train: 1 [ 200/390 ( 51%)]  Loss:  3.580710 (3.8421)  Time: 0.748s,  171.22/s  (0.750s,  170.62/s)  LR: 9.999e-03  Data: 0.000 (0.003)
Train: 1 [ 250/390 ( 64%)]  Loss:  3.687058 (3.8163)  Time: 0.748s,  171.20/s  (0.750s,  170.76/s)  LR: 9.999e-03  Data: 0.000 (0.003)
Train: 1 [ 300/390 ( 77%)]  Loss:  3.984650 (3.8403)  Time: 0.747s,  171.33/s  (0.749s,  170.84/s)  LR: 9.999e-03  Data: 0.001 (0.002)
Train: 1 [ 350/390 ( 90%)]  Loss:  3.492030 (3.7968)  Time: 0.750s,  170.73/s  (0.749s,  170.90/s)  LR: 9.999e-03  Data: 0.003 (0.002)
Train: 1 [ 389/390 (100%)]  Loss:  3.710732 (3.7872)  Time: 0.747s,  171.27/s  (0.749s,  170.94/s)  LR: 9.999e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.003 (1.003)  DataTime: 0.724 (0.724)  Loss:  2.9805 (2.9805)  Acc@1: 26.5625 (26.5625)  Acc@5: 58.5938 (58.5938)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.015)  Loss:  3.1934 (3.1109)  Acc@1: 21.0938 (22.4265)  Acc@5: 54.6875 (58.4252)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.009)  Loss:  3.2285 (3.1228)  Acc@1:  6.2500 (22.0500)  Acc@5: 50.0000 (58.1400)
tb
loss  0 : 4.2128375 1
loss  1 : 2.969546875 1
loss  2 : 3.46361875 1
loss  3 : 3.02696875 1
{'loss_different_0': 4.2128375, 'loss_different_1': 2.969546875, 'loss_different_2': 3.46361875, 'loss_different_3': 3.02696875}
Top1:  22.05
Train: 2 [   0/390 (  0%)]  Loss:  3.641089 (3.6411)  Time: 1.615s,   79.25/s  (1.615s,   79.25/s)  LR: 9.998e-03  Data: 0.866 (0.866)
Train: 2 [  50/390 ( 13%)]  Loss:  3.365276 (3.5032)  Time: 0.747s,  171.34/s  (0.769s,  166.35/s)  LR: 9.998e-03  Data: 0.000 (0.017)
Train: 2 [ 100/390 ( 26%)]  Loss:  3.530268 (3.5122)  Time: 0.747s,  171.28/s  (0.759s,  168.75/s)  LR: 9.998e-03  Data: 0.000 (0.009)
Train: 2 [ 150/390 ( 39%)]  Loss:  4.293145 (3.7074)  Time: 0.748s,  171.12/s  (0.755s,  169.57/s)  LR: 9.998e-03  Data: 0.000 (0.006)
Train: 2 [ 200/390 ( 51%)]  Loss:  3.300445 (3.6260)  Time: 0.747s,  171.36/s  (0.753s,  169.99/s)  LR: 9.998e-03  Data: 0.000 (0.005)
Train: 2 [ 250/390 ( 64%)]  Loss:  3.249701 (3.5633)  Time: 0.747s,  171.30/s  (0.752s,  170.26/s)  LR: 9.998e-03  Data: 0.000 (0.004)
Train: 2 [ 300/390 ( 77%)]  Loss:  3.131696 (3.5017)  Time: 0.749s,  170.99/s  (0.751s,  170.43/s)  LR: 9.998e-03  Data: 0.001 (0.003)
Train: 2 [ 350/390 ( 90%)]  Loss:  4.157935 (3.5837)  Time: 0.749s,  170.86/s  (0.751s,  170.55/s)  LR: 9.998e-03  Data: 0.003 (0.003)
Train: 2 [ 389/390 (100%)]  Loss:  4.026782 (3.6329)  Time: 0.746s,  171.47/s  (0.750s,  170.63/s)  LR: 9.998e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.868 (0.868)  DataTime: 0.594 (0.594)  Loss:  2.5859 (2.5859)  Acc@1: 34.3750 (34.3750)  Acc@5: 71.0938 (71.0938)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.014)  Loss:  2.8398 (2.7252)  Acc@1: 29.6875 (30.6526)  Acc@5: 61.7188 (67.4939)
Test: [  78/78]  Time: 0.034 (0.265)  DataTime: 0.000 (0.009)  Loss:  2.9336 (2.7320)  Acc@1: 31.2500 (30.3100)  Acc@5: 68.7500 (66.9700)
tb
loss  0 : 3.68275 2
loss  1 : 2.626553125 2
loss  2 : 3.11128125 2
loss  3 : 2.745128125 2
{'loss_different_0': 3.68275, 'loss_different_1': 2.626553125, 'loss_different_2': 3.11128125, 'loss_different_3': 2.745128125}
Top1:  30.31
Train: 3 [   0/390 (  0%)]  Loss:  3.216341 (3.2163)  Time: 1.218s,  105.12/s  (1.218s,  105.12/s)  LR: 9.994e-03  Data: 0.406 (0.406)
Train: 3 [  50/390 ( 13%)]  Loss:  3.114704 (3.1655)  Time: 0.747s,  171.33/s  (0.756s,  169.24/s)  LR: 9.994e-03  Data: 0.000 (0.008)
Train: 3 [ 100/390 ( 26%)]  Loss:  3.056466 (3.1292)  Time: 0.747s,  171.33/s  (0.752s,  170.25/s)  LR: 9.994e-03  Data: 0.000 (0.004)
Train: 3 [ 150/390 ( 39%)]  Loss:  3.021737 (3.1023)  Time: 0.747s,  171.25/s  (0.752s,  170.12/s)  LR: 9.994e-03  Data: 0.000 (0.003)
Train: 3 [ 200/390 ( 51%)]  Loss:  2.996358 (3.0811)  Time: 0.747s,  171.25/s  (0.751s,  170.41/s)  LR: 9.994e-03  Data: 0.000 (0.002)
Train: 3 [ 250/390 ( 64%)]  Loss:  2.980896 (3.0644)  Time: 0.748s,  171.23/s  (0.750s,  170.59/s)  LR: 9.994e-03  Data: 0.000 (0.002)
Train: 3 [ 300/390 ( 77%)]  Loss:  2.934954 (3.0459)  Time: 0.747s,  171.39/s  (0.750s,  170.71/s)  LR: 9.994e-03  Data: 0.000 (0.002)
Train: 3 [ 350/390 ( 90%)]  Loss:  4.117734 (3.1799)  Time: 0.749s,  170.88/s  (0.749s,  170.80/s)  LR: 9.994e-03  Data: 0.003 (0.002)
Train: 3 [ 389/390 (100%)]  Loss:  2.984432 (3.1582)  Time: 0.748s,  171.19/s  (0.749s,  170.85/s)  LR: 9.994e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.128 (1.128)  DataTime: 0.754 (0.754)  Loss:  2.3184 (2.3184)  Acc@1: 41.4062 (41.4062)  Acc@5: 77.3438 (77.3438)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.017)  Loss:  2.5410 (2.4681)  Acc@1: 33.5938 (36.0141)  Acc@5: 71.8750 (72.4571)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.011)  Loss:  2.7031 (2.4712)  Acc@1: 25.0000 (35.9000)  Acc@5: 62.5000 (72.5200)
tb
loss  0 : 3.29525 3
loss  1 : 2.394925 3
loss  2 : 2.881165625 3
loss  3 : 2.521378125 3
{'loss_different_0': 3.29525, 'loss_different_1': 2.394925, 'loss_different_2': 2.881165625, 'loss_different_3': 2.521378125}
Top1:  35.9
Train: 4 [   0/390 (  0%)]  Loss:  3.414786 (3.4148)  Time: 1.674s,   76.47/s  (1.674s,   76.47/s)  LR: 9.990e-03  Data: 0.925 (0.925)
Train: 4 [  50/390 ( 13%)]  Loss:  2.973228 (3.1940)  Time: 0.747s,  171.25/s  (0.766s,  167.21/s)  LR: 9.990e-03  Data: 0.001 (0.018)
Train: 4 [ 100/390 ( 26%)]  Loss:  2.858885 (3.0823)  Time: 0.748s,  171.08/s  (0.756s,  169.21/s)  LR: 9.990e-03  Data: 0.000 (0.009)
Train: 4 [ 150/390 ( 39%)]  Loss:  2.849151 (3.0240)  Time: 0.748s,  171.22/s  (0.753s,  169.88/s)  LR: 9.990e-03  Data: 0.001 (0.006)
Train: 4 [ 200/390 ( 51%)]  Loss:  4.189023 (3.2570)  Time: 0.747s,  171.25/s  (0.752s,  170.23/s)  LR: 9.990e-03  Data: 0.000 (0.005)
Train: 4 [ 250/390 ( 64%)]  Loss:  2.909592 (3.1991)  Time: 0.747s,  171.33/s  (0.751s,  170.44/s)  LR: 9.990e-03  Data: 0.000 (0.004)
Train: 4 [ 300/390 ( 77%)]  Loss:  2.891787 (3.1552)  Time: 0.747s,  171.37/s  (0.750s,  170.57/s)  LR: 9.990e-03  Data: 0.001 (0.003)
Train: 4 [ 350/390 ( 90%)]  Loss:  2.861311 (3.1185)  Time: 0.751s,  170.51/s  (0.750s,  170.67/s)  LR: 9.990e-03  Data: 0.003 (0.003)
Train: 4 [ 389/390 (100%)]  Loss:  2.973495 (3.1024)  Time: 0.747s,  171.29/s  (0.750s,  170.74/s)  LR: 9.990e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.018 (1.018)  DataTime: 0.731 (0.731)  Loss:  2.2285 (2.2285)  Acc@1: 42.1875 (42.1875)  Acc@5: 77.3438 (77.3438)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.015)  Loss:  2.4004 (2.3179)  Acc@1: 41.4062 (39.3995)  Acc@5: 71.0938 (75.1072)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.010)  Loss:  2.5703 (2.3249)  Acc@1: 31.2500 (39.0300)  Acc@5: 68.7500 (75.0700)
tb
loss  0 : 3.074453125 4
loss  1 : 2.268703125 4
loss  2 : 2.82271875 4
loss  3 : 2.435371875 4
{'loss_different_0': 3.074453125, 'loss_different_1': 2.268703125, 'loss_different_2': 2.82271875, 'loss_different_3': 2.435371875}
Top1:  39.03
Train: 5 [   0/390 (  0%)]  Loss:  2.871338 (2.8713)  Time: 1.394s,   91.80/s  (1.394s,   91.80/s)  LR: 9.985e-03  Data: 0.486 (0.486)
Train: 5 [  50/390 ( 13%)]  Loss:  2.769436 (2.8204)  Time: 0.747s,  171.43/s  (0.760s,  168.42/s)  LR: 9.985e-03  Data: 0.001 (0.010)
Train: 5 [ 100/390 ( 26%)]  Loss:  2.691174 (2.7773)  Time: 0.747s,  171.36/s  (0.756s,  169.25/s)  LR: 9.985e-03  Data: 0.000 (0.005)
Train: 5 [ 150/390 ( 39%)]  Loss:  2.721300 (2.7633)  Time: 0.747s,  171.28/s  (0.753s,  169.90/s)  LR: 9.985e-03  Data: 0.001 (0.004)
Train: 5 [ 200/390 ( 51%)]  Loss:  2.746355 (2.7599)  Time: 0.748s,  171.23/s  (0.752s,  170.24/s)  LR: 9.985e-03  Data: 0.000 (0.003)
Train: 5 [ 250/390 ( 64%)]  Loss:  3.887556 (2.9479)  Time: 0.747s,  171.26/s  (0.751s,  170.45/s)  LR: 9.985e-03  Data: 0.000 (0.002)
Train: 5 [ 300/390 ( 77%)]  Loss:  2.683920 (2.9102)  Time: 0.747s,  171.39/s  (0.750s,  170.58/s)  LR: 9.985e-03  Data: 0.000 (0.002)
Train: 5 [ 350/390 ( 90%)]  Loss:  3.090259 (2.9327)  Time: 0.749s,  170.79/s  (0.750s,  170.68/s)  LR: 9.985e-03  Data: 0.003 (0.002)
Train: 5 [ 389/390 (100%)]  Loss:  2.869459 (2.9256)  Time: 0.747s,  171.38/s  (0.750s,  170.74/s)  LR: 9.985e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.127 (1.127)  DataTime: 0.765 (0.765)  Loss:  2.0156 (2.0156)  Acc@1: 50.0000 (50.0000)  Acc@5: 81.2500 (81.2500)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.015)  Loss:  2.1152 (2.0994)  Acc@1: 47.6562 (45.7721)  Acc@5: 77.3438 (78.7837)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.010)  Loss:  2.3789 (2.1073)  Acc@1: 37.5000 (45.1800)  Acc@5: 56.2500 (78.8300)
tb
loss  0 : 2.8516875 5
loss  1 : 2.092315625 5
loss  2 : 2.58594375 5
loss  3 : 2.293084375 5
{'loss_different_0': 2.8516875, 'loss_different_1': 2.092315625, 'loss_different_2': 2.58594375, 'loss_different_3': 2.293084375}
Top1:  45.18
Train: 6 [   0/390 (  0%)]  Loss:  2.809296 (2.8093)  Time: 1.634s,   78.32/s  (1.634s,   78.32/s)  LR: 9.978e-03  Data: 0.883 (0.883)
Train: 6 [  50/390 ( 13%)]  Loss:  2.640654 (2.7250)  Time: 0.748s,  171.16/s  (0.765s,  167.40/s)  LR: 9.978e-03  Data: 0.000 (0.018)
Train: 6 [ 100/390 ( 26%)]  Loss:  3.633625 (3.0279)  Time: 0.748s,  171.17/s  (0.756s,  169.29/s)  LR: 9.978e-03  Data: 0.000 (0.009)
Train: 6 [ 150/390 ( 39%)]  Loss:  2.810151 (2.9734)  Time: 0.748s,  171.16/s  (0.753s,  169.94/s)  LR: 9.978e-03  Data: 0.000 (0.006)
Train: 6 [ 200/390 ( 51%)]  Loss:  2.507466 (2.8802)  Time: 0.748s,  171.13/s  (0.752s,  170.27/s)  LR: 9.978e-03  Data: 0.000 (0.005)
Train: 6 [ 250/390 ( 64%)]  Loss:  3.934779 (3.0560)  Time: 0.746s,  171.49/s  (0.752s,  170.20/s)  LR: 9.978e-03  Data: 0.000 (0.004)
Train: 6 [ 300/390 ( 77%)]  Loss:  2.576833 (2.9875)  Time: 0.746s,  171.50/s  (0.751s,  170.39/s)  LR: 9.978e-03  Data: 0.000 (0.003)
Train: 6 [ 350/390 ( 90%)]  Loss:  2.607007 (2.9400)  Time: 0.750s,  170.72/s  (0.751s,  170.52/s)  LR: 9.978e-03  Data: 0.003 (0.003)
Train: 6 [ 389/390 (100%)]  Loss:  3.028334 (2.9498)  Time: 0.747s,  171.25/s  (0.750s,  170.61/s)  LR: 9.978e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.958 (0.958)  DataTime: 0.688 (0.688)  Loss:  1.9551 (1.9551)  Acc@1: 48.4375 (48.4375)  Acc@5: 83.5938 (83.5938)
Test: [  50/78]  Time: 0.259 (0.272)  DataTime: 0.000 (0.014)  Loss:  2.0312 (2.0111)  Acc@1: 51.5625 (48.2996)  Acc@5: 75.7812 (80.8824)
Test: [  78/78]  Time: 0.034 (0.265)  DataTime: 0.000 (0.009)  Loss:  2.4492 (2.0105)  Acc@1: 25.0000 (48.1600)  Acc@5: 75.0000 (80.7700)
tb
loss  0 : 2.698328125 6
loss  1 : 2.037240625 6
loss  2 : 2.467115625 6
loss  3 : 2.230215625 6
{'loss_different_0': 2.698328125, 'loss_different_1': 2.037240625, 'loss_different_2': 2.467115625, 'loss_different_3': 2.230215625}
Top1:  48.16
Train: 7 [   0/390 (  0%)]  Loss:  2.699587 (2.6996)  Time: 1.233s,  103.79/s  (1.233s,  103.79/s)  LR: 9.970e-03  Data: 0.431 (0.431)
Train: 7 [  50/390 ( 13%)]  Loss:  2.576900 (2.6382)  Time: 0.748s,  171.19/s  (0.757s,  169.13/s)  LR: 9.970e-03  Data: 0.000 (0.009)
Train: 7 [ 100/390 ( 26%)]  Loss:  2.675452 (2.6506)  Time: 0.747s,  171.34/s  (0.752s,  170.19/s)  LR: 9.970e-03  Data: 0.000 (0.005)
Train: 7 [ 150/390 ( 39%)]  Loss:  2.654595 (2.6516)  Time: 0.748s,  171.20/s  (0.751s,  170.54/s)  LR: 9.970e-03  Data: 0.000 (0.003)
Train: 7 [ 200/390 ( 51%)]  Loss:  2.428422 (2.6070)  Time: 0.747s,  171.34/s  (0.750s,  170.73/s)  LR: 9.970e-03  Data: 0.000 (0.002)
Train: 7 [ 250/390 ( 64%)]  Loss:  2.705553 (2.6234)  Time: 0.747s,  171.32/s  (0.749s,  170.84/s)  LR: 9.970e-03  Data: 0.000 (0.002)
Train: 7 [ 300/390 ( 77%)]  Loss:  2.480345 (2.6030)  Time: 0.748s,  171.20/s  (0.749s,  170.91/s)  LR: 9.970e-03  Data: 0.000 (0.002)
Train: 7 [ 350/390 ( 90%)]  Loss:  2.973597 (2.6493)  Time: 0.749s,  170.84/s  (0.749s,  170.96/s)  LR: 9.970e-03  Data: 0.002 (0.002)
Train: 7 [ 389/390 (100%)]  Loss:  2.378517 (2.6192)  Time: 0.747s,  171.28/s  (0.749s,  171.00/s)  LR: 9.970e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.211 (1.211)  DataTime: 0.945 (0.945)  Loss:  1.8076 (1.8076)  Acc@1: 53.9062 (53.9062)  Acc@5: 86.7188 (86.7188)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.019)  Loss:  1.9648 (1.9232)  Acc@1: 45.3125 (48.6060)  Acc@5: 79.6875 (81.2040)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.012)  Loss:  2.3027 (1.9247)  Acc@1: 37.5000 (48.7200)  Acc@5: 75.0000 (81.2100)
tb
loss  0 : 2.53089375 7
loss  1 : 1.9996125 7
loss  2 : 2.4043375 7
loss  3 : 2.159734375 7
{'loss_different_0': 2.53089375, 'loss_different_1': 1.9996125, 'loss_different_2': 2.4043375, 'loss_different_3': 2.159734375}
Top1:  48.72
Train: 8 [   0/390 (  0%)]  Loss:  2.781858 (2.7819)  Time: 1.608s,   79.60/s  (1.608s,   79.60/s)  LR: 9.961e-03  Data: 0.859 (0.859)
Train: 8 [  50/390 ( 13%)]  Loss:  2.502628 (2.6422)  Time: 0.747s,  171.35/s  (0.764s,  167.51/s)  LR: 9.961e-03  Data: 0.000 (0.017)
Train: 8 [ 100/390 ( 26%)]  Loss:  2.387903 (2.5575)  Time: 0.747s,  171.34/s  (0.756s,  169.38/s)  LR: 9.961e-03  Data: 0.000 (0.009)
Train: 8 [ 150/390 ( 39%)]  Loss:  3.906118 (2.8946)  Time: 0.747s,  171.27/s  (0.754s,  169.67/s)  LR: 9.961e-03  Data: 0.000 (0.006)
Train: 8 [ 200/390 ( 51%)]  Loss:  2.309142 (2.7775)  Time: 0.748s,  171.15/s  (0.753s,  170.07/s)  LR: 9.961e-03  Data: 0.000 (0.005)
Train: 8 [ 250/390 ( 64%)]  Loss:  2.328290 (2.7027)  Time: 0.747s,  171.35/s  (0.752s,  170.32/s)  LR: 9.961e-03  Data: 0.000 (0.004)
Train: 8 [ 300/390 ( 77%)]  Loss:  2.399053 (2.6593)  Time: 0.747s,  171.30/s  (0.751s,  170.49/s)  LR: 9.961e-03  Data: 0.000 (0.003)
Train: 8 [ 350/390 ( 90%)]  Loss:  2.450946 (2.6332)  Time: 0.749s,  170.86/s  (0.750s,  170.60/s)  LR: 9.961e-03  Data: 0.003 (0.003)
Train: 8 [ 389/390 (100%)]  Loss:  2.606851 (2.6303)  Time: 0.747s,  171.35/s  (0.750s,  170.67/s)  LR: 9.961e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.045 (1.045)  DataTime: 0.762 (0.762)  Loss:  1.7119 (1.7119)  Acc@1: 57.8125 (57.8125)  Acc@5: 85.1562 (85.1562)
Test: [  50/78]  Time: 0.258 (0.274)  DataTime: 0.000 (0.015)  Loss:  1.8008 (1.7895)  Acc@1: 50.7812 (53.6918)  Acc@5: 82.8125 (83.8542)
Test: [  78/78]  Time: 0.034 (0.266)  DataTime: 0.000 (0.010)  Loss:  2.0586 (1.7944)  Acc@1: 43.7500 (53.3100)  Acc@5: 87.5000 (83.6800)
tb
loss  0 : 2.475975 8
loss  1 : 1.860690625 8
loss  2 : 2.255884375 8
loss  3 : 2.02658125 8
{'loss_different_0': 2.475975, 'loss_different_1': 1.860690625, 'loss_different_2': 2.255884375, 'loss_different_3': 2.02658125}
Top1:  53.31
Train: 9 [   0/390 (  0%)]  Loss:  2.322640 (2.3226)  Time: 1.082s,  118.32/s  (1.082s,  118.32/s)  LR: 9.950e-03  Data: 0.297 (0.297)
Train: 9 [  50/390 ( 13%)]  Loss:  3.323617 (2.8231)  Time: 0.747s,  171.31/s  (0.754s,  169.83/s)  LR: 9.950e-03  Data: 0.000 (0.006)
Train: 9 [ 100/390 ( 26%)]  Loss:  2.256464 (2.6342)  Time: 0.748s,  171.18/s  (0.750s,  170.58/s)  LR: 9.950e-03  Data: 0.000 (0.003)
Train: 9 [ 150/390 ( 39%)]  Loss:  2.403413 (2.5765)  Time: 0.747s,  171.36/s  (0.749s,  170.84/s)  LR: 9.950e-03  Data: 0.000 (0.002)
Train: 9 [ 200/390 ( 51%)]  Loss:  3.881454 (2.8375)  Time: 0.748s,  171.20/s  (0.749s,  170.96/s)  LR: 9.950e-03  Data: 0.000 (0.002)
Train: 9 [ 250/390 ( 64%)]  Loss:  2.404346 (2.7653)  Time: 0.746s,  171.49/s  (0.748s,  171.04/s)  LR: 9.950e-03  Data: 0.000 (0.002)
Train: 9 [ 300/390 ( 77%)]  Loss:  2.315668 (2.7011)  Time: 0.747s,  171.35/s  (0.749s,  170.85/s)  LR: 9.950e-03  Data: 0.000 (0.001)
Train: 9 [ 350/390 ( 90%)]  Loss:  2.377383 (2.6606)  Time: 0.749s,  170.85/s  (0.749s,  170.92/s)  LR: 9.950e-03  Data: 0.003 (0.001)
Train: 9 [ 389/390 (100%)]  Loss:  2.102804 (2.5986)  Time: 0.747s,  171.38/s  (0.749s,  170.97/s)  LR: 9.950e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 0.994 (0.994)  DataTime: 0.669 (0.669)  Loss:  1.7354 (1.7354)  Acc@1: 53.9062 (53.9062)  Acc@5: 82.8125 (82.8125)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.013)  Loss:  1.7881 (1.7467)  Acc@1: 54.6875 (54.7947)  Acc@5: 85.1562 (84.7120)
Test: [  78/78]  Time: 0.034 (0.265)  DataTime: 0.000 (0.009)  Loss:  2.1289 (1.7498)  Acc@1: 56.2500 (54.9700)  Acc@5: 68.7500 (84.5300)
tb
loss  0 : 2.43189375 9
loss  1 : 1.820896875 9
loss  2 : 2.247865625 9
loss  3 : 1.974496875 9
{'loss_different_0': 2.43189375, 'loss_different_1': 1.820896875, 'loss_different_2': 2.247865625, 'loss_different_3': 1.974496875}
Top1:  54.97
Train: 10 [   0/390 (  0%)]  Loss:  2.239973 (2.2400)  Time: 1.421s,   90.08/s  (1.421s,   90.08/s)  LR: 9.938e-03  Data: 0.611 (0.611)
Train: 10 [  50/390 ( 13%)]  Loss:  3.258571 (2.7493)  Time: 0.747s,  171.25/s  (0.760s,  168.36/s)  LR: 9.938e-03  Data: 0.000 (0.012)
Train: 10 [ 100/390 ( 26%)]  Loss:  2.771416 (2.7567)  Time: 0.747s,  171.38/s  (0.754s,  169.83/s)  LR: 9.938e-03  Data: 0.000 (0.006)
Train: 10 [ 150/390 ( 39%)]  Loss:  2.374655 (2.6612)  Time: 0.747s,  171.35/s  (0.751s,  170.33/s)  LR: 9.938e-03  Data: 0.000 (0.004)
Train: 10 [ 200/390 ( 51%)]  Loss:  3.108807 (2.7507)  Time: 0.747s,  171.25/s  (0.750s,  170.59/s)  LR: 9.938e-03  Data: 0.000 (0.003)
Train: 10 [ 250/390 ( 64%)]  Loss:  2.188995 (2.6571)  Time: 0.747s,  171.31/s  (0.750s,  170.74/s)  LR: 9.938e-03  Data: 0.000 (0.003)
Train: 10 [ 300/390 ( 77%)]  Loss:  2.123304 (2.5808)  Time: 0.747s,  171.28/s  (0.749s,  170.83/s)  LR: 9.938e-03  Data: 0.000 (0.002)
Train: 10 [ 350/390 ( 90%)]  Loss:  2.478096 (2.5680)  Time: 0.749s,  170.79/s  (0.749s,  170.90/s)  LR: 9.938e-03  Data: 0.003 (0.002)
Train: 10 [ 389/390 (100%)]  Loss:  2.314396 (2.5398)  Time: 0.747s,  171.35/s  (0.749s,  170.95/s)  LR: 9.938e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.152 (1.152)  DataTime: 0.883 (0.883)  Loss:  1.5918 (1.5918)  Acc@1: 59.3750 (59.3750)  Acc@5: 82.8125 (82.8125)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.018)  Loss:  1.7275 (1.6378)  Acc@1: 52.3438 (56.6942)  Acc@5: 85.9375 (85.8150)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.8652 (1.6466)  Acc@1: 62.5000 (56.4300)  Acc@5: 75.0000 (85.5000)
tb
loss  0 : 2.339159375 10
loss  1 : 1.742746875 10
loss  2 : 2.128534375 10
loss  3 : 1.905709375 10
{'loss_different_0': 2.339159375, 'loss_different_1': 1.742746875, 'loss_different_2': 2.128534375, 'loss_different_3': 1.905709375}
Top1:  56.43
Saving model to ./output/train/20250829-042433-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_2/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_2_10.pt
Train: 11 [   0/390 (  0%)]  Loss:  2.288083 (2.2881)  Time: 1.588s,   80.61/s  (1.588s,   80.61/s)  LR: 9.926e-03  Data: 0.839 (0.839)
Train: 11 [  50/390 ( 13%)]  Loss:  2.274806 (2.2814)  Time: 0.747s,  171.39/s  (0.763s,  167.65/s)  LR: 9.926e-03  Data: 0.000 (0.017)
Train: 11 [ 100/390 ( 26%)]  Loss:  2.302032 (2.2883)  Time: 0.747s,  171.34/s  (0.755s,  169.48/s)  LR: 9.926e-03  Data: 0.000 (0.009)
Train: 11 [ 150/390 ( 39%)]  Loss:  2.270590 (2.2839)  Time: 0.747s,  171.24/s  (0.753s,  170.10/s)  LR: 9.926e-03  Data: 0.000 (0.006)
Train: 11 [ 200/390 ( 51%)]  Loss:  3.271541 (2.4814)  Time: 0.747s,  171.27/s  (0.752s,  170.15/s)  LR: 9.926e-03  Data: 0.000 (0.005)
Train: 11 [ 250/390 ( 64%)]  Loss:  3.171887 (2.5965)  Time: 0.746s,  171.55/s  (0.751s,  170.38/s)  LR: 9.926e-03  Data: 0.000 (0.004)
Train: 11 [ 300/390 ( 77%)]  Loss:  2.144467 (2.5319)  Time: 0.747s,  171.34/s  (0.751s,  170.54/s)  LR: 9.926e-03  Data: 0.000 (0.003)
Train: 11 [ 350/390 ( 90%)]  Loss:  2.280980 (2.5005)  Time: 0.750s,  170.73/s  (0.750s,  170.65/s)  LR: 9.926e-03  Data: 0.003 (0.003)
Train: 11 [ 389/390 (100%)]  Loss:  3.796768 (2.6446)  Time: 0.746s,  171.49/s  (0.750s,  170.72/s)  LR: 9.926e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.321 (1.321)  DataTime: 1.051 (1.051)  Loss:  1.4746 (1.4746)  Acc@1: 58.5938 (58.5938)  Acc@5: 89.0625 (89.0625)
Test: [  50/78]  Time: 0.258 (0.279)  DataTime: 0.000 (0.021)  Loss:  1.6631 (1.6203)  Acc@1: 54.6875 (57.4142)  Acc@5: 85.9375 (86.7647)
Test: [  78/78]  Time: 0.034 (0.269)  DataTime: 0.000 (0.014)  Loss:  1.8633 (1.6314)  Acc@1: 50.0000 (57.1900)  Acc@5: 87.5000 (86.2000)
tb
loss  0 : 2.2885 11
loss  1 : 1.7462375 11
loss  2 : 2.105890625 11
loss  3 : 1.9106625 11
{'loss_different_0': 2.2885, 'loss_different_1': 1.7462375, 'loss_different_2': 2.105890625, 'loss_different_3': 1.9106625}
Top1:  57.19
Train: 12 [   0/390 (  0%)]  Loss:  2.290018 (2.2900)  Time: 1.101s,  116.23/s  (1.101s,  116.23/s)  LR: 9.911e-03  Data: 0.265 (0.265)
Train: 12 [  50/390 ( 13%)]  Loss:  2.433995 (2.3620)  Time: 0.748s,  171.21/s  (0.754s,  169.71/s)  LR: 9.911e-03  Data: 0.000 (0.006)
Train: 12 [ 100/390 ( 26%)]  Loss:  2.391796 (2.3719)  Time: 0.747s,  171.40/s  (0.751s,  170.49/s)  LR: 9.911e-03  Data: 0.000 (0.003)
Train: 12 [ 150/390 ( 39%)]  Loss:  3.210882 (2.5817)  Time: 0.748s,  171.09/s  (0.750s,  170.76/s)  LR: 9.911e-03  Data: 0.000 (0.002)
Train: 12 [ 200/390 ( 51%)]  Loss:  2.178269 (2.5010)  Time: 0.748s,  171.15/s  (0.749s,  170.89/s)  LR: 9.911e-03  Data: 0.000 (0.002)
Train: 12 [ 250/390 ( 64%)]  Loss:  2.172205 (2.4462)  Time: 0.748s,  171.08/s  (0.749s,  170.97/s)  LR: 9.911e-03  Data: 0.000 (0.001)
Train: 12 [ 300/390 ( 77%)]  Loss:  2.283306 (2.4229)  Time: 0.747s,  171.24/s  (0.748s,  171.01/s)  LR: 9.911e-03  Data: 0.000 (0.001)
Train: 12 [ 350/390 ( 90%)]  Loss:  2.263689 (2.4030)  Time: 0.749s,  170.84/s  (0.749s,  170.82/s)  LR: 9.911e-03  Data: 0.003 (0.001)
Train: 12 [ 389/390 (100%)]  Loss:  2.256644 (2.3868)  Time: 0.747s,  171.46/s  (0.749s,  170.87/s)  LR: 9.911e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 0.882 (0.882)  DataTime: 0.617 (0.617)  Loss:  1.4570 (1.4570)  Acc@1: 60.9375 (60.9375)  Acc@5: 86.7188 (86.7188)
Test: [  50/78]  Time: 0.259 (0.275)  DataTime: 0.000 (0.017)  Loss:  1.7188 (1.6006)  Acc@1: 58.5938 (58.1189)  Acc@5: 85.9375 (86.5962)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.011)  Loss:  1.5391 (1.6099)  Acc@1: 56.2500 (57.7700)  Acc@5: 81.2500 (86.5300)
tb
loss  0 : 2.2273578125 12
loss  1 : 1.73826875 12
loss  2 : 2.03999375 12
loss  3 : 1.9054703125 12
{'loss_different_0': 2.2273578125, 'loss_different_1': 1.73826875, 'loss_different_2': 2.03999375, 'loss_different_3': 1.9054703125}
Top1:  57.77
Train: 13 [   0/390 (  0%)]  Loss:  2.398195 (2.3982)  Time: 1.089s,  117.57/s  (1.089s,  117.57/s)  LR: 9.896e-03  Data: 0.314 (0.314)
Train: 13 [  50/390 ( 13%)]  Loss:  2.073808 (2.2360)  Time: 0.747s,  171.41/s  (0.754s,  169.75/s)  LR: 9.896e-03  Data: 0.000 (0.007)
Train: 13 [ 100/390 ( 26%)]  Loss:  2.090330 (2.1874)  Time: 0.748s,  171.20/s  (0.751s,  170.50/s)  LR: 9.896e-03  Data: 0.001 (0.003)
Train: 13 [ 150/390 ( 39%)]  Loss:  2.182112 (2.1861)  Time: 0.748s,  171.21/s  (0.750s,  170.75/s)  LR: 9.896e-03  Data: 0.000 (0.002)
Train: 13 [ 200/390 ( 51%)]  Loss:  3.170652 (2.3830)  Time: 0.747s,  171.28/s  (0.749s,  170.87/s)  LR: 9.896e-03  Data: 0.000 (0.002)
Train: 13 [ 250/390 ( 64%)]  Loss:  2.429259 (2.3907)  Time: 0.748s,  171.14/s  (0.749s,  170.95/s)  LR: 9.896e-03  Data: 0.000 (0.002)
Train: 13 [ 300/390 ( 77%)]  Loss:  1.922627 (2.3239)  Time: 0.748s,  171.17/s  (0.749s,  171.00/s)  LR: 9.896e-03  Data: 0.000 (0.001)
Train: 13 [ 350/390 ( 90%)]  Loss:  2.071737 (2.2923)  Time: 0.749s,  170.83/s  (0.748s,  171.03/s)  LR: 9.896e-03  Data: 0.003 (0.001)
Train: 13 [ 389/390 (100%)]  Loss:  2.211717 (2.2834)  Time: 0.747s,  171.35/s  (0.748s,  171.06/s)  LR: 9.896e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.173 (1.173)  DataTime: 0.835 (0.835)  Loss:  1.3301 (1.3301)  Acc@1: 66.4062 (66.4062)  Acc@5: 88.2812 (88.2812)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.017)  Loss:  1.6475 (1.5004)  Acc@1: 59.3750 (59.5435)  Acc@5: 85.1562 (87.4081)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.5381 (1.5117)  Acc@1: 62.5000 (59.2500)  Acc@5: 81.2500 (87.3600)
tb
loss  0 : 2.158034375 13
loss  1 : 1.6765328125 13
loss  2 : 1.9633546875 13
loss  3 : 1.8385875 13
{'loss_different_0': 2.158034375, 'loss_different_1': 1.6765328125, 'loss_different_2': 1.9633546875, 'loss_different_3': 1.8385875}
Top1:  59.25
Train: 14 [   0/390 (  0%)]  Loss:  1.896589 (1.8966)  Time: 1.670s,   76.63/s  (1.670s,   76.63/s)  LR: 9.880e-03  Data: 0.923 (0.923)
Train: 14 [  50/390 ( 13%)]  Loss:  2.214252 (2.0554)  Time: 0.748s,  171.21/s  (0.765s,  167.24/s)  LR: 9.880e-03  Data: 0.000 (0.018)
Train: 14 [ 100/390 ( 26%)]  Loss:  2.007328 (2.0394)  Time: 0.746s,  171.65/s  (0.756s,  169.23/s)  LR: 9.880e-03  Data: 0.000 (0.009)
Train: 14 [ 150/390 ( 39%)]  Loss:  2.089852 (2.0520)  Time: 0.747s,  171.27/s  (0.756s,  169.40/s)  LR: 9.880e-03  Data: 0.000 (0.006)
Train: 14 [ 200/390 ( 51%)]  Loss:  2.338473 (2.1093)  Time: 0.747s,  171.36/s  (0.754s,  169.86/s)  LR: 9.880e-03  Data: 0.000 (0.005)
Train: 14 [ 250/390 ( 64%)]  Loss:  4.064178 (2.4351)  Time: 0.747s,  171.25/s  (0.752s,  170.15/s)  LR: 9.880e-03  Data: 0.000 (0.004)
Train: 14 [ 300/390 ( 77%)]  Loss:  1.886504 (2.3567)  Time: 0.747s,  171.32/s  (0.751s,  170.34/s)  LR: 9.880e-03  Data: 0.000 (0.003)
Train: 14 [ 350/390 ( 90%)]  Loss:  2.049459 (2.3183)  Time: 0.749s,  170.83/s  (0.751s,  170.48/s)  LR: 9.880e-03  Data: 0.003 (0.003)
Train: 14 [ 389/390 (100%)]  Loss:  2.268859 (2.3128)  Time: 0.747s,  171.37/s  (0.750s,  170.57/s)  LR: 9.880e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.953 (0.953)  DataTime: 0.671 (0.671)  Loss:  1.3555 (1.3555)  Acc@1: 64.8438 (64.8438)  Acc@5: 86.7188 (86.7188)
Test: [  50/78]  Time: 0.258 (0.273)  DataTime: 0.000 (0.014)  Loss:  1.6514 (1.4690)  Acc@1: 56.2500 (61.0141)  Acc@5: 85.1562 (88.6183)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  1.4648 (1.4656)  Acc@1: 62.5000 (61.2200)  Acc@5: 87.5000 (88.4900)
tb
loss  0 : 2.0679984375 14
loss  1 : 1.6097625 14
loss  2 : 1.89520625 14
loss  3 : 1.766678125 14
{'loss_different_0': 2.0679984375, 'loss_different_1': 1.6097625, 'loss_different_2': 1.89520625, 'loss_different_3': 1.766678125}
Top1:  61.22
Train: 15 [   0/390 (  0%)]  Loss:  1.903322 (1.9033)  Time: 1.644s,   77.86/s  (1.644s,   77.86/s)  LR: 9.862e-03  Data: 0.889 (0.889)
Train: 15 [  50/390 ( 13%)]  Loss:  2.570490 (2.2369)  Time: 0.748s,  171.05/s  (0.765s,  167.36/s)  LR: 9.862e-03  Data: 0.000 (0.018)
Train: 15 [ 100/390 ( 26%)]  Loss:  2.455390 (2.3097)  Time: 0.747s,  171.33/s  (0.756s,  169.30/s)  LR: 9.862e-03  Data: 0.000 (0.009)
Train: 15 [ 150/390 ( 39%)]  Loss:  2.063619 (2.2482)  Time: 0.748s,  171.22/s  (0.753s,  169.97/s)  LR: 9.862e-03  Data: 0.000 (0.006)
Train: 15 [ 200/390 ( 51%)]  Loss:  3.818682 (2.5623)  Time: 0.747s,  171.32/s  (0.752s,  170.30/s)  LR: 9.862e-03  Data: 0.000 (0.005)
Train: 15 [ 250/390 ( 64%)]  Loss:  2.095097 (2.4844)  Time: 0.747s,  171.37/s  (0.751s,  170.50/s)  LR: 9.862e-03  Data: 0.000 (0.004)
Train: 15 [ 300/390 ( 77%)]  Loss:  2.172955 (2.4399)  Time: 0.747s,  171.32/s  (0.750s,  170.64/s)  LR: 9.862e-03  Data: 0.000 (0.003)
Train: 15 [ 350/390 ( 90%)]  Loss:  2.011569 (2.3864)  Time: 0.749s,  170.80/s  (0.750s,  170.73/s)  LR: 9.862e-03  Data: 0.003 (0.003)
Train: 15 [ 389/390 (100%)]  Loss:  2.119890 (2.3568)  Time: 0.747s,  171.46/s  (0.749s,  170.79/s)  LR: 9.862e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.199 (1.199)  DataTime: 0.913 (0.913)  Loss:  1.3936 (1.3936)  Acc@1: 61.7188 (61.7188)  Acc@5: 87.5000 (87.5000)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.018)  Loss:  1.5244 (1.5246)  Acc@1: 63.2812 (60.3094)  Acc@5: 86.7188 (87.9749)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.3789 (1.5225)  Acc@1: 68.7500 (60.6900)  Acc@5: 93.7500 (87.8800)
tb
loss  0 : 2.1801328125 15
loss  1 : 1.64981875 15
loss  2 : 1.959878125 15
loss  3 : 1.81965 15
{'loss_different_0': 2.1801328125, 'loss_different_1': 1.64981875, 'loss_different_2': 1.959878125, 'loss_different_3': 1.81965}
Top1:  60.69
Train: 16 [   0/390 (  0%)]  Loss:  2.258723 (2.2587)  Time: 1.613s,   79.38/s  (1.613s,   79.38/s)  LR: 9.843e-03  Data: 0.864 (0.864)
Train: 16 [  50/390 ( 13%)]  Loss:  2.073700 (2.1662)  Time: 0.747s,  171.36/s  (0.768s,  166.60/s)  LR: 9.843e-03  Data: 0.000 (0.017)
Train: 16 [ 100/390 ( 26%)]  Loss:  1.976336 (2.1029)  Time: 0.747s,  171.46/s  (0.758s,  168.92/s)  LR: 9.843e-03  Data: 0.000 (0.009)
Train: 16 [ 150/390 ( 39%)]  Loss:  1.944366 (2.0633)  Time: 0.747s,  171.28/s  (0.754s,  169.69/s)  LR: 9.843e-03  Data: 0.000 (0.006)
Train: 16 [ 200/390 ( 51%)]  Loss:  2.011591 (2.0529)  Time: 0.746s,  171.58/s  (0.753s,  170.09/s)  LR: 9.843e-03  Data: 0.000 (0.005)
Train: 16 [ 250/390 ( 64%)]  Loss:  1.921120 (2.0310)  Time: 0.747s,  171.26/s  (0.751s,  170.33/s)  LR: 9.843e-03  Data: 0.000 (0.004)
Train: 16 [ 300/390 ( 77%)]  Loss:  2.111676 (2.0425)  Time: 0.748s,  171.23/s  (0.751s,  170.49/s)  LR: 9.843e-03  Data: 0.001 (0.003)
Train: 16 [ 350/390 ( 90%)]  Loss:  2.911676 (2.1511)  Time: 0.750s,  170.71/s  (0.750s,  170.61/s)  LR: 9.843e-03  Data: 0.003 (0.003)
Train: 16 [ 389/390 (100%)]  Loss:  1.909909 (2.1243)  Time: 0.747s,  171.44/s  (0.750s,  170.69/s)  LR: 9.843e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.915 (0.915)  DataTime: 0.640 (0.640)  Loss:  1.3340 (1.3340)  Acc@1: 64.8438 (64.8438)  Acc@5: 88.2812 (88.2812)
Test: [  50/78]  Time: 0.258 (0.272)  DataTime: 0.000 (0.013)  Loss:  1.4951 (1.4363)  Acc@1: 59.3750 (61.9638)  Acc@5: 89.0625 (88.8174)
Test: [  78/78]  Time: 0.035 (0.264)  DataTime: 0.000 (0.008)  Loss:  1.4072 (1.4416)  Acc@1: 56.2500 (62.1300)  Acc@5: 93.7500 (88.5900)
tb
loss  0 : 2.1651953125 16
loss  1 : 1.6228890625 16
loss  2 : 1.892215625 16
loss  3 : 1.73903125 16
{'loss_different_0': 2.1651953125, 'loss_different_1': 1.6228890625, 'loss_different_2': 1.892215625, 'loss_different_3': 1.73903125}
Top1:  62.13
Train: 17 [   0/390 (  0%)]  Loss:  1.804878 (1.8049)  Time: 1.663s,   76.97/s  (1.663s,   76.97/s)  LR: 9.823e-03  Data: 0.914 (0.914)
Train: 17 [  50/390 ( 13%)]  Loss:  2.427639 (2.1163)  Time: 0.747s,  171.25/s  (0.765s,  167.26/s)  LR: 9.823e-03  Data: 0.000 (0.018)
Train: 17 [ 100/390 ( 26%)]  Loss:  2.117073 (2.1165)  Time: 0.747s,  171.24/s  (0.756s,  169.21/s)  LR: 9.823e-03  Data: 0.000 (0.009)
Train: 17 [ 150/390 ( 39%)]  Loss:  3.833390 (2.5457)  Time: 0.747s,  171.32/s  (0.753s,  169.89/s)  LR: 9.823e-03  Data: 0.001 (0.006)
Train: 17 [ 200/390 ( 51%)]  Loss:  1.931179 (2.4228)  Time: 0.748s,  171.09/s  (0.754s,  169.87/s)  LR: 9.823e-03  Data: 0.000 (0.005)
Train: 17 [ 250/390 ( 64%)]  Loss:  1.892482 (2.3344)  Time: 0.748s,  171.17/s  (0.752s,  170.15/s)  LR: 9.823e-03  Data: 0.000 (0.004)
Train: 17 [ 300/390 ( 77%)]  Loss:  1.707052 (2.2448)  Time: 0.747s,  171.35/s  (0.751s,  170.33/s)  LR: 9.823e-03  Data: 0.000 (0.003)
Train: 17 [ 350/390 ( 90%)]  Loss:  2.093273 (2.2259)  Time: 0.749s,  170.84/s  (0.751s,  170.46/s)  LR: 9.823e-03  Data: 0.003 (0.003)
Train: 17 [ 389/390 (100%)]  Loss:  2.054899 (2.2069)  Time: 0.747s,  171.43/s  (0.751s,  170.55/s)  LR: 9.823e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.987 (0.987)  DataTime: 0.675 (0.675)  Loss:  1.3799 (1.3799)  Acc@1: 60.9375 (60.9375)  Acc@5: 89.8438 (89.8438)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.014)  Loss:  1.5420 (1.4242)  Acc@1: 58.5938 (61.7341)  Acc@5: 84.3750 (89.2157)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  1.1904 (1.4165)  Acc@1: 62.5000 (62.5500)  Acc@5: 93.7500 (89.2100)
tb
loss  0 : 2.1393171875 17
loss  1 : 1.591328125 17
loss  2 : 1.8540640625 17
loss  3 : 1.7649984375 17
{'loss_different_0': 2.1393171875, 'loss_different_1': 1.591328125, 'loss_different_2': 1.8540640625, 'loss_different_3': 1.7649984375}
Top1:  62.55
Train: 18 [   0/390 (  0%)]  Loss:  1.828265 (1.8283)  Time: 1.613s,   79.35/s  (1.613s,   79.35/s)  LR: 9.801e-03  Data: 0.865 (0.865)
Train: 18 [  50/390 ( 13%)]  Loss:  2.031496 (1.9299)  Time: 0.748s,  171.16/s  (0.764s,  167.43/s)  LR: 9.801e-03  Data: 0.000 (0.017)
Train: 18 [ 100/390 ( 26%)]  Loss:  2.010271 (1.9567)  Time: 0.747s,  171.33/s  (0.756s,  169.32/s)  LR: 9.801e-03  Data: 0.000 (0.009)
Train: 18 [ 150/390 ( 39%)]  Loss:  1.807030 (1.9193)  Time: 0.748s,  171.22/s  (0.753s,  169.95/s)  LR: 9.801e-03  Data: 0.000 (0.006)
Train: 18 [ 200/390 ( 51%)]  Loss:  3.799501 (2.2953)  Time: 0.747s,  171.34/s  (0.752s,  170.27/s)  LR: 9.801e-03  Data: 0.000 (0.005)
Train: 18 [ 250/390 ( 64%)]  Loss:  1.796781 (2.2122)  Time: 0.747s,  171.29/s  (0.751s,  170.46/s)  LR: 9.801e-03  Data: 0.000 (0.004)
Train: 18 [ 300/390 ( 77%)]  Loss:  1.714740 (2.1412)  Time: 0.748s,  171.09/s  (0.750s,  170.60/s)  LR: 9.801e-03  Data: 0.000 (0.003)
Train: 18 [ 350/390 ( 90%)]  Loss:  1.870609 (2.1073)  Time: 0.749s,  170.85/s  (0.750s,  170.69/s)  LR: 9.801e-03  Data: 0.003 (0.003)
Train: 18 [ 389/390 (100%)]  Loss:  1.776912 (2.0706)  Time: 0.747s,  171.39/s  (0.750s,  170.75/s)  LR: 9.801e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.159 (1.159)  DataTime: 0.897 (0.897)  Loss:  1.2383 (1.2383)  Acc@1: 64.8438 (64.8438)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.018)  Loss:  1.5205 (1.4041)  Acc@1: 62.5000 (63.7408)  Acc@5: 86.7188 (89.4455)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.012)  Loss:  1.6318 (1.4090)  Acc@1: 50.0000 (63.6200)  Acc@5: 81.2500 (89.0700)
tb
loss  0 : 2.10610625 18
loss  1 : 1.5544921875 18
loss  2 : 1.902803125 18
loss  3 : 1.7148703125 18
{'loss_different_0': 2.10610625, 'loss_different_1': 1.5544921875, 'loss_different_2': 1.902803125, 'loss_different_3': 1.7148703125}
Top1:  63.62
Train: 19 [   0/390 (  0%)]  Loss:  1.866793 (1.8668)  Time: 1.248s,  102.58/s  (1.248s,  102.58/s)  LR: 9.779e-03  Data: 0.452 (0.452)
Train: 19 [  50/390 ( 13%)]  Loss:  1.659344 (1.7631)  Time: 0.748s,  171.19/s  (0.757s,  169.05/s)  LR: 9.779e-03  Data: 0.000 (0.009)
Train: 19 [ 100/390 ( 26%)]  Loss:  1.953927 (1.8267)  Time: 0.747s,  171.29/s  (0.755s,  169.62/s)  LR: 9.779e-03  Data: 0.000 (0.005)
Train: 19 [ 150/390 ( 39%)]  Loss:  1.723448 (1.8009)  Time: 0.748s,  171.15/s  (0.752s,  170.18/s)  LR: 9.779e-03  Data: 0.000 (0.003)
Train: 19 [ 200/390 ( 51%)]  Loss:  1.902566 (1.8212)  Time: 0.747s,  171.36/s  (0.751s,  170.45/s)  LR: 9.779e-03  Data: 0.000 (0.003)
Train: 19 [ 250/390 ( 64%)]  Loss:  1.752987 (1.8098)  Time: 0.747s,  171.26/s  (0.750s,  170.62/s)  LR: 9.779e-03  Data: 0.000 (0.002)
Train: 19 [ 300/390 ( 77%)]  Loss:  2.618702 (1.9254)  Time: 0.747s,  171.30/s  (0.750s,  170.73/s)  LR: 9.779e-03  Data: 0.000 (0.002)
Train: 19 [ 350/390 ( 90%)]  Loss:  1.788257 (1.9083)  Time: 0.750s,  170.58/s  (0.749s,  170.81/s)  LR: 9.779e-03  Data: 0.003 (0.002)
Train: 19 [ 389/390 (100%)]  Loss:  2.130238 (1.9329)  Time: 0.746s,  171.47/s  (0.749s,  170.87/s)  LR: 9.779e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.403 (1.403)  DataTime: 1.121 (1.121)  Loss:  1.2354 (1.2354)  Acc@1: 67.1875 (67.1875)  Acc@5: 90.6250 (90.6250)
Test: [  50/78]  Time: 0.258 (0.281)  DataTime: 0.000 (0.022)  Loss:  1.5195 (1.3896)  Acc@1: 61.7188 (63.8327)  Acc@5: 86.7188 (89.7059)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.014)  Loss:  1.5713 (1.3974)  Acc@1: 56.2500 (63.6800)  Acc@5: 87.5000 (89.3500)
tb
loss  0 : 2.111453125 19
loss  1 : 1.559696875 19
loss  2 : 1.8611484375 19
loss  3 : 1.74723125 19
{'loss_different_0': 2.111453125, 'loss_different_1': 1.559696875, 'loss_different_2': 1.8611484375, 'loss_different_3': 1.74723125}
Top1:  63.68
Train: 20 [   0/390 (  0%)]  Loss:  1.851418 (1.8514)  Time: 1.344s,   95.22/s  (1.344s,   95.22/s)  LR: 9.755e-03  Data: 0.527 (0.527)
Train: 20 [  50/390 ( 13%)]  Loss:  2.526721 (2.1891)  Time: 0.747s,  171.39/s  (0.759s,  168.68/s)  LR: 9.755e-03  Data: 0.000 (0.011)
Train: 20 [ 100/390 ( 26%)]  Loss:  3.858171 (2.7454)  Time: 0.748s,  171.12/s  (0.753s,  169.98/s)  LR: 9.755e-03  Data: 0.000 (0.006)
Train: 20 [ 150/390 ( 39%)]  Loss:  1.867816 (2.5260)  Time: 0.747s,  171.45/s  (0.751s,  170.42/s)  LR: 9.755e-03  Data: 0.000 (0.004)
Train: 20 [ 200/390 ( 51%)]  Loss:  3.825142 (2.7859)  Time: 0.746s,  171.52/s  (0.750s,  170.64/s)  LR: 9.755e-03  Data: 0.000 (0.003)
Train: 20 [ 250/390 ( 64%)]  Loss:  2.271376 (2.7001)  Time: 0.747s,  171.45/s  (0.751s,  170.48/s)  LR: 9.755e-03  Data: 0.000 (0.002)
Train: 20 [ 300/390 ( 77%)]  Loss:  2.538167 (2.6770)  Time: 0.747s,  171.26/s  (0.750s,  170.61/s)  LR: 9.755e-03  Data: 0.000 (0.002)
Train: 20 [ 350/390 ( 90%)]  Loss:  1.878061 (2.5771)  Time: 0.750s,  170.77/s  (0.750s,  170.71/s)  LR: 9.755e-03  Data: 0.003 (0.002)
Train: 20 [ 389/390 (100%)]  Loss:  1.820216 (2.4930)  Time: 0.746s,  171.60/s  (0.750s,  170.78/s)  LR: 9.755e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.224 (1.224)  DataTime: 0.936 (0.936)  Loss:  1.2070 (1.2070)  Acc@1: 66.4062 (66.4062)  Acc@5: 92.9688 (92.9688)
Test: [  50/78]  Time: 0.259 (0.278)  DataTime: 0.000 (0.019)  Loss:  1.5000 (1.3628)  Acc@1: 60.1562 (64.4761)  Acc@5: 90.6250 (89.8438)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.4170 (1.3558)  Acc@1: 68.7500 (64.9700)  Acc@5: 93.7500 (89.7900)
tb
loss  0 : 2.054734375 20
loss  1 : 1.543190625 20
loss  2 : 1.76743125 20
loss  3 : 1.716246875 20
{'loss_different_0': 2.054734375, 'loss_different_1': 1.543190625, 'loss_different_2': 1.76743125, 'loss_different_3': 1.716246875}
Top1:  64.97
Saving model to ./output/train/20250829-042433-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_2/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_2_20.pt
Train: 21 [   0/390 (  0%)]  Loss:  1.808111 (1.8081)  Time: 1.639s,   78.12/s  (1.639s,   78.12/s)  LR: 9.730e-03  Data: 0.889 (0.889)
Train: 21 [  50/390 ( 13%)]  Loss:  2.613123 (2.2106)  Time: 0.748s,  171.22/s  (0.765s,  167.37/s)  LR: 9.730e-03  Data: 0.000 (0.018)
Train: 21 [ 100/390 ( 26%)]  Loss:  1.782487 (2.0679)  Time: 0.747s,  171.36/s  (0.756s,  169.28/s)  LR: 9.730e-03  Data: 0.000 (0.009)
Train: 21 [ 150/390 ( 39%)]  Loss:  1.650587 (1.9636)  Time: 0.747s,  171.41/s  (0.753s,  169.95/s)  LR: 9.730e-03  Data: 0.000 (0.006)
Train: 21 [ 200/390 ( 51%)]  Loss:  1.881416 (1.9471)  Time: 0.747s,  171.37/s  (0.752s,  170.29/s)  LR: 9.730e-03  Data: 0.000 (0.005)
Train: 21 [ 250/390 ( 64%)]  Loss:  2.268434 (2.0007)  Time: 0.747s,  171.46/s  (0.751s,  170.48/s)  LR: 9.730e-03  Data: 0.000 (0.004)
Train: 21 [ 300/390 ( 77%)]  Loss:  1.737106 (1.9630)  Time: 0.747s,  171.25/s  (0.750s,  170.62/s)  LR: 9.730e-03  Data: 0.000 (0.003)
Train: 21 [ 350/390 ( 90%)]  Loss:  1.789148 (1.9413)  Time: 0.750s,  170.74/s  (0.750s,  170.71/s)  LR: 9.730e-03  Data: 0.003 (0.003)
Train: 21 [ 389/390 (100%)]  Loss:  1.809884 (1.9267)  Time: 0.747s,  171.44/s  (0.750s,  170.78/s)  LR: 9.730e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.921 (0.921)  DataTime: 0.655 (0.655)  Loss:  1.1387 (1.1387)  Acc@1: 67.1875 (67.1875)  Acc@5: 92.9688 (92.9688)
Test: [  50/78]  Time: 0.258 (0.273)  DataTime: 0.000 (0.014)  Loss:  1.4893 (1.3269)  Acc@1: 60.1562 (64.6293)  Acc@5: 85.9375 (89.7825)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  1.2734 (1.3174)  Acc@1: 62.5000 (64.8500)  Acc@5: 87.5000 (89.7800)
tb
loss  0 : 2.090678125 21
loss  1 : 1.5009484375 21
loss  2 : 1.7750671875 21
loss  3 : 1.6654671875 21
{'loss_different_0': 2.090678125, 'loss_different_1': 1.5009484375, 'loss_different_2': 1.7750671875, 'loss_different_3': 1.6654671875}
Top1:  64.85
Train: 22 [   0/390 (  0%)]  Loss:  1.779271 (1.7793)  Time: 1.537s,   83.26/s  (1.537s,   83.26/s)  LR: 9.704e-03  Data: 0.788 (0.788)
Train: 22 [  50/390 ( 13%)]  Loss:  2.282068 (2.0307)  Time: 0.747s,  171.24/s  (0.763s,  167.76/s)  LR: 9.704e-03  Data: 0.000 (0.016)
Train: 22 [ 100/390 ( 26%)]  Loss:  1.757085 (1.9395)  Time: 0.748s,  171.18/s  (0.755s,  169.46/s)  LR: 9.704e-03  Data: 0.000 (0.008)
Train: 22 [ 150/390 ( 39%)]  Loss:  2.099930 (1.9796)  Time: 0.747s,  171.33/s  (0.754s,  169.66/s)  LR: 9.704e-03  Data: 0.000 (0.006)
Train: 22 [ 200/390 ( 51%)]  Loss:  1.798400 (1.9434)  Time: 0.748s,  171.17/s  (0.753s,  170.04/s)  LR: 9.704e-03  Data: 0.000 (0.004)
Train: 22 [ 250/390 ( 64%)]  Loss:  1.809609 (1.9211)  Time: 0.748s,  171.19/s  (0.752s,  170.28/s)  LR: 9.704e-03  Data: 0.000 (0.003)
Train: 22 [ 300/390 ( 77%)]  Loss:  1.769158 (1.8994)  Time: 0.747s,  171.30/s  (0.751s,  170.44/s)  LR: 9.704e-03  Data: 0.000 (0.003)
Train: 22 [ 350/390 ( 90%)]  Loss:  3.868430 (2.1455)  Time: 0.751s,  170.47/s  (0.751s,  170.55/s)  LR: 9.704e-03  Data: 0.003 (0.003)
Train: 22 [ 389/390 (100%)]  Loss:  1.896396 (2.1178)  Time: 0.747s,  171.34/s  (0.750s,  170.63/s)  LR: 9.704e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.965 (0.965)  DataTime: 0.690 (0.690)  Loss:  1.1504 (1.1504)  Acc@1: 71.8750 (71.8750)  Acc@5: 92.9688 (92.9688)
Test: [  50/78]  Time: 0.258 (0.273)  DataTime: 0.000 (0.014)  Loss:  1.4600 (1.2988)  Acc@1: 63.2812 (65.5025)  Acc@5: 87.5000 (90.5025)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  1.1123 (1.2995)  Acc@1: 81.2500 (65.7200)  Acc@5: 93.7500 (90.4400)
tb
loss  0 : 2.065034375 22
loss  1 : 1.48575 22
loss  2 : 1.7517421875 22
loss  3 : 1.6079875 22
{'loss_different_0': 2.065034375, 'loss_different_1': 1.48575, 'loss_different_2': 1.7517421875, 'loss_different_3': 1.6079875}
Top1:  65.72
Train: 23 [   0/390 (  0%)]  Loss:  3.687953 (3.6880)  Time: 1.658s,   77.22/s  (1.658s,   77.22/s)  LR: 9.677e-03  Data: 0.910 (0.910)
Train: 23 [  50/390 ( 13%)]  Loss:  1.750793 (2.7194)  Time: 0.748s,  171.15/s  (0.765s,  167.24/s)  LR: 9.677e-03  Data: 0.000 (0.018)
Train: 23 [ 100/390 ( 26%)]  Loss:  1.743370 (2.3940)  Time: 0.746s,  171.47/s  (0.757s,  169.20/s)  LR: 9.677e-03  Data: 0.000 (0.009)
Train: 23 [ 150/390 ( 39%)]  Loss:  2.004965 (2.2968)  Time: 0.748s,  171.17/s  (0.753s,  169.88/s)  LR: 9.677e-03  Data: 0.000 (0.006)
Train: 23 [ 200/390 ( 51%)]  Loss:  1.967485 (2.2309)  Time: 0.747s,  171.24/s  (0.752s,  170.22/s)  LR: 9.677e-03  Data: 0.000 (0.005)
Train: 23 [ 250/390 ( 64%)]  Loss:  1.883305 (2.1730)  Time: 0.748s,  171.20/s  (0.751s,  170.42/s)  LR: 9.677e-03  Data: 0.000 (0.004)
Train: 23 [ 300/390 ( 77%)]  Loss:  2.022742 (2.1515)  Time: 0.748s,  171.24/s  (0.752s,  170.32/s)  LR: 9.677e-03  Data: 0.000 (0.003)
Train: 23 [ 350/390 ( 90%)]  Loss:  1.794160 (2.1068)  Time: 0.749s,  170.78/s  (0.751s,  170.45/s)  LR: 9.677e-03  Data: 0.003 (0.003)
Train: 23 [ 389/390 (100%)]  Loss:  2.026320 (2.0979)  Time: 0.747s,  171.33/s  (0.751s,  170.54/s)  LR: 9.677e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.864 (0.864)  DataTime: 0.596 (0.596)  Loss:  1.2734 (1.2734)  Acc@1: 66.4062 (66.4062)  Acc@5: 88.2812 (88.2812)
Test: [  50/78]  Time: 0.259 (0.272)  DataTime: 0.000 (0.013)  Loss:  1.4629 (1.3130)  Acc@1: 64.0625 (65.6710)  Acc@5: 88.2812 (90.4412)
Test: [  78/78]  Time: 0.035 (0.264)  DataTime: 0.000 (0.008)  Loss:  1.2012 (1.3170)  Acc@1: 62.5000 (65.5300)  Acc@5: 93.7500 (90.3400)
tb
loss  0 : 2.123684375 23
loss  1 : 1.5170296875 23
loss  2 : 1.7558484375 23
loss  3 : 1.6526109375 23
{'loss_different_0': 2.123684375, 'loss_different_1': 1.5170296875, 'loss_different_2': 1.7558484375, 'loss_different_3': 1.6526109375}
Top1:  65.53
Train: 24 [   0/390 (  0%)]  Loss:  1.733447 (1.7334)  Time: 1.650s,   77.59/s  (1.650s,   77.59/s)  LR: 9.649e-03  Data: 0.900 (0.900)
Train: 24 [  50/390 ( 13%)]  Loss:  1.896781 (1.8151)  Time: 0.746s,  171.57/s  (0.765s,  167.34/s)  LR: 9.649e-03  Data: 0.000 (0.018)
Train: 24 [ 100/390 ( 26%)]  Loss:  2.956797 (2.1957)  Time: 0.747s,  171.34/s  (0.756s,  169.28/s)  LR: 9.649e-03  Data: 0.000 (0.009)
Train: 24 [ 150/390 ( 39%)]  Loss:  1.748540 (2.0839)  Time: 0.747s,  171.38/s  (0.753s,  169.94/s)  LR: 9.649e-03  Data: 0.000 (0.006)
Train: 24 [ 200/390 ( 51%)]  Loss:  1.697945 (2.0067)  Time: 0.747s,  171.41/s  (0.752s,  170.27/s)  LR: 9.649e-03  Data: 0.000 (0.005)
Train: 24 [ 250/390 ( 64%)]  Loss:  3.100097 (2.1889)  Time: 0.748s,  171.16/s  (0.751s,  170.47/s)  LR: 9.649e-03  Data: 0.000 (0.004)
Train: 24 [ 300/390 ( 77%)]  Loss:  1.860145 (2.1420)  Time: 0.747s,  171.29/s  (0.750s,  170.61/s)  LR: 9.649e-03  Data: 0.000 (0.003)
Train: 24 [ 350/390 ( 90%)]  Loss:  2.811108 (2.2256)  Time: 0.749s,  170.82/s  (0.750s,  170.71/s)  LR: 9.649e-03  Data: 0.003 (0.003)
Train: 24 [ 389/390 (100%)]  Loss:  1.855163 (2.1844)  Time: 0.747s,  171.39/s  (0.750s,  170.78/s)  LR: 9.649e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.855 (0.855)  DataTime: 0.590 (0.590)  Loss:  1.3164 (1.3164)  Acc@1: 66.4062 (66.4062)  Acc@5: 90.6250 (90.6250)
Test: [  50/78]  Time: 0.259 (0.271)  DataTime: 0.000 (0.012)  Loss:  1.3896 (1.3205)  Acc@1: 64.8438 (65.6097)  Acc@5: 89.0625 (89.7518)
Test: [  78/78]  Time: 0.034 (0.263)  DataTime: 0.000 (0.008)  Loss:  1.3701 (1.3145)  Acc@1: 75.0000 (65.4300)  Acc@5: 87.5000 (89.7400)
tb
loss  0 : 2.079675 24
loss  1 : 1.5253 24
loss  2 : 1.754346875 24
loss  3 : 1.644578125 24
{'loss_different_0': 2.079675, 'loss_different_1': 1.5253, 'loss_different_2': 1.754346875, 'loss_different_3': 1.644578125}
Top1:  65.43
Train: 25 [   0/390 (  0%)]  Loss:  1.674988 (1.6750)  Time: 1.647s,   77.73/s  (1.647s,   77.73/s)  LR: 9.619e-03  Data: 0.899 (0.899)
Train: 25 [  50/390 ( 13%)]  Loss:  1.977835 (1.8264)  Time: 0.747s,  171.39/s  (0.765s,  167.38/s)  LR: 9.619e-03  Data: 0.000 (0.018)
Train: 25 [ 100/390 ( 26%)]  Loss:  1.652549 (1.7685)  Time: 0.747s,  171.30/s  (0.756s,  169.29/s)  LR: 9.619e-03  Data: 0.000 (0.009)
Train: 25 [ 150/390 ( 39%)]  Loss:  3.866953 (2.2931)  Time: 0.747s,  171.42/s  (0.753s,  169.95/s)  LR: 9.619e-03  Data: 0.000 (0.006)
Train: 25 [ 200/390 ( 51%)]  Loss:  1.590197 (2.1525)  Time: 0.748s,  171.23/s  (0.752s,  170.29/s)  LR: 9.619e-03  Data: 0.000 (0.005)
Train: 25 [ 250/390 ( 64%)]  Loss:  1.868455 (2.1052)  Time: 0.748s,  171.22/s  (0.752s,  170.28/s)  LR: 9.619e-03  Data: 0.000 (0.004)
Train: 25 [ 300/390 ( 77%)]  Loss:  2.974250 (2.2293)  Time: 0.748s,  171.19/s  (0.751s,  170.45/s)  LR: 9.619e-03  Data: 0.000 (0.003)
Train: 25 [ 350/390 ( 90%)]  Loss:  1.799136 (2.1755)  Time: 0.749s,  170.82/s  (0.750s,  170.57/s)  LR: 9.619e-03  Data: 0.003 (0.003)
Train: 25 [ 389/390 (100%)]  Loss:  1.742053 (2.1274)  Time: 0.746s,  171.49/s  (0.750s,  170.65/s)  LR: 9.619e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.164 (1.164)  DataTime: 0.892 (0.892)  Loss:  1.1455 (1.1455)  Acc@1: 67.9688 (67.9688)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.258 (0.276)  DataTime: 0.000 (0.018)  Loss:  1.4619 (1.3482)  Acc@1: 64.8438 (66.0386)  Acc@5: 87.5000 (90.0123)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.012)  Loss:  1.2412 (1.3511)  Acc@1: 62.5000 (65.7800)  Acc@5: 100.0000 (90.0700)
tb
loss  0 : 2.1323921875 25
loss  1 : 1.5602515625 25
loss  2 : 1.7679 25
loss  3 : 1.6618140625 25
{'loss_different_0': 2.1323921875, 'loss_different_1': 1.5602515625, 'loss_different_2': 1.7679, 'loss_different_3': 1.6618140625}
Top1:  65.78
Train: 26 [   0/390 (  0%)]  Loss:  3.195809 (3.1958)  Time: 1.640s,   78.06/s  (1.640s,   78.06/s)  LR: 9.589e-03  Data: 0.891 (0.891)
Train: 26 [  50/390 ( 13%)]  Loss:  1.623149 (2.4095)  Time: 0.747s,  171.34/s  (0.765s,  167.39/s)  LR: 9.589e-03  Data: 0.000 (0.018)
Train: 26 [ 100/390 ( 26%)]  Loss:  2.141296 (2.3201)  Time: 0.747s,  171.33/s  (0.756s,  169.30/s)  LR: 9.589e-03  Data: 0.000 (0.009)
Train: 26 [ 150/390 ( 39%)]  Loss:  2.176896 (2.2843)  Time: 0.746s,  171.56/s  (0.753s,  169.96/s)  LR: 9.589e-03  Data: 0.000 (0.006)
Train: 26 [ 200/390 ( 51%)]  Loss:  1.613550 (2.1501)  Time: 0.747s,  171.37/s  (0.752s,  170.29/s)  LR: 9.589e-03  Data: 0.000 (0.005)
Train: 26 [ 250/390 ( 64%)]  Loss:  3.741042 (2.4153)  Time: 0.747s,  171.42/s  (0.751s,  170.49/s)  LR: 9.589e-03  Data: 0.000 (0.004)
Train: 26 [ 300/390 ( 77%)]  Loss:  1.917794 (2.3442)  Time: 0.748s,  171.10/s  (0.750s,  170.62/s)  LR: 9.589e-03  Data: 0.000 (0.003)
Train: 26 [ 350/390 ( 90%)]  Loss:  1.651066 (2.2576)  Time: 0.749s,  170.98/s  (0.750s,  170.72/s)  LR: 9.589e-03  Data: 0.003 (0.003)
Train: 26 [ 389/390 (100%)]  Loss:  1.507910 (2.1743)  Time: 0.747s,  171.39/s  (0.750s,  170.59/s)  LR: 9.589e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.889 (0.889)  DataTime: 0.602 (0.602)  Loss:  1.2490 (1.2490)  Acc@1: 68.7500 (68.7500)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.271)  DataTime: 0.000 (0.012)  Loss:  1.4336 (1.3178)  Acc@1: 64.8438 (65.5484)  Acc@5: 88.2812 (89.8591)
Test: [  78/78]  Time: 0.034 (0.264)  DataTime: 0.000 (0.008)  Loss:  0.8462 (1.3139)  Acc@1: 75.0000 (65.5800)  Acc@5: 100.0000 (90.2500)
tb
loss  0 : 2.1032765625 26
loss  1 : 1.5075109375 26
loss  2 : 1.7541453125 26
loss  3 : 1.656778125 26
{'loss_different_0': 2.1032765625, 'loss_different_1': 1.5075109375, 'loss_different_2': 1.7541453125, 'loss_different_3': 1.656778125}
Top1:  65.58
Train: 27 [   0/390 (  0%)]  Loss:  1.707877 (1.7079)  Time: 1.521s,   84.13/s  (1.521s,   84.13/s)  LR: 9.557e-03  Data: 0.753 (0.753)
Train: 27 [  50/390 ( 13%)]  Loss:  1.711344 (1.7096)  Time: 0.747s,  171.24/s  (0.763s,  167.84/s)  LR: 9.557e-03  Data: 0.000 (0.015)
Train: 27 [ 100/390 ( 26%)]  Loss:  3.443432 (2.2876)  Time: 0.747s,  171.44/s  (0.755s,  169.52/s)  LR: 9.557e-03  Data: 0.000 (0.008)
Train: 27 [ 150/390 ( 39%)]  Loss:  1.856041 (2.1797)  Time: 0.747s,  171.26/s  (0.752s,  170.11/s)  LR: 9.557e-03  Data: 0.000 (0.005)
Train: 27 [ 200/390 ( 51%)]  Loss:  1.727721 (2.0893)  Time: 0.747s,  171.32/s  (0.751s,  170.39/s)  LR: 9.557e-03  Data: 0.000 (0.004)
Train: 27 [ 250/390 ( 64%)]  Loss:  3.763239 (2.3683)  Time: 0.747s,  171.29/s  (0.750s,  170.55/s)  LR: 9.557e-03  Data: 0.000 (0.003)
Train: 27 [ 300/390 ( 77%)]  Loss:  1.664190 (2.2677)  Time: 0.747s,  171.41/s  (0.750s,  170.67/s)  LR: 9.557e-03  Data: 0.000 (0.003)
Train: 27 [ 350/390 ( 90%)]  Loss:  1.740483 (2.2018)  Time: 0.750s,  170.62/s  (0.750s,  170.75/s)  LR: 9.557e-03  Data: 0.003 (0.003)
Train: 27 [ 389/390 (100%)]  Loss:  1.777824 (2.1547)  Time: 0.747s,  171.25/s  (0.749s,  170.81/s)  LR: 9.557e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.348 (1.348)  DataTime: 1.085 (1.085)  Loss:  1.1074 (1.1074)  Acc@1: 70.3125 (70.3125)  Acc@5: 92.1875 (92.1875)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.022)  Loss:  1.3564 (1.2652)  Acc@1: 65.6250 (67.2794)  Acc@5: 89.0625 (90.9007)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.014)  Loss:  1.0361 (1.2617)  Acc@1: 75.0000 (67.3300)  Acc@5: 87.5000 (90.8200)
tb
loss  0 : 2.000228125 27
loss  1 : 1.489946875 27
loss  2 : 1.6935796875 27
loss  3 : 1.60288125 27
{'loss_different_0': 2.000228125, 'loss_different_1': 1.489946875, 'loss_different_2': 1.6935796875, 'loss_different_3': 1.60288125}
Top1:  67.33
Train: 28 [   0/390 (  0%)]  Loss:  1.747143 (1.7471)  Time: 1.509s,   84.85/s  (1.509s,   84.85/s)  LR: 9.524e-03  Data: 0.761 (0.761)
Train: 28 [  50/390 ( 13%)]  Loss:  1.689722 (1.7184)  Time: 0.748s,  171.16/s  (0.762s,  167.92/s)  LR: 9.524e-03  Data: 0.000 (0.015)
Train: 28 [ 100/390 ( 26%)]  Loss:  1.720573 (1.7191)  Time: 0.746s,  171.47/s  (0.755s,  169.56/s)  LR: 9.524e-03  Data: 0.000 (0.008)
Train: 28 [ 150/390 ( 39%)]  Loss:  3.196299 (2.0884)  Time: 0.747s,  171.29/s  (0.755s,  169.53/s)  LR: 9.524e-03  Data: 0.000 (0.005)
Train: 28 [ 200/390 ( 51%)]  Loss:  1.674408 (2.0056)  Time: 0.747s,  171.32/s  (0.753s,  169.96/s)  LR: 9.524e-03  Data: 0.000 (0.004)
Train: 28 [ 250/390 ( 64%)]  Loss:  1.415106 (1.9072)  Time: 0.748s,  171.22/s  (0.752s,  170.22/s)  LR: 9.524e-03  Data: 0.000 (0.003)
Train: 28 [ 300/390 ( 77%)]  Loss:  1.731798 (1.8821)  Time: 0.748s,  171.17/s  (0.751s,  170.39/s)  LR: 9.524e-03  Data: 0.000 (0.003)
Train: 28 [ 350/390 ( 90%)]  Loss:  3.081803 (2.0321)  Time: 0.749s,  170.85/s  (0.751s,  170.51/s)  LR: 9.524e-03  Data: 0.003 (0.003)
Train: 28 [ 389/390 (100%)]  Loss:  1.804020 (2.0068)  Time: 0.747s,  171.31/s  (0.750s,  170.60/s)  LR: 9.524e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.063 (1.063)  DataTime: 0.768 (0.768)  Loss:  1.1768 (1.1768)  Acc@1: 68.7500 (68.7500)  Acc@5: 90.6250 (90.6250)
Test: [  50/78]  Time: 0.258 (0.275)  DataTime: 0.000 (0.015)  Loss:  1.5586 (1.2864)  Acc@1: 62.5000 (67.6777)  Acc@5: 88.2812 (90.8701)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.010)  Loss:  1.3965 (1.2938)  Acc@1: 62.5000 (66.9500)  Acc@5: 93.7500 (90.9300)
tb
loss  0 : 2.0385453125 28
loss  1 : 1.5275890625 28
loss  2 : 1.694665625 28
loss  3 : 1.6337703125 28
{'loss_different_0': 2.0385453125, 'loss_different_1': 1.5275890625, 'loss_different_2': 1.694665625, 'loss_different_3': 1.6337703125}
Top1:  66.95
Train: 29 [   0/390 (  0%)]  Loss:  1.693556 (1.6936)  Time: 1.247s,  102.61/s  (1.247s,  102.61/s)  LR: 9.490e-03  Data: 0.438 (0.438)
Train: 29 [  50/390 ( 13%)]  Loss:  1.667894 (1.6807)  Time: 0.747s,  171.25/s  (0.757s,  169.04/s)  LR: 9.490e-03  Data: 0.000 (0.009)
Train: 29 [ 100/390 ( 26%)]  Loss:  1.519347 (1.6269)  Time: 0.747s,  171.39/s  (0.752s,  170.16/s)  LR: 9.490e-03  Data: 0.000 (0.005)
Train: 29 [ 150/390 ( 39%)]  Loss:  2.392043 (1.8182)  Time: 0.747s,  171.35/s  (0.751s,  170.53/s)  LR: 9.490e-03  Data: 0.000 (0.003)
Train: 29 [ 200/390 ( 51%)]  Loss:  1.566559 (1.7679)  Time: 0.747s,  171.38/s  (0.750s,  170.73/s)  LR: 9.490e-03  Data: 0.000 (0.003)
Train: 29 [ 250/390 ( 64%)]  Loss:  1.563237 (1.7338)  Time: 0.748s,  171.20/s  (0.749s,  170.84/s)  LR: 9.490e-03  Data: 0.000 (0.002)
Train: 29 [ 300/390 ( 77%)]  Loss:  1.680337 (1.7261)  Time: 0.747s,  171.29/s  (0.749s,  170.92/s)  LR: 9.490e-03  Data: 0.000 (0.002)
Train: 29 [ 350/390 ( 90%)]  Loss:  1.742555 (1.7282)  Time: 0.749s,  170.83/s  (0.749s,  170.97/s)  LR: 9.490e-03  Data: 0.003 (0.002)
Train: 29 [ 389/390 (100%)]  Loss:  1.613138 (1.7154)  Time: 0.746s,  171.49/s  (0.748s,  171.02/s)  LR: 9.490e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 0.854 (0.854)  DataTime: 0.583 (0.583)  Loss:  1.1426 (1.1426)  Acc@1: 69.5312 (69.5312)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.271)  DataTime: 0.000 (0.012)  Loss:  1.3115 (1.2300)  Acc@1: 67.1875 (68.1679)  Acc@5: 91.4062 (91.2377)
Test: [  78/78]  Time: 0.035 (0.264)  DataTime: 0.000 (0.008)  Loss:  1.2031 (1.2349)  Acc@1: 62.5000 (67.9700)  Acc@5: 87.5000 (91.2200)
tb
loss  0 : 2.0262359375 29
loss  1 : 1.460353125 29
loss  2 : 1.6396421875 29
loss  3 : 1.5383109375 29
{'loss_different_0': 2.0262359375, 'loss_different_1': 1.460353125, 'loss_different_2': 1.6396421875, 'loss_different_3': 1.5383109375}
Top1:  67.97
Train: 30 [   0/390 (  0%)]  Loss:  1.453821 (1.4538)  Time: 1.644s,   77.88/s  (1.644s,   77.88/s)  LR: 9.455e-03  Data: 0.895 (0.895)
Train: 30 [  50/390 ( 13%)]  Loss:  1.691620 (1.5727)  Time: 0.747s,  171.46/s  (0.770s,  166.31/s)  LR: 9.455e-03  Data: 0.000 (0.018)
Train: 30 [ 100/390 ( 26%)]  Loss:  1.586407 (1.5773)  Time: 0.748s,  171.15/s  (0.759s,  168.74/s)  LR: 9.455e-03  Data: 0.000 (0.009)
Train: 30 [ 150/390 ( 39%)]  Loss:  1.622050 (1.5885)  Time: 0.748s,  171.17/s  (0.755s,  169.57/s)  LR: 9.455e-03  Data: 0.000 (0.006)
Train: 30 [ 200/390 ( 51%)]  Loss:  3.705582 (2.0119)  Time: 0.747s,  171.37/s  (0.753s,  169.99/s)  LR: 9.455e-03  Data: 0.000 (0.005)
Train: 30 [ 250/390 ( 64%)]  Loss:  1.645403 (1.9508)  Time: 0.748s,  171.15/s  (0.752s,  170.25/s)  LR: 9.455e-03  Data: 0.000 (0.004)
Train: 30 [ 300/390 ( 77%)]  Loss:  1.573689 (1.8969)  Time: 0.747s,  171.28/s  (0.751s,  170.43/s)  LR: 9.455e-03  Data: 0.000 (0.003)
Train: 30 [ 350/390 ( 90%)]  Loss:  2.832437 (2.0139)  Time: 0.749s,  170.81/s  (0.751s,  170.55/s)  LR: 9.455e-03  Data: 0.003 (0.003)
Train: 30 [ 389/390 (100%)]  Loss:  1.699018 (1.9789)  Time: 0.747s,  171.31/s  (0.750s,  170.63/s)  LR: 9.455e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.141 (1.141)  DataTime: 0.757 (0.757)  Loss:  1.1445 (1.1445)  Acc@1: 69.5312 (69.5312)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.015)  Loss:  1.3203 (1.2253)  Acc@1: 64.8438 (67.8768)  Acc@5: 88.2812 (91.0692)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.010)  Loss:  0.9194 (1.2297)  Acc@1: 81.2500 (67.8200)  Acc@5: 93.7500 (91.0800)
tb
loss  0 : 1.9848546875 30
loss  1 : 1.4575109375 30
loss  2 : 1.6422296875 30
loss  3 : 1.5878046875 30
{'loss_different_0': 1.9848546875, 'loss_different_1': 1.4575109375, 'loss_different_2': 1.6422296875, 'loss_different_3': 1.5878046875}
Top1:  67.82
Saving model to ./output/train/20250829-042433-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_2/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_2_30.pt
Train: 31 [   0/390 (  0%)]  Loss:  3.430528 (3.4305)  Time: 1.394s,   91.81/s  (1.394s,   91.81/s)  LR: 9.419e-03  Data: 0.634 (0.634)
Train: 31 [  50/390 ( 13%)]  Loss:  1.749136 (2.5898)  Time: 0.747s,  171.34/s  (0.760s,  168.42/s)  LR: 9.419e-03  Data: 0.001 (0.013)
Train: 31 [ 100/390 ( 26%)]  Loss:  3.469812 (2.8832)  Time: 0.747s,  171.33/s  (0.754s,  169.85/s)  LR: 9.419e-03  Data: 0.000 (0.007)
Train: 31 [ 150/390 ( 39%)]  Loss:  1.697554 (2.5868)  Time: 0.748s,  171.14/s  (0.752s,  170.32/s)  LR: 9.419e-03  Data: 0.000 (0.005)
Train: 31 [ 200/390 ( 51%)]  Loss:  2.265498 (2.5225)  Time: 0.747s,  171.33/s  (0.752s,  170.21/s)  LR: 9.419e-03  Data: 0.000 (0.003)
Train: 31 [ 250/390 ( 64%)]  Loss:  1.661381 (2.3790)  Time: 0.747s,  171.35/s  (0.751s,  170.43/s)  LR: 9.419e-03  Data: 0.001 (0.003)
Train: 31 [ 300/390 ( 77%)]  Loss:  1.624696 (2.2712)  Time: 0.747s,  171.39/s  (0.750s,  170.57/s)  LR: 9.419e-03  Data: 0.000 (0.002)
Train: 31 [ 350/390 ( 90%)]  Loss:  2.458859 (2.2947)  Time: 0.749s,  170.86/s  (0.750s,  170.67/s)  LR: 9.419e-03  Data: 0.003 (0.002)
Train: 31 [ 389/390 (100%)]  Loss:  2.728028 (2.3428)  Time: 0.746s,  171.47/s  (0.750s,  170.74/s)  LR: 9.419e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.292 (1.292)  DataTime: 1.032 (1.032)  Loss:  1.0898 (1.0898)  Acc@1: 70.3125 (70.3125)  Acc@5: 89.8438 (89.8438)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.021)  Loss:  1.3506 (1.2548)  Acc@1: 66.4062 (68.0453)  Acc@5: 91.4062 (90.7782)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.013)  Loss:  1.2041 (1.2505)  Acc@1: 68.7500 (67.9800)  Acc@5: 93.7500 (90.9500)
tb
loss  0 : 2.1250875 31
loss  1 : 1.4282921875 31
loss  2 : 1.6958890625 31
loss  3 : 1.56600625 31
{'loss_different_0': 2.1250875, 'loss_different_1': 1.4282921875, 'loss_different_2': 1.6958890625, 'loss_different_3': 1.56600625}
Top1:  67.98
Train: 32 [   0/390 (  0%)]  Loss:  2.683386 (2.6834)  Time: 1.574s,   81.31/s  (1.574s,   81.31/s)  LR: 9.382e-03  Data: 0.827 (0.827)
Train: 32 [  50/390 ( 13%)]  Loss:  1.572232 (2.1278)  Time: 0.747s,  171.27/s  (0.764s,  167.60/s)  LR: 9.382e-03  Data: 0.000 (0.017)
Train: 32 [ 100/390 ( 26%)]  Loss:  2.154000 (2.1365)  Time: 0.747s,  171.29/s  (0.756s,  169.39/s)  LR: 9.382e-03  Data: 0.000 (0.009)
Train: 32 [ 150/390 ( 39%)]  Loss:  1.539758 (1.9873)  Time: 0.748s,  171.18/s  (0.753s,  170.00/s)  LR: 9.382e-03  Data: 0.000 (0.006)
Train: 32 [ 200/390 ( 51%)]  Loss:  1.818334 (1.9535)  Time: 0.747s,  171.29/s  (0.752s,  170.31/s)  LR: 9.382e-03  Data: 0.000 (0.004)
Train: 32 [ 250/390 ( 64%)]  Loss:  1.521096 (1.8815)  Time: 0.747s,  171.36/s  (0.751s,  170.50/s)  LR: 9.382e-03  Data: 0.000 (0.004)
Train: 32 [ 300/390 ( 77%)]  Loss:  1.735558 (1.8606)  Time: 0.748s,  171.21/s  (0.750s,  170.62/s)  LR: 9.382e-03  Data: 0.000 (0.003)
Train: 32 [ 350/390 ( 90%)]  Loss:  1.517717 (1.8178)  Time: 0.749s,  170.82/s  (0.750s,  170.71/s)  LR: 9.382e-03  Data: 0.003 (0.003)
Train: 32 [ 389/390 (100%)]  Loss:  1.755069 (1.8108)  Time: 0.747s,  171.34/s  (0.750s,  170.77/s)  LR: 9.382e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.894 (0.894)  DataTime: 0.626 (0.626)  Loss:  1.2520 (1.2520)  Acc@1: 64.8438 (64.8438)  Acc@5: 89.0625 (89.0625)
Test: [  50/78]  Time: 0.259 (0.272)  DataTime: 0.000 (0.013)  Loss:  1.4326 (1.2290)  Acc@1: 64.0625 (67.5705)  Acc@5: 90.6250 (91.1765)
Test: [  78/78]  Time: 0.035 (0.264)  DataTime: 0.000 (0.008)  Loss:  0.8794 (1.2361)  Acc@1: 75.0000 (67.4700)  Acc@5: 87.5000 (91.0200)
tb
loss  0 : 1.958953125 32
loss  1 : 1.4875234375 32
loss  2 : 1.655165625 32
loss  3 : 1.637265625 32
{'loss_different_0': 1.958953125, 'loss_different_1': 1.4875234375, 'loss_different_2': 1.655165625, 'loss_different_3': 1.637265625}
Top1:  67.47
Train: 33 [   0/390 (  0%)]  Loss:  1.722039 (1.7220)  Time: 1.411s,   90.72/s  (1.411s,   90.72/s)  LR: 9.343e-03  Data: 0.621 (0.621)
Train: 33 [  50/390 ( 13%)]  Loss:  3.764261 (2.7432)  Time: 0.748s,  171.17/s  (0.761s,  168.30/s)  LR: 9.343e-03  Data: 0.000 (0.013)
Train: 33 [ 100/390 ( 26%)]  Loss:  1.586182 (2.3575)  Time: 0.747s,  171.31/s  (0.756s,  169.24/s)  LR: 9.343e-03  Data: 0.000 (0.006)
Train: 33 [ 150/390 ( 39%)]  Loss:  1.593110 (2.1664)  Time: 0.748s,  171.16/s  (0.753s,  169.90/s)  LR: 9.343e-03  Data: 0.000 (0.004)
Train: 33 [ 200/390 ( 51%)]  Loss:  1.639058 (2.0609)  Time: 0.748s,  171.03/s  (0.752s,  170.23/s)  LR: 9.343e-03  Data: 0.000 (0.003)
Train: 33 [ 250/390 ( 64%)]  Loss:  1.711277 (2.0027)  Time: 0.747s,  171.26/s  (0.751s,  170.43/s)  LR: 9.343e-03  Data: 0.000 (0.003)
Train: 33 [ 300/390 ( 77%)]  Loss:  1.617966 (1.9477)  Time: 0.747s,  171.31/s  (0.750s,  170.57/s)  LR: 9.343e-03  Data: 0.000 (0.002)
Train: 33 [ 350/390 ( 90%)]  Loss:  1.555055 (1.8986)  Time: 0.749s,  170.85/s  (0.750s,  170.66/s)  LR: 9.343e-03  Data: 0.003 (0.002)
Train: 33 [ 389/390 (100%)]  Loss:  1.603122 (1.8658)  Time: 0.747s,  171.39/s  (0.750s,  170.73/s)  LR: 9.343e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.011 (1.011)  DataTime: 0.732 (0.732)  Loss:  1.0840 (1.0840)  Acc@1: 69.5312 (69.5312)  Acc@5: 93.7500 (93.7500)
Test: [  50/78]  Time: 0.258 (0.274)  DataTime: 0.000 (0.015)  Loss:  1.4141 (1.2830)  Acc@1: 65.6250 (66.4522)  Acc@5: 89.8438 (90.8088)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.010)  Loss:  1.0908 (1.2799)  Acc@1: 81.2500 (66.6000)  Acc@5: 87.5000 (90.9000)
tb
loss  0 : 2.0679296875 33
loss  1 : 1.512046875 33
loss  2 : 1.716715625 33
loss  3 : 1.65128125 33
{'loss_different_0': 2.0679296875, 'loss_different_1': 1.512046875, 'loss_different_2': 1.716715625, 'loss_different_3': 1.65128125}
Top1:  66.6
Train: 34 [   0/390 (  0%)]  Loss:  3.000268 (3.0003)  Time: 1.622s,   78.92/s  (1.622s,   78.92/s)  LR: 9.304e-03  Data: 0.874 (0.874)
Train: 34 [  50/390 ( 13%)]  Loss:  1.661767 (2.3310)  Time: 0.747s,  171.33/s  (0.764s,  167.45/s)  LR: 9.304e-03  Data: 0.000 (0.017)
Train: 34 [ 100/390 ( 26%)]  Loss:  3.630756 (2.7643)  Time: 0.747s,  171.39/s  (0.756s,  169.33/s)  LR: 9.304e-03  Data: 0.000 (0.009)
Train: 34 [ 150/390 ( 39%)]  Loss:  1.495092 (2.4470)  Time: 0.748s,  171.19/s  (0.753s,  169.98/s)  LR: 9.304e-03  Data: 0.000 (0.006)
Train: 34 [ 200/390 ( 51%)]  Loss:  1.557275 (2.2690)  Time: 0.747s,  171.25/s  (0.752s,  170.31/s)  LR: 9.304e-03  Data: 0.000 (0.005)
Train: 34 [ 250/390 ( 64%)]  Loss:  1.539506 (2.1474)  Time: 0.748s,  171.20/s  (0.752s,  170.23/s)  LR: 9.304e-03  Data: 0.000 (0.004)
Train: 34 [ 300/390 ( 77%)]  Loss:  1.648800 (2.0762)  Time: 0.747s,  171.30/s  (0.751s,  170.40/s)  LR: 9.304e-03  Data: 0.000 (0.003)
Train: 34 [ 350/390 ( 90%)]  Loss:  1.508371 (2.0052)  Time: 0.749s,  170.79/s  (0.751s,  170.53/s)  LR: 9.304e-03  Data: 0.003 (0.003)
Train: 34 [ 389/390 (100%)]  Loss:  1.667682 (1.9677)  Time: 0.747s,  171.43/s  (0.750s,  170.62/s)  LR: 9.304e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.039 (1.039)  DataTime: 0.725 (0.725)  Loss:  1.1055 (1.1055)  Acc@1: 69.5312 (69.5312)  Acc@5: 92.9688 (92.9688)
Test: [  50/78]  Time: 0.258 (0.274)  DataTime: 0.000 (0.015)  Loss:  1.2930 (1.2278)  Acc@1: 70.3125 (68.4743)  Acc@5: 91.4062 (91.4062)
Test: [  78/78]  Time: 0.034 (0.266)  DataTime: 0.000 (0.009)  Loss:  1.1025 (1.2223)  Acc@1: 75.0000 (68.3500)  Acc@5: 87.5000 (91.3900)
tb
loss  0 : 2.0032578125 34
loss  1 : 1.4497109375 34
loss  2 : 1.636803125 34
loss  3 : 1.5417 34
{'loss_different_0': 2.0032578125, 'loss_different_1': 1.4497109375, 'loss_different_2': 1.636803125, 'loss_different_3': 1.5417}
Top1:  68.35
Train: 35 [   0/390 (  0%)]  Loss:  1.521311 (1.5213)  Time: 1.637s,   78.19/s  (1.637s,   78.19/s)  LR: 9.263e-03  Data: 0.889 (0.889)
Train: 35 [  50/390 ( 13%)]  Loss:  3.071345 (2.2963)  Time: 0.748s,  171.21/s  (0.765s,  167.41/s)  LR: 9.263e-03  Data: 0.000 (0.018)
Train: 35 [ 100/390 ( 26%)]  Loss:  1.698353 (2.0970)  Time: 0.747s,  171.27/s  (0.756s,  169.31/s)  LR: 9.263e-03  Data: 0.000 (0.009)
Train: 35 [ 150/390 ( 39%)]  Loss:  1.567526 (1.9646)  Time: 0.746s,  171.56/s  (0.753s,  169.96/s)  LR: 9.263e-03  Data: 0.000 (0.006)
Train: 35 [ 200/390 ( 51%)]  Loss:  1.675844 (1.9069)  Time: 0.747s,  171.42/s  (0.752s,  170.30/s)  LR: 9.263e-03  Data: 0.000 (0.005)
Train: 35 [ 250/390 ( 64%)]  Loss:  2.655177 (2.0316)  Time: 0.747s,  171.34/s  (0.751s,  170.50/s)  LR: 9.263e-03  Data: 0.000 (0.004)
Train: 35 [ 300/390 ( 77%)]  Loss:  3.367994 (2.2225)  Time: 0.747s,  171.28/s  (0.750s,  170.63/s)  LR: 9.263e-03  Data: 0.000 (0.003)
Train: 35 [ 350/390 ( 90%)]  Loss:  1.625142 (2.1478)  Time: 0.749s,  170.90/s  (0.750s,  170.73/s)  LR: 9.263e-03  Data: 0.003 (0.003)
Train: 35 [ 389/390 (100%)]  Loss:  2.827818 (2.2234)  Time: 0.747s,  171.44/s  (0.749s,  170.79/s)  LR: 9.263e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.254 (1.254)  DataTime: 0.967 (0.967)  Loss:  1.1426 (1.1426)  Acc@1: 76.5625 (76.5625)  Acc@5: 89.8438 (89.8438)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.019)  Loss:  1.4043 (1.2772)  Acc@1: 67.9688 (67.6930)  Acc@5: 89.8438 (90.8854)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.013)  Loss:  1.1055 (1.2748)  Acc@1: 68.7500 (67.6400)  Acc@5: 87.5000 (90.8900)
tb
loss  0 : 2.015521875 35
loss  1 : 1.4938921875 35
loss  2 : 1.71221875 35
loss  3 : 1.627946875 35
{'loss_different_0': 2.015521875, 'loss_different_1': 1.4938921875, 'loss_different_2': 1.71221875, 'loss_different_3': 1.627946875}
Top1:  67.64
Train: 36 [   0/390 (  0%)]  Loss:  3.215960 (3.2160)  Time: 1.655s,   77.34/s  (1.655s,   77.34/s)  LR: 9.222e-03  Data: 0.905 (0.905)
Train: 36 [  50/390 ( 13%)]  Loss:  1.455373 (2.3357)  Time: 0.747s,  171.45/s  (0.765s,  167.32/s)  LR: 9.222e-03  Data: 0.000 (0.018)
Train: 36 [ 100/390 ( 26%)]  Loss:  1.789165 (2.1535)  Time: 0.747s,  171.41/s  (0.756s,  169.27/s)  LR: 9.222e-03  Data: 0.000 (0.009)
Train: 36 [ 150/390 ( 39%)]  Loss:  1.685043 (2.0364)  Time: 0.747s,  171.38/s  (0.753s,  169.95/s)  LR: 9.222e-03  Data: 0.000 (0.006)
Train: 36 [ 200/390 ( 51%)]  Loss:  1.675348 (1.9642)  Time: 0.747s,  171.33/s  (0.753s,  170.03/s)  LR: 9.222e-03  Data: 0.000 (0.005)
Train: 36 [ 250/390 ( 64%)]  Loss:  3.693069 (2.2523)  Time: 0.747s,  171.35/s  (0.752s,  170.29/s)  LR: 9.222e-03  Data: 0.000 (0.004)
Train: 36 [ 300/390 ( 77%)]  Loss:  1.825257 (2.1913)  Time: 0.747s,  171.35/s  (0.751s,  170.46/s)  LR: 9.222e-03  Data: 0.000 (0.003)
Train: 36 [ 350/390 ( 90%)]  Loss:  2.626368 (2.2457)  Time: 0.749s,  170.89/s  (0.750s,  170.58/s)  LR: 9.222e-03  Data: 0.003 (0.003)
Train: 37 [ 100/390 ( 26%)]  Loss:  1.381367 (1.4419)  Time: 0.747s,  171.31/s  (0.752s,  170.14/s)  LR: 9.179e-03  Data: 0.000 (0.004)
Train: 37 [ 300/390 ( 77%)]  Loss:  1.723310 (1.7345)  Time: 0.747s,  171.35/s  (0.749s,  170.89/s)  LR: 9.179e-03  Data: 0.000 (0.002)
Train: 37 [ 350/390 ( 90%)]  Loss:  1.809010 (1.7438)  Time: 0.750s,  170.55/s  (0.750s,  170.72/s)  LR: 9.179e-03  Data: 0.003 (0.001)
Train: 37 [ 389/390 (100%)]  Loss:  1.553923 (1.7227)  Time: 0.747s,  171.28/s  (0.749s,  170.78/s)  LR: 9.179e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.176 (1.176)  DataTime: 0.810 (0.810)  Loss:  1.0840 (1.0840)  Acc@1: 70.3125 (70.3125)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.016)  Loss:  1.4668 (1.2401)  Acc@1: 60.1562 (67.9381)  Acc@5: 85.9375 (90.5178)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.011)  Loss:  1.3545 (1.2384)  Acc@1: 68.7500 (68.0200)  Acc@5: 81.2500 (90.6000)
tb
loss  0 : 1.974521875 37
loss  1 : 1.4714640625 37
loss  2 : 1.6956578125 37
loss  3 : 1.6049125 37
{'loss_different_0': 1.974521875, 'loss_different_1': 1.4714640625, 'loss_different_2': 1.6956578125, 'loss_different_3': 1.6049125}
Top1:  68.02
Train: 38 [   0/390 (  0%)]  Loss:  1.601803 (1.6018)  Time: 1.173s,  109.10/s  (1.173s,  109.10/s)  LR: 9.135e-03  Data: 0.359 (0.359)
Train: 38 [  50/390 ( 13%)]  Loss:  1.570097 (1.5859)  Time: 0.747s,  171.27/s  (0.756s,  169.36/s)  LR: 9.135e-03  Data: 0.000 (0.007)
Train: 38 [ 100/390 ( 26%)]  Loss:  1.564918 (1.5789)  Time: 0.747s,  171.29/s  (0.752s,  170.30/s)  LR: 9.135e-03  Data: 0.000 (0.004)
Train: 38 [ 150/390 ( 39%)]  Loss:  2.216579 (1.7383)  Time: 0.747s,  171.31/s  (0.750s,  170.61/s)  LR: 9.135e-03  Data: 0.000 (0.003)
Train: 38 [ 200/390 ( 51%)]  Loss:  1.756216 (1.7419)  Time: 0.747s,  171.44/s  (0.750s,  170.78/s)  LR: 9.135e-03  Data: 0.000 (0.002)
Train: 38 [ 250/390 ( 64%)]  Loss:  2.150348 (1.8100)  Time: 0.747s,  171.29/s  (0.749s,  170.88/s)  LR: 9.135e-03  Data: 0.001 (0.002)
Train: 38 [ 300/390 ( 77%)]  Loss:  2.639781 (1.9285)  Time: 0.748s,  171.24/s  (0.749s,  170.94/s)  LR: 9.135e-03  Data: 0.000 (0.002)
Train: 38 [ 350/390 ( 90%)]  Loss:  1.590609 (1.8863)  Time: 0.750s,  170.74/s  (0.749s,  170.99/s)  LR: 9.135e-03  Data: 0.003 (0.001)
Train: 38 [ 389/390 (100%)]  Loss:  1.449403 (1.8378)  Time: 0.747s,  171.34/s  (0.748s,  171.02/s)  LR: 9.135e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.206 (1.206)  DataTime: 0.917 (0.917)  Loss:  1.1211 (1.1211)  Acc@1: 73.4375 (73.4375)  Acc@5: 92.9688 (92.9688)
Test: [  50/78]  Time: 0.258 (0.277)  DataTime: 0.000 (0.018)  Loss:  1.3115 (1.2249)  Acc@1: 64.8438 (68.8725)  Acc@5: 90.6250 (91.0539)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.012)  Loss:  0.9326 (1.2217)  Acc@1: 75.0000 (68.7100)  Acc@5: 93.7500 (90.9500)
tb
loss  0 : 1.986325 38
loss  1 : 1.455940625 38
loss  2 : 1.642025 38
loss  3 : 1.598521875 38
{'loss_different_0': 1.986325, 'loss_different_1': 1.455940625, 'loss_different_2': 1.642025, 'loss_different_3': 1.598521875}
Top1:  68.71
Train: 39 [   0/390 (  0%)]  Loss:  1.483608 (1.4836)  Time: 1.601s,   79.95/s  (1.601s,   79.95/s)  LR: 9.091e-03  Data: 0.853 (0.853)
Train: 39 [  50/390 ( 13%)]  Loss:  1.546151 (1.5149)  Time: 0.747s,  171.41/s  (0.764s,  167.57/s)  LR: 9.091e-03  Data: 0.000 (0.017)
Train: 39 [ 100/390 ( 26%)]  Loss:  1.812773 (1.6142)  Time: 0.747s,  171.27/s  (0.756s,  169.39/s)  LR: 9.091e-03  Data: 0.000 (0.009)
Train: 39 [ 150/390 ( 39%)]  Loss:  1.628682 (1.6178)  Time: 0.747s,  171.31/s  (0.753s,  170.02/s)  LR: 9.091e-03  Data: 0.000 (0.006)
Train: 39 [ 200/390 ( 51%)]  Loss:  1.716162 (1.6375)  Time: 0.747s,  171.40/s  (0.751s,  170.34/s)  LR: 9.091e-03  Data: 0.000 (0.005)
Train: 39 [ 250/390 ( 64%)]  Loss:  1.487475 (1.6125)  Time: 0.747s,  171.36/s  (0.752s,  170.31/s)  LR: 9.091e-03  Data: 0.000 (0.004)
Train: 39 [ 300/390 ( 77%)]  Loss:  1.592631 (1.6096)  Time: 0.748s,  171.23/s  (0.751s,  170.48/s)  LR: 9.091e-03  Data: 0.000 (0.003)
Train: 39 [ 350/390 ( 90%)]  Loss:  1.533092 (1.6001)  Time: 0.750s,  170.61/s  (0.750s,  170.60/s)  LR: 9.091e-03  Data: 0.003 (0.003)
Train: 39 [ 389/390 (100%)]  Loss:  1.558032 (1.5954)  Time: 0.747s,  171.41/s  (0.750s,  170.67/s)  LR: 9.091e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.127 (1.127)  DataTime: 0.848 (0.848)  Loss:  1.0488 (1.0488)  Acc@1: 74.2188 (74.2188)  Acc@5: 93.7500 (93.7500)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.017)  Loss:  1.3350 (1.1805)  Acc@1: 67.9688 (69.3474)  Acc@5: 89.0625 (91.8964)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.0635 (1.1854)  Acc@1: 68.7500 (69.4500)  Acc@5: 93.7500 (91.6300)
tb
loss  0 : 1.9229875 39
loss  1 : 1.4418390625 39
loss  2 : 1.6154078125 39
loss  3 : 1.558934375 39
{'loss_different_0': 1.9229875, 'loss_different_1': 1.4418390625, 'loss_different_2': 1.6154078125, 'loss_different_3': 1.558934375}
Top1:  69.45
Train: 40 [   0/390 (  0%)]  Loss:  2.480076 (2.4801)  Time: 1.486s,   86.15/s  (1.486s,   86.15/s)  LR: 9.045e-03  Data: 0.733 (0.733)
Train: 40 [  50/390 ( 13%)]  Loss:  2.013951 (2.2470)  Time: 0.748s,  171.16/s  (0.762s,  168.05/s)  LR: 9.045e-03  Data: 0.000 (0.015)
Train: 40 [ 100/390 ( 26%)]  Loss:  1.490628 (1.9949)  Time: 0.746s,  171.53/s  (0.755s,  169.64/s)  LR: 9.045e-03  Data: 0.000 (0.008)
Train: 40 [ 150/390 ( 39%)]  Loss:  1.551414 (1.8840)  Time: 0.748s,  171.15/s  (0.752s,  170.19/s)  LR: 9.045e-03  Data: 0.000 (0.005)
Train: 40 [ 200/390 ( 51%)]  Loss:  1.472382 (1.8017)  Time: 0.746s,  171.57/s  (0.751s,  170.46/s)  LR: 9.045e-03  Data: 0.000 (0.004)
Train: 40 [ 250/390 ( 64%)]  Loss:  2.591746 (1.9334)  Time: 0.747s,  171.36/s  (0.750s,  170.63/s)  LR: 9.045e-03  Data: 0.000 (0.003)
Train: 40 [ 300/390 ( 77%)]  Loss:  1.613074 (1.8876)  Time: 0.747s,  171.32/s  (0.750s,  170.74/s)  LR: 9.045e-03  Data: 0.000 (0.003)
Train: 40 [ 350/390 ( 90%)]  Loss:  1.556356 (1.8462)  Time: 0.749s,  170.80/s  (0.749s,  170.82/s)  LR: 9.045e-03  Data: 0.003 (0.002)
Train: 40 [ 389/390 (100%)]  Loss:  1.519540 (1.8099)  Time: 0.747s,  171.34/s  (0.750s,  170.69/s)  LR: 9.045e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.137 (1.137)  DataTime: 0.871 (0.871)  Loss:  1.1377 (1.1377)  Acc@1: 71.8750 (71.8750)  Acc@5: 90.6250 (90.6250)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.017)  Loss:  1.3926 (1.2174)  Acc@1: 67.1875 (68.7347)  Acc@5: 89.8438 (91.3756)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  0.9058 (1.2146)  Acc@1: 68.7500 (69.1100)  Acc@5: 100.0000 (91.4300)
tb
loss  0 : 1.9594984375 40
loss  1 : 1.48279296875 40
loss  2 : 1.6457828125 40
loss  3 : 1.6167296875 40
{'loss_different_0': 1.9594984375, 'loss_different_1': 1.48279296875, 'loss_different_2': 1.6457828125, 'loss_different_3': 1.6167296875}
Top1:  69.11
Saving model to ./output/train/20250829-042433-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_2/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_2_40.pt
Train: 41 [   0/390 (  0%)]  Loss:  1.482802 (1.4828)  Time: 1.588s,   80.61/s  (1.588s,   80.61/s)  LR: 8.998e-03  Data: 0.827 (0.827)
Train: 41 [  50/390 ( 13%)]  Loss:  1.394804 (1.4388)  Time: 0.747s,  171.43/s  (0.764s,  167.63/s)  LR: 8.998e-03  Data: 0.000 (0.017)
Train: 41 [ 100/390 ( 26%)]  Loss:  3.034799 (1.9708)  Time: 0.748s,  171.17/s  (0.756s,  169.42/s)  LR: 8.998e-03  Data: 0.000 (0.009)
Train: 41 [ 150/390 ( 39%)]  Loss:  1.590036 (1.8756)  Time: 0.747s,  171.42/s  (0.753s,  170.04/s)  LR: 8.998e-03  Data: 0.000 (0.006)
^C
Traceback (most recent call last):
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 233, in <module>
    main()
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 69, in main
    train_metrics = train_epoch(
                    ^^^^^^^^^^^^
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 127, in train_epoch
    loss_scaler(loss, optimizer)
  File "/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py", line 96, in __call__
    self._scaler.step(optimizer)
  File "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py", line 457, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
               ^^^^^^^^
KeyboardInterrupt