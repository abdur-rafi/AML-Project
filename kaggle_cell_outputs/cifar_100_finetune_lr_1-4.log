2025-08-28 17:49:51.432056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1756403391.626360      98 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1756403391.681216      98 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
create dataset:  cifar100
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Creating model..
, number of params: 27599012
SGD!!!
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Train: 0 [   0/390 (  0%)]  Loss:  4.622955 (4.6230)  Time: 3.796s,   33.72/s  (3.796s,   33.72/s)  LR: 1.000e-04  Data: 1.349 (1.349)
Train: 0 [  50/390 ( 13%)]  Loss:  4.613052 (4.6180)  Time: 0.748s,  171.21/s  (0.808s,  158.36/s)  LR: 1.000e-04  Data: 0.000 (0.027)
Train: 0 [ 100/390 ( 26%)]  Loss:  4.607854 (4.6146)  Time: 0.748s,  171.18/s  (0.779s,  164.42/s)  LR: 1.000e-04  Data: 0.000 (0.014)
Train: 0 [ 150/390 ( 39%)]  Loss:  4.615495 (4.6148)  Time: 0.748s,  171.10/s  (0.768s,  166.57/s)  LR: 1.000e-04  Data: 0.001 (0.009)
Train: 0 [ 200/390 ( 51%)]  Loss:  4.617033 (4.6153)  Time: 0.748s,  171.01/s  (0.766s,  167.17/s)  LR: 1.000e-04  Data: 0.000 (0.007)
Train: 0 [ 250/390 ( 64%)]  Loss:  4.615662 (4.6153)  Time: 0.747s,  171.28/s  (0.762s,  167.94/s)  LR: 1.000e-04  Data: 0.000 (0.006)
Train: 0 [ 300/390 ( 77%)]  Loss:  4.617152 (4.6156)  Time: 0.748s,  171.09/s  (0.760s,  168.46/s)  LR: 1.000e-04  Data: 0.000 (0.005)
Train: 0 [ 350/390 ( 90%)]  Loss:  4.627350 (4.6171)  Time: 0.751s,  170.37/s  (0.758s,  168.82/s)  LR: 1.000e-04  Data: 0.004 (0.004)
Train: 0 [ 389/390 (100%)]  Loss:  4.617190 (4.6171)  Time: 0.748s,  171.23/s  (0.757s,  169.06/s)  LR: 1.000e-04  Data: 0.000 (0.004)
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Test: [   0/78]  Time: 1.549 (1.549)  DataTime: 1.049 (1.049)  Loss:  4.6250 (4.6250)  Acc@1:  0.7812 ( 0.7812)  Acc@5:  3.1250 ( 3.1250)
Test: [  50/78]  Time: 0.259 (0.284)  DataTime: 0.000 (0.021)  Loss:  4.6094 (4.6117)  Acc@1:  0.0000 ( 1.4246)  Acc@5:  2.3438 ( 4.9786)
Test: [  78/78]  Time: 0.047 (0.273)  DataTime: 0.000 (0.014)  Loss:  4.5977 (4.6121)  Acc@1:  6.2500 ( 1.4100)  Acc@5:  6.2500 ( 5.0100)
tb
loss  0 : 4.6079375 0
loss  1 : 4.61625625 0
loss  2 : 4.61653125 0
loss  3 : 4.6187125 0
{'loss_different_0': 4.6079375, 'loss_different_1': 4.61625625, 'loss_different_2': 4.61653125, 'loss_different_3': 4.6187125}
Top1:  1.41
Saving model to ./output/train/20250828-175014-pretrain_cifar_100_using_resuming_cifar_10_lr_1-4/pretrain_cifar_100_using_resuming_cifar_10_lr_1-4_0.pt
Train: 1 [   0/390 (  0%)]  Loss:  4.613043 (4.6130)  Time: 2.003s,   63.90/s  (2.003s,   63.90/s)  LR: 9.999e-05  Data: 1.252 (1.252)
Train: 1 [  50/390 ( 13%)]  Loss:  4.619077 (4.6161)  Time: 0.748s,  171.12/s  (0.773s,  165.65/s)  LR: 9.999e-05  Data: 0.000 (0.025)
Train: 1 [ 100/390 ( 26%)]  Loss:  4.608582 (4.6136)  Time: 0.749s,  171.01/s  (0.760s,  168.32/s)  LR: 9.999e-05  Data: 0.001 (0.013)
Train: 1 [ 150/390 ( 39%)]  Loss:  4.614318 (4.6138)  Time: 0.748s,  171.10/s  (0.756s,  169.25/s)  LR: 9.999e-05  Data: 0.000 (0.009)
Train: 1 [ 200/390 ( 51%)]  Loss:  4.620239 (4.6151)  Time: 0.748s,  171.16/s  (0.754s,  169.72/s)  LR: 9.999e-05  Data: 0.000 (0.007)
Train: 1 [ 250/390 ( 64%)]  Loss:  4.579498 (4.6091)  Time: 0.748s,  171.15/s  (0.753s,  170.00/s)  LR: 9.999e-05  Data: 0.000 (0.005)
Train: 1 [ 300/390 ( 77%)]  Loss:  4.627500 (4.6118)  Time: 0.749s,  170.94/s  (0.752s,  170.18/s)  LR: 9.999e-05  Data: 0.000 (0.005)
Train: 1 [ 350/390 ( 90%)]  Loss:  4.597412 (4.6100)  Time: 0.751s,  170.52/s  (0.752s,  170.31/s)  LR: 9.999e-05  Data: 0.003 (0.004)
Train: 1 [ 389/390 (100%)]  Loss:  4.611855 (4.6102)  Time: 0.747s,  171.25/s  (0.751s,  170.40/s)  LR: 9.999e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.439 (1.439)  DataTime: 1.141 (1.141)  Loss:  4.6172 (4.6172)  Acc@1:  0.7812 ( 0.7812)  Acc@5:  3.1250 ( 3.1250)
Test: [  50/78]  Time: 0.259 (0.282)  DataTime: 0.000 (0.023)  Loss:  4.5977 (4.6045)  Acc@1:  0.0000 ( 1.4553)  Acc@5:  4.6875 ( 5.5913)
Test: [  78/78]  Time: 0.035 (0.271)  DataTime: 0.000 (0.015)  Loss:  4.5898 (4.6047)  Acc@1:  6.2500 ( 1.5000)  Acc@5:  6.2500 ( 5.5800)
tb
loss  0 : 4.6078875 1
loss  1 : 4.60593125 1
loss  2 : 4.60781875 1
loss  3 : 4.60991875 1
{'loss_different_0': 4.6078875, 'loss_different_1': 4.60593125, 'loss_different_2': 4.60781875, 'loss_different_3': 4.60991875}
Top1:  1.5
Train: 2 [   0/390 (  0%)]  Loss:  4.594016 (4.5940)  Time: 2.146s,   59.65/s  (2.146s,   59.65/s)  LR: 9.998e-05  Data: 1.391 (1.391)
Train: 2 [  50/390 ( 13%)]  Loss:  4.607128 (4.6006)  Time: 0.748s,  171.09/s  (0.781s,  163.88/s)  LR: 9.998e-05  Data: 0.000 (0.028)
Train: 2 [ 100/390 ( 26%)]  Loss:  4.608527 (4.6032)  Time: 0.748s,  171.08/s  (0.765s,  167.39/s)  LR: 9.998e-05  Data: 0.000 (0.014)
Train: 2 [ 150/390 ( 39%)]  Loss:  4.612300 (4.6055)  Time: 0.747s,  171.24/s  (0.759s,  168.61/s)  LR: 9.998e-05  Data: 0.000 (0.010)
Train: 2 [ 200/390 ( 51%)]  Loss:  4.615048 (4.6074)  Time: 0.748s,  171.06/s  (0.756s,  169.22/s)  LR: 9.998e-05  Data: 0.000 (0.007)
Train: 2 [ 250/390 ( 64%)]  Loss:  4.608551 (4.6076)  Time: 0.748s,  171.15/s  (0.755s,  169.59/s)  LR: 9.998e-05  Data: 0.000 (0.006)
Train: 2 [ 300/390 ( 77%)]  Loss:  4.605682 (4.6073)  Time: 0.748s,  171.05/s  (0.754s,  169.85/s)  LR: 9.998e-05  Data: 0.001 (0.005)
Train: 2 [ 350/390 ( 90%)]  Loss:  4.617063 (4.6085)  Time: 0.751s,  170.55/s  (0.753s,  170.03/s)  LR: 9.998e-05  Data: 0.003 (0.004)
Train: 2 [ 389/390 (100%)]  Loss:  4.613058 (4.6090)  Time: 0.748s,  171.14/s  (0.752s,  170.14/s)  LR: 9.998e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.369 (1.369)  DataTime: 0.904 (0.904)  Loss:  4.6094 (4.6094)  Acc@1:  1.5625 ( 1.5625)  Acc@5:  3.1250 ( 3.1250)
Test: [  50/78]  Time: 0.259 (0.281)  DataTime: 0.000 (0.018)  Loss:  4.5898 (4.5969)  Acc@1:  0.0000 ( 1.6850)  Acc@5:  4.6875 ( 6.0968)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.012)  Loss:  4.5898 (4.5977)  Acc@1:  6.2500 ( 1.6500)  Acc@5:  6.2500 ( 6.0200)
tb
loss  0 : 4.6077375 2
loss  1 : 4.59516875 2
loss  2 : 4.59973125 2
loss  3 : 4.59994375 2
{'loss_different_0': 4.6077375, 'loss_different_1': 4.59516875, 'loss_different_2': 4.59973125, 'loss_different_3': 4.59994375}
Top1:  1.65
Train: 3 [   0/390 (  0%)]  Loss:  4.595036 (4.5950)  Time: 1.891s,   67.68/s  (1.891s,   67.68/s)  LR: 9.994e-05  Data: 0.999 (0.999)
Train: 3 [  50/390 ( 13%)]  Loss:  4.612244 (4.6036)  Time: 0.748s,  171.20/s  (0.770s,  166.17/s)  LR: 9.994e-05  Data: 0.000 (0.020)
Train: 3 [ 100/390 ( 26%)]  Loss:  4.588165 (4.5985)  Time: 0.748s,  171.11/s  (0.759s,  168.60/s)  LR: 9.994e-05  Data: 0.000 (0.010)
Train: 3 [ 150/390 ( 39%)]  Loss:  4.603973 (4.5999)  Time: 0.747s,  171.32/s  (0.758s,  168.90/s)  LR: 9.994e-05  Data: 0.001 (0.007)
Train: 3 [ 200/390 ( 51%)]  Loss:  4.585114 (4.5969)  Time: 0.747s,  171.31/s  (0.755s,  169.45/s)  LR: 9.994e-05  Data: 0.000 (0.005)
Train: 3 [ 250/390 ( 64%)]  Loss:  4.588685 (4.5955)  Time: 0.748s,  171.11/s  (0.754s,  169.78/s)  LR: 9.994e-05  Data: 0.001 (0.004)
Train: 3 [ 300/390 ( 77%)]  Loss:  4.575441 (4.5927)  Time: 0.748s,  171.21/s  (0.753s,  170.01/s)  LR: 9.994e-05  Data: 0.001 (0.004)
Train: 3 [ 350/390 ( 90%)]  Loss:  4.620615 (4.5962)  Time: 0.752s,  170.31/s  (0.752s,  170.17/s)  LR: 9.994e-05  Data: 0.003 (0.003)
Train: 3 [ 389/390 (100%)]  Loss:  4.596618 (4.5962)  Time: 0.748s,  171.17/s  (0.752s,  170.27/s)  LR: 9.994e-05  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.943 (0.943)  DataTime: 0.682 (0.682)  Loss:  4.5977 (4.5977)  Acc@1:  1.5625 ( 1.5625)  Acc@5:  3.9062 ( 3.9062)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.019)  Loss:  4.5898 (4.5898)  Acc@1:  0.7812 ( 1.8382)  Acc@5:  3.9062 ( 6.9547)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.013)  Loss:  4.5742 (4.5905)  Acc@1:  6.2500 ( 1.8600)  Acc@5: 12.5000 ( 6.8700)
tb
loss  0 : 4.6072875 3
loss  1 : 4.58373125 3
loss  2 : 4.59269375 3
loss  3 : 4.5900375 3
{'loss_different_0': 4.6072875, 'loss_different_1': 4.58373125, 'loss_different_2': 4.59269375, 'loss_different_3': 4.5900375}
Top1:  1.86
Train: 4 [   0/390 (  0%)]  Loss:  4.612623 (4.6126)  Time: 2.139s,   59.85/s  (2.139s,   59.85/s)  LR: 9.990e-05  Data: 1.382 (1.382)
Train: 4 [  50/390 ( 13%)]  Loss:  4.602796 (4.6077)  Time: 0.748s,  171.06/s  (0.775s,  165.12/s)  LR: 9.990e-05  Data: 0.000 (0.028)
Train: 4 [ 100/390 ( 26%)]  Loss:  4.601196 (4.6055)  Time: 0.748s,  171.22/s  (0.762s,  168.04/s)  LR: 9.990e-05  Data: 0.000 (0.014)
Train: 4 [ 150/390 ( 39%)]  Loss:  4.597139 (4.6034)  Time: 0.748s,  171.17/s  (0.757s,  169.05/s)  LR: 9.990e-05  Data: 0.001 (0.010)
Train: 4 [ 200/390 ( 51%)]  Loss:  4.607712 (4.6043)  Time: 0.748s,  171.16/s  (0.755s,  169.57/s)  LR: 9.990e-05  Data: 0.001 (0.007)
Train: 4 [ 250/390 ( 64%)]  Loss:  4.600372 (4.6036)  Time: 0.749s,  170.93/s  (0.753s,  169.88/s)  LR: 9.990e-05  Data: 0.001 (0.006)
Train: 4 [ 300/390 ( 77%)]  Loss:  4.590149 (4.6017)  Time: 0.748s,  171.06/s  (0.753s,  170.09/s)  LR: 9.990e-05  Data: 0.000 (0.005)
Train: 4 [ 350/390 ( 90%)]  Loss:  4.577851 (4.5987)  Time: 0.750s,  170.69/s  (0.752s,  170.24/s)  LR: 9.990e-05  Data: 0.003 (0.004)
Train: 4 [ 389/390 (100%)]  Loss:  4.585083 (4.5972)  Time: 0.747s,  171.27/s  (0.751s,  170.33/s)  LR: 9.990e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 0.981 (0.981)  DataTime: 0.713 (0.713)  Loss:  4.5938 (4.5938)  Acc@1:  1.5625 ( 1.5625)  Acc@5:  5.4688 ( 5.4688)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.014)  Loss:  4.5781 (4.5831)  Acc@1:  1.5625 ( 1.9608)  Acc@5:  7.8125 ( 7.8738)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.009)  Loss:  4.5703 (4.5838)  Acc@1:  6.2500 ( 2.0600)  Acc@5: 12.5000 ( 7.9300)
tb
loss  0 : 4.60713125 4
loss  1 : 4.57413125 4
loss  2 : 4.58494375 4
loss  3 : 4.58180625 4
{'loss_different_0': 4.60713125, 'loss_different_1': 4.57413125, 'loss_different_2': 4.58494375, 'loss_different_3': 4.58180625}
Top1:  2.06
Train: 5 [   0/390 (  0%)]  Loss:  4.589069 (4.5891)  Time: 1.872s,   68.37/s  (1.872s,   68.37/s)  LR: 9.985e-05  Data: 0.888 (0.888)
Train: 5 [  50/390 ( 13%)]  Loss:  4.604991 (4.5970)  Time: 0.747s,  171.27/s  (0.770s,  166.23/s)  LR: 9.985e-05  Data: 0.001 (0.018)
Train: 5 [ 100/390 ( 26%)]  Loss:  4.583252 (4.5924)  Time: 0.748s,  171.16/s  (0.762s,  168.01/s)  LR: 9.985e-05  Data: 0.000 (0.009)
Train: 5 [ 150/390 ( 39%)]  Loss:  4.593384 (4.5927)  Time: 0.748s,  171.15/s  (0.757s,  169.04/s)  LR: 9.985e-05  Data: 0.000 (0.006)
Train: 5 [ 200/390 ( 51%)]  Loss:  4.578190 (4.5898)  Time: 0.748s,  171.04/s  (0.755s,  169.56/s)  LR: 9.985e-05  Data: 0.001 (0.005)
Train: 5 [ 250/390 ( 64%)]  Loss:  4.581439 (4.5884)  Time: 0.748s,  171.23/s  (0.754s,  169.87/s)  LR: 9.985e-05  Data: 0.001 (0.004)
Train: 5 [ 300/390 ( 77%)]  Loss:  4.569183 (4.5856)  Time: 0.748s,  171.21/s  (0.753s,  170.09/s)  LR: 9.985e-05  Data: 0.000 (0.003)
Train: 5 [ 350/390 ( 90%)]  Loss:  4.599433 (4.5874)  Time: 0.751s,  170.50/s  (0.752s,  170.24/s)  LR: 9.985e-05  Data: 0.003 (0.003)
Train: 5 [ 389/390 (100%)]  Loss:  4.569370 (4.5854)  Time: 0.748s,  171.15/s  (0.751s,  170.34/s)  LR: 9.985e-05  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.346 (1.346)  DataTime: 0.920 (0.920)  Loss:  4.5820 (4.5820)  Acc@1:  2.3438 ( 2.3438)  Acc@5:  7.0312 ( 7.0312)
Test: [  50/78]  Time: 0.258 (0.280)  DataTime: 0.000 (0.018)  Loss:  4.5742 (4.5756)  Acc@1:  0.7812 ( 2.3744)  Acc@5:  7.0312 ( 9.2218)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.012)  Loss:  4.5664 (4.5765)  Acc@1:  6.2500 ( 2.3600)  Acc@5:  6.2500 ( 9.0900)
tb
loss  0 : 4.60708125 5
loss  1 : 4.56304375 5
loss  2 : 4.57750625 5
loss  3 : 4.5710625 5
{'loss_different_0': 4.60708125, 'loss_different_1': 4.56304375, 'loss_different_2': 4.57750625, 'loss_different_3': 4.5710625}
Top1:  2.36
Train: 6 [   0/390 (  0%)]  Loss:  4.591309 (4.5913)  Time: 2.180s,   58.72/s  (2.180s,   58.72/s)  LR: 9.978e-05  Data: 1.430 (1.430)
Train: 6 [  50/390 ( 13%)]  Loss:  4.578582 (4.5849)  Time: 0.748s,  171.14/s  (0.776s,  164.98/s)  LR: 9.978e-05  Data: 0.001 (0.028)
Train: 6 [ 100/390 ( 26%)]  Loss:  4.593928 (4.5879)  Time: 0.748s,  171.17/s  (0.762s,  167.98/s)  LR: 9.978e-05  Data: 0.001 (0.015)
Train: 6 [ 150/390 ( 39%)]  Loss:  4.577942 (4.5854)  Time: 0.747s,  171.27/s  (0.757s,  169.03/s)  LR: 9.978e-05  Data: 0.000 (0.010)
Train: 6 [ 200/390 ( 51%)]  Loss:  4.604654 (4.5893)  Time: 0.747s,  171.29/s  (0.755s,  169.56/s)  LR: 9.978e-05  Data: 0.000 (0.008)
Train: 6 [ 250/390 ( 64%)]  Loss:  4.609044 (4.5926)  Time: 0.748s,  171.07/s  (0.755s,  169.56/s)  LR: 9.978e-05  Data: 0.001 (0.006)
Train: 6 [ 300/390 ( 77%)]  Loss:  4.578857 (4.5906)  Time: 0.747s,  171.26/s  (0.754s,  169.83/s)  LR: 9.978e-05  Data: 0.000 (0.005)
Train: 6 [ 350/390 ( 90%)]  Loss:  4.576357 (4.5888)  Time: 0.750s,  170.77/s  (0.753s,  170.02/s)  LR: 9.978e-05  Data: 0.003 (0.004)
Train: 6 [ 389/390 (100%)]  Loss:  4.581171 (4.5880)  Time: 0.747s,  171.32/s  (0.752s,  170.14/s)  LR: 9.978e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.214 (1.214)  DataTime: 0.760 (0.760)  Loss:  4.5781 (4.5781)  Acc@1:  2.3438 ( 2.3438)  Acc@5:  7.0312 ( 7.0312)
Test: [  50/78]  Time: 0.259 (0.278)  DataTime: 0.000 (0.015)  Loss:  4.5664 (4.5695)  Acc@1:  1.5625 ( 2.6961)  Acc@5:  7.0312 (10.0797)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.010)  Loss:  4.5664 (4.5703)  Acc@1:  6.2500 ( 2.7400)  Acc@5: 12.5000 (10.1000)
tb
loss  0 : 4.60693125 6
loss  1 : 4.5526 6
loss  2 : 4.57026875 6
loss  3 : 4.56215625 6
{'loss_different_0': 4.60693125, 'loss_different_1': 4.5526, 'loss_different_2': 4.57026875, 'loss_different_3': 4.56215625}
Top1:  2.74
Train: 7 [   0/390 (  0%)]  Loss:  4.573885 (4.5739)  Time: 2.128s,   60.15/s  (2.128s,   60.15/s)  LR: 9.970e-05  Data: 1.378 (1.378)
Train: 7 [  50/390 ( 13%)]  Loss:  4.576752 (4.5753)  Time: 0.748s,  171.18/s  (0.775s,  165.14/s)  LR: 9.970e-05  Data: 0.001 (0.027)
Train: 7 [ 100/390 ( 26%)]  Loss:  4.562582 (4.5711)  Time: 0.748s,  171.14/s  (0.762s,  168.05/s)  LR: 9.970e-05  Data: 0.000 (0.014)
Train: 7 [ 150/390 ( 39%)]  Loss:  4.589111 (4.5756)  Time: 0.749s,  170.99/s  (0.757s,  169.05/s)  LR: 9.970e-05  Data: 0.001 (0.010)
Train: 7 [ 200/390 ( 51%)]  Loss:  4.571564 (4.5748)  Time: 0.748s,  171.17/s  (0.755s,  169.56/s)  LR: 9.970e-05  Data: 0.000 (0.007)
Train: 7 [ 250/390 ( 64%)]  Loss:  4.601365 (4.5792)  Time: 0.749s,  171.00/s  (0.754s,  169.86/s)  LR: 9.970e-05  Data: 0.000 (0.006)
Train: 7 [ 300/390 ( 77%)]  Loss:  4.580933 (4.5795)  Time: 0.747s,  171.25/s  (0.753s,  170.07/s)  LR: 9.970e-05  Data: 0.000 (0.005)
Train: 7 [ 350/390 ( 90%)]  Loss:  4.578001 (4.5793)  Time: 0.751s,  170.43/s  (0.752s,  170.21/s)  LR: 9.970e-05  Data: 0.003 (0.004)
Train: 7 [ 389/390 (100%)]  Loss:  4.563537 (4.5775)  Time: 0.748s,  171.18/s  (0.752s,  170.31/s)  LR: 9.970e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.550 (1.550)  DataTime: 1.281 (1.281)  Loss:  4.5664 (4.5664)  Acc@1:  3.1250 ( 3.1250)  Acc@5: 10.1562 (10.1562)
Test: [  50/78]  Time: 0.258 (0.284)  DataTime: 0.000 (0.026)  Loss:  4.5586 (4.5617)  Acc@1:  1.5625 ( 3.2169)  Acc@5: 10.1562 (11.9332)
Test: [  78/78]  Time: 0.035 (0.272)  DataTime: 0.000 (0.017)  Loss:  4.5547 (4.5626)  Acc@1:  6.2500 ( 3.2600)  Acc@5: 12.5000 (11.6500)
tb
loss  0 : 4.60693125 7
loss  1 : 4.54188125 7
loss  2 : 4.562875 7
loss  3 : 4.55203125 7
{'loss_different_0': 4.60693125, 'loss_different_1': 4.54188125, 'loss_different_2': 4.562875, 'loss_different_3': 4.55203125}
Top1:  3.26
Train: 8 [   0/390 (  0%)]  Loss:  4.567812 (4.5678)  Time: 1.609s,   79.56/s  (1.609s,   79.56/s)  LR: 9.961e-05  Data: 0.632 (0.632)
Train: 8 [  50/390 ( 13%)]  Loss:  4.575968 (4.5719)  Time: 0.748s,  171.08/s  (0.765s,  167.33/s)  LR: 9.961e-05  Data: 0.001 (0.013)
Train: 8 [ 100/390 ( 26%)]  Loss:  4.578674 (4.5742)  Time: 0.748s,  171.18/s  (0.757s,  169.18/s)  LR: 9.961e-05  Data: 0.000 (0.007)
Train: 8 [ 150/390 ( 39%)]  Loss:  4.581737 (4.5760)  Time: 0.748s,  171.02/s  (0.756s,  169.42/s)  LR: 9.961e-05  Data: 0.000 (0.005)
Train: 8 [ 200/390 ( 51%)]  Loss:  4.573014 (4.5754)  Time: 0.748s,  171.23/s  (0.754s,  169.83/s)  LR: 9.961e-05  Data: 0.000 (0.004)
Train: 8 [ 250/390 ( 64%)]  Loss:  4.579620 (4.5761)  Time: 0.748s,  171.04/s  (0.753s,  170.09/s)  LR: 9.961e-05  Data: 0.000 (0.003)
Train: 8 [ 300/390 ( 77%)]  Loss:  4.573497 (4.5758)  Time: 0.748s,  171.06/s  (0.752s,  170.26/s)  LR: 9.961e-05  Data: 0.000 (0.003)
Train: 8 [ 350/390 ( 90%)]  Loss:  4.569946 (4.5750)  Time: 0.750s,  170.58/s  (0.751s,  170.38/s)  LR: 9.961e-05  Data: 0.003 (0.002)
Train: 8 [ 389/390 (100%)]  Loss:  4.592587 (4.5770)  Time: 0.747s,  171.24/s  (0.751s,  170.46/s)  LR: 9.961e-05  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.301 (1.301)  DataTime: 0.888 (0.888)  Loss:  4.5547 (4.5547)  Acc@1:  3.9062 ( 3.9062)  Acc@5:  9.3750 ( 9.3750)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.018)  Loss:  4.5547 (4.5555)  Acc@1:  3.1250 ( 3.6152)  Acc@5:  9.3750 (12.9749)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.012)  Loss:  4.5508 (4.5567)  Acc@1:  6.2500 ( 3.5300)  Acc@5: 12.5000 (12.8700)
tb
loss  0 : 4.606725 8
loss  1 : 4.5329375 8
loss  2 : 4.5561625 8
loss  3 : 4.5442125 8
{'loss_different_0': 4.606725, 'loss_different_1': 4.5329375, 'loss_different_2': 4.5561625, 'loss_different_3': 4.5442125}
Top1:  3.53
Train: 9 [   0/390 (  0%)]  Loss:  4.574251 (4.5743)  Time: 2.074s,   61.72/s  (2.074s,   61.72/s)  LR: 9.950e-05  Data: 1.323 (1.323)
Train: 9 [  50/390 ( 13%)]  Loss:  4.560395 (4.5673)  Time: 0.747s,  171.34/s  (0.774s,  165.42/s)  LR: 9.950e-05  Data: 0.000 (0.026)
Train: 9 [ 100/390 ( 26%)]  Loss:  4.575500 (4.5700)  Time: 0.748s,  171.18/s  (0.761s,  168.21/s)  LR: 9.950e-05  Data: 0.000 (0.013)
Train: 9 [ 150/390 ( 39%)]  Loss:  4.569000 (4.5698)  Time: 0.748s,  171.16/s  (0.757s,  169.17/s)  LR: 9.950e-05  Data: 0.000 (0.009)
Train: 9 [ 200/390 ( 51%)]  Loss:  4.580266 (4.5719)  Time: 0.748s,  171.14/s  (0.754s,  169.67/s)  LR: 9.950e-05  Data: 0.001 (0.007)
Train: 9 [ 250/390 ( 64%)]  Loss:  4.559082 (4.5697)  Time: 0.747s,  171.27/s  (0.753s,  169.96/s)  LR: 9.950e-05  Data: 0.000 (0.006)
Train: 9 [ 300/390 ( 77%)]  Loss:  4.566589 (4.5693)  Time: 0.747s,  171.28/s  (0.753s,  169.89/s)  LR: 9.950e-05  Data: 0.000 (0.005)
Train: 9 [ 350/390 ( 90%)]  Loss:  4.567139 (4.5690)  Time: 0.750s,  170.63/s  (0.753s,  170.07/s)  LR: 9.950e-05  Data: 0.003 (0.004)
Train: 9 [ 389/390 (100%)]  Loss:  4.546387 (4.5665)  Time: 0.748s,  171.23/s  (0.752s,  170.19/s)  LR: 9.950e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.446 (1.446)  DataTime: 1.159 (1.159)  Loss:  4.5508 (4.5508)  Acc@1:  4.6875 ( 4.6875)  Acc@5: 13.2812 (13.2812)
Test: [  50/78]  Time: 0.258 (0.282)  DataTime: 0.000 (0.023)  Loss:  4.5430 (4.5487)  Acc@1:  4.6875 ( 4.0594)  Acc@5: 10.1562 (14.4455)
Test: [  78/78]  Time: 0.035 (0.271)  DataTime: 0.000 (0.015)  Loss:  4.5469 (4.5500)  Acc@1:  6.2500 ( 3.9500)  Acc@5: 18.7500 (14.2700)
tb
loss  0 : 4.60668125 9
loss  1 : 4.5239125 9
loss  2 : 4.54774375 9
loss  3 : 4.534825 9
{'loss_different_0': 4.60668125, 'loss_different_1': 4.5239125, 'loss_different_2': 4.54774375, 'loss_different_3': 4.534825}
Top1:  3.95
Train: 10 [   0/390 (  0%)]  Loss:  4.558014 (4.5580)  Time: 1.476s,   86.71/s  (1.476s,   86.71/s)  LR: 9.938e-05  Data: 0.568 (0.568)
Train: 10 [  50/390 ( 13%)]  Loss:  4.574578 (4.5663)  Time: 0.748s,  171.07/s  (0.762s,  167.95/s)  LR: 9.938e-05  Data: 0.000 (0.012)
Train: 10 [ 100/390 ( 26%)]  Loss:  4.560497 (4.5644)  Time: 0.747s,  171.30/s  (0.755s,  169.55/s)  LR: 9.938e-05  Data: 0.000 (0.006)
Train: 10 [ 150/390 ( 39%)]  Loss:  4.564819 (4.5645)  Time: 0.748s,  171.12/s  (0.753s,  170.08/s)  LR: 9.938e-05  Data: 0.000 (0.004)
Train: 10 [ 200/390 ( 51%)]  Loss:  4.572402 (4.5661)  Time: 0.748s,  171.06/s  (0.751s,  170.35/s)  LR: 9.938e-05  Data: 0.000 (0.003)
Train: 10 [ 250/390 ( 64%)]  Loss:  4.558472 (4.5648)  Time: 0.748s,  171.10/s  (0.751s,  170.52/s)  LR: 9.938e-05  Data: 0.000 (0.003)
Train: 10 [ 300/390 ( 77%)]  Loss:  4.567200 (4.5651)  Time: 0.748s,  171.18/s  (0.750s,  170.63/s)  LR: 9.938e-05  Data: 0.000 (0.002)
Train: 10 [ 350/390 ( 90%)]  Loss:  4.553619 (4.5637)  Time: 0.750s,  170.69/s  (0.750s,  170.71/s)  LR: 9.938e-05  Data: 0.003 (0.002)
Train: 10 [ 389/390 (100%)]  Loss:  4.554933 (4.5627)  Time: 0.747s,  171.28/s  (0.750s,  170.76/s)  LR: 9.938e-05  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.268 (1.268)  DataTime: 0.982 (0.982)  Loss:  4.5391 (4.5391)  Acc@1:  6.2500 ( 6.2500)  Acc@5: 15.6250 (15.6250)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.020)  Loss:  4.5391 (4.5402)  Acc@1:  3.9062 ( 4.5190)  Acc@5: 12.5000 (15.8548)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.013)  Loss:  4.5352 (4.5416)  Acc@1:  0.0000 ( 4.3600)  Acc@5: 25.0000 (15.5500)
tb
loss  0 : 4.606475 10
loss  1 : 4.5108375 10
loss  2 : 4.54063125 10
loss  3 : 4.52375625 10
{'loss_different_0': 4.606475, 'loss_different_1': 4.5108375, 'loss_different_2': 4.54063125, 'loss_different_3': 4.52375625}
Top1:  4.36
Saving model to ./output/train/20250828-175014-pretrain_cifar_100_using_resuming_cifar_10_lr_1-4/pretrain_cifar_100_using_resuming_cifar_10_lr_1-4_10.pt
Train: 11 [   0/390 (  0%)]  Loss:  4.558716 (4.5587)  Time: 2.114s,   60.55/s  (2.114s,   60.55/s)  LR: 9.926e-05  Data: 1.365 (1.365)
Train: 11 [  50/390 ( 13%)]  Loss:  4.550872 (4.5548)  Time: 0.747s,  171.31/s  (0.774s,  165.29/s)  LR: 9.926e-05  Data: 0.000 (0.027)
Train: 11 [ 100/390 ( 26%)]  Loss:  4.566300 (4.5586)  Time: 0.747s,  171.26/s  (0.761s,  168.17/s)  LR: 9.926e-05  Data: 0.000 (0.014)
Train: 11 [ 150/390 ( 39%)]  Loss:  4.545908 (4.5554)  Time: 0.748s,  171.20/s  (0.757s,  169.17/s)  LR: 9.926e-05  Data: 0.000 (0.009)
Train: 11 [ 200/390 ( 51%)]  Loss:  4.577243 (4.5598)  Time: 0.748s,  171.21/s  (0.756s,  169.38/s)  LR: 9.926e-05  Data: 0.000 (0.007)
Train: 11 [ 250/390 ( 64%)]  Loss:  4.579564 (4.5631)  Time: 0.749s,  170.99/s  (0.754s,  169.74/s)  LR: 9.926e-05  Data: 0.000 (0.006)
Train: 11 [ 300/390 ( 77%)]  Loss:  4.562073 (4.5630)  Time: 0.748s,  171.06/s  (0.753s,  169.98/s)  LR: 9.926e-05  Data: 0.000 (0.005)
Train: 11 [ 350/390 ( 90%)]  Loss:  4.557990 (4.5623)  Time: 0.750s,  170.57/s  (0.752s,  170.15/s)  LR: 9.926e-05  Data: 0.003 (0.004)
Train: 11 [ 389/390 (100%)]  Loss:  4.581109 (4.5644)  Time: 0.748s,  171.08/s  (0.752s,  170.26/s)  LR: 9.926e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.111 (1.111)  DataTime: 0.804 (0.804)  Loss:  4.5312 (4.5312)  Acc@1:  7.0312 ( 7.0312)  Acc@5: 17.1875 (17.1875)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.016)  Loss:  4.5312 (4.5338)  Acc@1:  4.6875 ( 4.8713)  Acc@5: 13.2812 (16.7126)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.011)  Loss:  4.5352 (4.5355)  Acc@1:  0.0000 ( 4.8500)  Acc@5: 18.7500 (16.5900)
tb
loss  0 : 4.606425 11
loss  1 : 4.50179375 11
loss  2 : 4.53270625 11
loss  3 : 4.516375 11
{'loss_different_0': 4.606425, 'loss_different_1': 4.50179375, 'loss_different_2': 4.53270625, 'loss_different_3': 4.516375}
Top1:  4.85
Train: 12 [   0/390 (  0%)]  Loss:  4.555359 (4.5554)  Time: 1.625s,   78.79/s  (1.625s,   78.79/s)  LR: 9.911e-05  Data: 0.786 (0.786)
Train: 12 [  50/390 ( 13%)]  Loss:  4.557835 (4.5566)  Time: 0.747s,  171.33/s  (0.765s,  167.34/s)  LR: 9.911e-05  Data: 0.000 (0.016)
Train: 12 [ 100/390 ( 26%)]  Loss:  4.552307 (4.5552)  Time: 0.748s,  171.10/s  (0.756s,  169.22/s)  LR: 9.911e-05  Data: 0.000 (0.008)
Train: 12 [ 150/390 ( 39%)]  Loss:  4.566528 (4.5580)  Time: 0.748s,  171.20/s  (0.754s,  169.87/s)  LR: 9.911e-05  Data: 0.000 (0.006)
Train: 12 [ 200/390 ( 51%)]  Loss:  4.545382 (4.5555)  Time: 0.748s,  171.15/s  (0.752s,  170.19/s)  LR: 9.911e-05  Data: 0.001 (0.004)
Train: 12 [ 250/390 ( 64%)]  Loss:  4.552351 (4.5550)  Time: 0.748s,  171.17/s  (0.751s,  170.39/s)  LR: 9.911e-05  Data: 0.000 (0.004)
Train: 12 [ 300/390 ( 77%)]  Loss:  4.556891 (4.5552)  Time: 0.748s,  171.21/s  (0.751s,  170.52/s)  LR: 9.911e-05  Data: 0.001 (0.003)
Train: 12 [ 350/390 ( 90%)]  Loss:  4.550543 (4.5546)  Time: 0.751s,  170.52/s  (0.751s,  170.39/s)  LR: 9.911e-05  Data: 0.003 (0.003)
Train: 12 [ 389/390 (100%)]  Loss:  4.548562 (4.5540)  Time: 0.747s,  171.43/s  (0.751s,  170.47/s)  LR: 9.911e-05  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.026 (1.026)  DataTime: 0.733 (0.733)  Loss:  4.5234 (4.5234)  Acc@1:  6.2500 ( 6.2500)  Acc@5: 16.4062 (16.4062)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.019)  Loss:  4.5273 (4.5265)  Acc@1:  3.9062 ( 5.1777)  Acc@5: 16.4062 (17.7543)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.012)  Loss:  4.5352 (4.5282)  Acc@1:  0.0000 ( 5.0100)  Acc@5: 18.7500 (17.4100)
tb
loss  0 : 4.605925 12
loss  1 : 4.49079375 12
loss  2 : 4.5261125 12
loss  3 : 4.5066625 12
{'loss_different_0': 4.605925, 'loss_different_1': 4.49079375, 'loss_different_2': 4.5261125, 'loss_different_3': 4.5066625}
Top1:  5.01
Train: 13 [   0/390 (  0%)]  Loss:  4.556579 (4.5566)  Time: 1.666s,   76.85/s  (1.666s,   76.85/s)  LR: 9.896e-05  Data: 0.749 (0.749)
Train: 13 [  50/390 ( 13%)]  Loss:  4.553473 (4.5550)  Time: 0.748s,  171.23/s  (0.766s,  167.14/s)  LR: 9.896e-05  Data: 0.000 (0.015)
Train: 13 [ 100/390 ( 26%)]  Loss:  4.545666 (4.5519)  Time: 0.747s,  171.31/s  (0.757s,  169.11/s)  LR: 9.896e-05  Data: 0.000 (0.008)
Train: 13 [ 150/390 ( 39%)]  Loss:  4.545593 (4.5503)  Time: 0.748s,  171.04/s  (0.754s,  169.79/s)  LR: 9.896e-05  Data: 0.000 (0.005)
Train: 13 [ 200/390 ( 51%)]  Loss:  4.566483 (4.5536)  Time: 0.748s,  171.14/s  (0.752s,  170.13/s)  LR: 9.896e-05  Data: 0.000 (0.004)
Train: 13 [ 250/390 ( 64%)]  Loss:  4.557472 (4.5542)  Time: 0.747s,  171.32/s  (0.751s,  170.34/s)  LR: 9.896e-05  Data: 0.000 (0.003)
Train: 13 [ 300/390 ( 77%)]  Loss:  4.528748 (4.5506)  Time: 0.747s,  171.32/s  (0.751s,  170.48/s)  LR: 9.896e-05  Data: 0.000 (0.003)
Train: 13 [ 350/390 ( 90%)]  Loss:  4.538116 (4.5490)  Time: 0.750s,  170.67/s  (0.750s,  170.57/s)  LR: 9.896e-05  Data: 0.003 (0.003)
Train: 13 [ 389/390 (100%)]  Loss:  4.557583 (4.5500)  Time: 0.747s,  171.40/s  (0.750s,  170.64/s)  LR: 9.896e-05  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.240 (1.240)  DataTime: 0.905 (0.905)  Loss:  4.5117 (4.5117)  Acc@1:  6.2500 ( 6.2500)  Acc@5: 20.3125 (20.3125)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.018)  Loss:  4.5195 (4.5184)  Acc@1:  4.6875 ( 5.4075)  Acc@5: 14.8438 (18.8419)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.012)  Loss:  4.5195 (4.5201)  Acc@1:  0.0000 ( 5.4300)  Acc@5: 18.7500 (18.3800)
tb
loss  0 : 4.60576875 13
loss  1 : 4.4787 13
loss  2 : 4.51763125 13
loss  3 : 4.49545 13
{'loss_different_0': 4.60576875, 'loss_different_1': 4.4787, 'loss_different_2': 4.51763125, 'loss_different_3': 4.49545}
Top1:  5.43
Train: 14 [   0/390 (  0%)]  Loss:  4.545776 (4.5458)  Time: 1.551s,   82.54/s  (1.551s,   82.54/s)  LR: 9.880e-05  Data: 0.596 (0.596)
Train: 14 [  50/390 ( 13%)]  Loss:  4.545685 (4.5457)  Time: 0.748s,  171.15/s  (0.764s,  167.63/s)  LR: 9.880e-05  Data: 0.000 (0.012)
Train: 14 [ 100/390 ( 26%)]  Loss:  4.532091 (4.5412)  Time: 0.748s,  171.16/s  (0.756s,  169.36/s)  LR: 9.880e-05  Data: 0.000 (0.006)
Train: 14 [ 150/390 ( 39%)]  Loss:  4.548035 (4.5429)  Time: 0.748s,  171.19/s  (0.753s,  169.96/s)  LR: 9.880e-05  Data: 0.000 (0.004)
Train: 14 [ 200/390 ( 51%)]  Loss:  4.545432 (4.5434)  Time: 0.748s,  171.13/s  (0.752s,  170.27/s)  LR: 9.880e-05  Data: 0.000 (0.003)
Train: 14 [ 250/390 ( 64%)]  Loss:  4.574735 (4.5486)  Time: 0.748s,  171.23/s  (0.751s,  170.45/s)  LR: 9.880e-05  Data: 0.000 (0.003)
Train: 14 [ 300/390 ( 77%)]  Loss:  4.551239 (4.5490)  Time: 0.748s,  171.07/s  (0.751s,  170.37/s)  LR: 9.880e-05  Data: 0.000 (0.002)
Train: 14 [ 350/390 ( 90%)]  Loss:  4.530035 (4.5466)  Time: 0.750s,  170.58/s  (0.751s,  170.48/s)  LR: 9.880e-05  Data: 0.003 (0.002)
Train: 14 [ 389/390 (100%)]  Loss:  4.540588 (4.5460)  Time: 0.747s,  171.32/s  (0.750s,  170.56/s)  LR: 9.880e-05  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.473 (1.473)  DataTime: 1.198 (1.198)  Loss:  4.5078 (4.5078)  Acc@1:  7.0312 ( 7.0312)  Acc@5: 21.0938 (21.0938)
Test: [  50/78]  Time: 0.259 (0.283)  DataTime: 0.000 (0.024)  Loss:  4.5195 (4.5136)  Acc@1:  4.6875 ( 5.9896)  Acc@5: 17.9688 (19.9449)
Test: [  78/78]  Time: 0.035 (0.271)  DataTime: 0.000 (0.015)  Loss:  4.5156 (4.5155)  Acc@1:  0.0000 ( 5.8700)  Acc@5: 25.0000 (19.5900)
tb
loss  0 : 4.60596875 14
loss  1 : 4.47279375 14
loss  2 : 4.51185625 14
loss  3 : 4.48830625 14
{'loss_different_0': 4.60596875, 'loss_different_1': 4.47279375, 'loss_different_2': 4.51185625, 'loss_different_3': 4.48830625}
Top1:  5.87
Train: 15 [   0/390 (  0%)]  Loss:  4.551855 (4.5519)  Time: 2.082s,   61.49/s  (2.082s,   61.49/s)  LR: 9.862e-05  Data: 1.332 (1.332)
Train: 15 [  50/390 ( 13%)]  Loss:  4.551847 (4.5519)  Time: 0.749s,  170.99/s  (0.774s,  165.36/s)  LR: 9.862e-05  Data: 0.001 (0.026)
Train: 15 [ 100/390 ( 26%)]  Loss:  4.531591 (4.5451)  Time: 0.749s,  170.92/s  (0.761s,  168.16/s)  LR: 9.862e-05  Data: 0.001 (0.014)
Train: 15 [ 150/390 ( 39%)]  Loss:  4.547333 (4.5457)  Time: 0.748s,  171.03/s  (0.757s,  169.13/s)  LR: 9.862e-05  Data: 0.001 (0.009)
Train: 15 [ 200/390 ( 51%)]  Loss:  4.569358 (4.5504)  Time: 0.747s,  171.31/s  (0.755s,  169.62/s)  LR: 9.862e-05  Data: 0.000 (0.007)
Train: 15 [ 250/390 ( 64%)]  Loss:  4.557588 (4.5516)  Time: 0.747s,  171.37/s  (0.753s,  169.92/s)  LR: 9.862e-05  Data: 0.000 (0.006)
Train: 15 [ 300/390 ( 77%)]  Loss:  4.547913 (4.5511)  Time: 0.748s,  171.05/s  (0.752s,  170.13/s)  LR: 9.862e-05  Data: 0.000 (0.005)
Train: 15 [ 350/390 ( 90%)]  Loss:  4.540009 (4.5497)  Time: 0.750s,  170.61/s  (0.752s,  170.28/s)  LR: 9.862e-05  Data: 0.003 (0.004)
Train: 15 [ 389/390 (100%)]  Loss:  4.519916 (4.5464)  Time: 0.747s,  171.30/s  (0.751s,  170.37/s)  LR: 9.862e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.080 (1.080)  DataTime: 0.787 (0.787)  Loss:  4.4961 (4.4961)  Acc@1:  7.0312 ( 7.0312)  Acc@5: 21.8750 (21.8750)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.016)  Loss:  4.5078 (4.5058)  Acc@1:  7.0312 ( 6.3879)  Acc@5: 17.1875 (20.4657)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.010)  Loss:  4.5117 (4.5078)  Acc@1:  0.0000 ( 6.2400)  Acc@5: 18.7500 (20.0600)
tb
loss  0 : 4.60576875 15
loss  1 : 4.462 15
loss  2 : 4.50390625 15
loss  3 : 4.4776875 15
{'loss_different_0': 4.60576875, 'loss_different_1': 4.462, 'loss_different_2': 4.50390625, 'loss_different_3': 4.4776875}
Top1:  6.24
Train: 16 [   0/390 (  0%)]  Loss:  4.539599 (4.5396)  Time: 2.028s,   63.10/s  (2.028s,   63.10/s)  LR: 9.843e-05  Data: 1.264 (1.264)
Train: 16 [  50/390 ( 13%)]  Loss:  4.529129 (4.5344)  Time: 0.748s,  171.16/s  (0.780s,  164.09/s)  LR: 9.843e-05  Data: 0.000 (0.025)
Train: 16 [ 100/390 ( 26%)]  Loss:  4.535817 (4.5348)  Time: 0.748s,  171.03/s  (0.764s,  167.50/s)  LR: 9.843e-05  Data: 0.000 (0.013)
Train: 16 [ 150/390 ( 39%)]  Loss:  4.532120 (4.5342)  Time: 0.748s,  171.14/s  (0.759s,  168.68/s)  LR: 9.843e-05  Data: 0.000 (0.009)
Train: 16 [ 200/390 ( 51%)]  Loss:  4.549593 (4.5373)  Time: 0.748s,  171.14/s  (0.756s,  169.29/s)  LR: 9.843e-05  Data: 0.000 (0.007)
Train: 16 [ 250/390 ( 64%)]  Loss:  4.540497 (4.5378)  Time: 0.747s,  171.29/s  (0.754s,  169.65/s)  LR: 9.843e-05  Data: 0.001 (0.005)
Train: 16 [ 300/390 ( 77%)]  Loss:  4.541888 (4.5384)  Time: 0.747s,  171.25/s  (0.753s,  169.91/s)  LR: 9.843e-05  Data: 0.000 (0.005)
Train: 16 [ 350/390 ( 90%)]  Loss:  4.552644 (4.5402)  Time: 0.751s,  170.46/s  (0.753s,  170.08/s)  LR: 9.843e-05  Data: 0.003 (0.004)
Train: 16 [ 389/390 (100%)]  Loss:  4.540709 (4.5402)  Time: 0.748s,  171.12/s  (0.752s,  170.19/s)  LR: 9.843e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.422 (1.422)  DataTime: 1.137 (1.137)  Loss:  4.4922 (4.4922)  Acc@1:  8.5938 ( 8.5938)  Acc@5: 21.0938 (21.0938)
Test: [  50/78]  Time: 0.259 (0.282)  DataTime: 0.000 (0.023)  Loss:  4.4961 (4.4976)  Acc@1:  5.4688 ( 6.3879)  Acc@5: 20.3125 (21.1857)
Test: [  78/78]  Time: 0.035 (0.271)  DataTime: 0.000 (0.015)  Loss:  4.5078 (4.4999)  Acc@1:  0.0000 ( 6.3100)  Acc@5: 25.0000 (20.6900)
tb
loss  0 : 4.60571875 16
loss  1 : 4.44979375 16
loss  2 : 4.4964625 16
loss  3 : 4.46665 16
{'loss_different_0': 4.60571875, 'loss_different_1': 4.44979375, 'loss_different_2': 4.4964625, 'loss_different_3': 4.46665}
Top1:  6.31
Train: 17 [   0/390 (  0%)]  Loss:  4.524109 (4.5241)  Time: 1.860s,   68.82/s  (1.860s,   68.82/s)  LR: 9.823e-05  Data: 1.058 (1.058)
Train: 17 [  50/390 ( 13%)]  Loss:  4.547392 (4.5358)  Time: 0.747s,  171.33/s  (0.770s,  166.29/s)  LR: 9.823e-05  Data: 0.000 (0.021)
Train: 17 [ 100/390 ( 26%)]  Loss:  4.544373 (4.5386)  Time: 0.748s,  171.03/s  (0.759s,  168.66/s)  LR: 9.823e-05  Data: 0.000 (0.011)
Train: 17 [ 150/390 ( 39%)]  Loss:  4.573281 (4.5473)  Time: 0.748s,  171.22/s  (0.755s,  169.48/s)  LR: 9.823e-05  Data: 0.001 (0.007)
Train: 17 [ 200/390 ( 51%)]  Loss:  4.521301 (4.5421)  Time: 0.748s,  171.23/s  (0.753s,  169.89/s)  LR: 9.823e-05  Data: 0.001 (0.006)
Train: 17 [ 250/390 ( 64%)]  Loss:  4.532976 (4.5406)  Time: 0.749s,  170.93/s  (0.752s,  170.13/s)  LR: 9.823e-05  Data: 0.001 (0.005)
Train: 17 [ 300/390 ( 77%)]  Loss:  4.520416 (4.5377)  Time: 0.748s,  171.08/s  (0.752s,  170.29/s)  LR: 9.823e-05  Data: 0.000 (0.004)
Train: 17 [ 350/390 ( 90%)]  Loss:  4.513019 (4.5346)  Time: 0.750s,  170.61/s  (0.752s,  170.24/s)  LR: 9.823e-05  Data: 0.003 (0.003)
Train: 17 [ 389/390 (100%)]  Loss:  4.534821 (4.5346)  Time: 0.747s,  171.33/s  (0.751s,  170.34/s)  LR: 9.823e-05  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.185 (1.185)  DataTime: 0.806 (0.806)  Loss:  4.4844 (4.4844)  Acc@1:  8.5938 ( 8.5938)  Acc@5: 21.8750 (21.8750)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.016)  Loss:  4.4922 (4.4903)  Acc@1:  7.0312 ( 6.7249)  Acc@5: 21.8750 (21.8137)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.011)  Loss:  4.5000 (4.4927)  Acc@1:  6.2500 ( 6.5700)  Acc@5: 12.5000 (21.2700)
tb
loss  0 : 4.60546875 17
loss  1 : 4.43943125 17
loss  2 : 4.48950625 17
loss  3 : 4.4560875 17
{'loss_different_0': 4.60546875, 'loss_different_1': 4.43943125, 'loss_different_2': 4.48950625, 'loss_different_3': 4.4560875}
Top1:  6.57
Train: 18 [   0/390 (  0%)]  Loss:  4.507648 (4.5076)  Time: 1.504s,   85.12/s  (1.504s,   85.12/s)  LR: 9.801e-05  Data: 0.571 (0.571)
Train: 18 [  50/390 ( 13%)]  Loss:  4.523879 (4.5158)  Time: 0.748s,  171.05/s  (0.763s,  167.81/s)  LR: 9.801e-05  Data: 0.000 (0.012)
Train: 18 [ 100/390 ( 26%)]  Loss:  4.544594 (4.5254)  Time: 0.747s,  171.27/s  (0.755s,  169.46/s)  LR: 9.801e-05  Data: 0.001 (0.006)
Train: 18 [ 150/390 ( 39%)]  Loss:  4.531815 (4.5270)  Time: 0.749s,  171.01/s  (0.753s,  170.01/s)  LR: 9.801e-05  Data: 0.000 (0.004)
Train: 18 [ 200/390 ( 51%)]  Loss:  4.573971 (4.5364)  Time: 0.747s,  171.25/s  (0.752s,  170.29/s)  LR: 9.801e-05  Data: 0.000 (0.003)
Train: 18 [ 250/390 ( 64%)]  Loss:  4.519633 (4.5336)  Time: 0.748s,  171.04/s  (0.751s,  170.46/s)  LR: 9.801e-05  Data: 0.001 (0.003)
Train: 18 [ 300/390 ( 77%)]  Loss:  4.506622 (4.5297)  Time: 0.748s,  171.21/s  (0.750s,  170.57/s)  LR: 9.801e-05  Data: 0.001 (0.002)
Train: 18 [ 350/390 ( 90%)]  Loss:  4.521440 (4.5287)  Time: 0.751s,  170.46/s  (0.750s,  170.65/s)  LR: 9.801e-05  Data: 0.003 (0.002)
Train: 18 [ 389/390 (100%)]  Loss:  4.519745 (4.5277)  Time: 0.747s,  171.24/s  (0.750s,  170.70/s)  LR: 9.801e-05  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.575 (1.575)  DataTime: 1.228 (1.228)  Loss:  4.4727 (4.4727)  Acc@1:  9.3750 ( 9.3750)  Acc@5: 21.0938 (21.0938)
Test: [  50/78]  Time: 0.259 (0.285)  DataTime: 0.000 (0.025)  Loss:  4.4883 (4.4830)  Acc@1:  7.0312 ( 6.8934)  Acc@5: 18.7500 (22.4571)
Test: [  78/78]  Time: 0.035 (0.273)  DataTime: 0.000 (0.016)  Loss:  4.4922 (4.4853)  Acc@1:  0.0000 ( 6.7700)  Acc@5: 18.7500 (21.9700)
tb
loss  0 : 4.6054625 18
loss  1 : 4.4289625 18
loss  2 : 4.4824 18
loss  3 : 4.44660625 18
{'loss_different_0': 4.6054625, 'loss_different_1': 4.4289625, 'loss_different_2': 4.4824, 'loss_different_3': 4.44660625}
Top1:  6.77
Train: 19 [   0/390 (  0%)]  Loss:  4.515914 (4.5159)  Time: 2.210s,   57.91/s  (2.210s,   57.91/s)  LR: 9.779e-05  Data: 1.461 (1.461)
Train: 19 [  50/390 ( 13%)]  Loss:  4.513056 (4.5145)  Time: 0.748s,  171.23/s  (0.777s,  164.79/s)  LR: 9.779e-05  Data: 0.001 (0.029)
Train: 19 [ 100/390 ( 26%)]  Loss:  4.508087 (4.5124)  Time: 0.749s,  170.96/s  (0.763s,  167.86/s)  LR: 9.779e-05  Data: 0.000 (0.015)
Train: 19 [ 150/390 ( 39%)]  Loss:  4.535583 (4.5182)  Time: 0.748s,  171.17/s  (0.758s,  168.92/s)  LR: 9.779e-05  Data: 0.000 (0.010)
Train: 19 [ 200/390 ( 51%)]  Loss:  4.513977 (4.5173)  Time: 0.747s,  171.24/s  (0.755s,  169.45/s)  LR: 9.779e-05  Data: 0.000 (0.008)
Train: 19 [ 250/390 ( 64%)]  Loss:  4.509768 (4.5161)  Time: 0.748s,  171.19/s  (0.755s,  169.52/s)  LR: 9.779e-05  Data: 0.000 (0.006)
Train: 19 [ 300/390 ( 77%)]  Loss:  4.552708 (4.5213)  Time: 0.748s,  171.24/s  (0.754s,  169.78/s)  LR: 9.779e-05  Data: 0.000 (0.005)
Train: 19 [ 350/390 ( 90%)]  Loss:  4.525970 (4.5219)  Time: 0.751s,  170.54/s  (0.753s,  169.96/s)  LR: 9.779e-05  Data: 0.003 (0.005)
Train: 19 [ 389/390 (100%)]  Loss:  4.503704 (4.5199)  Time: 0.748s,  171.22/s  (0.753s,  170.08/s)  LR: 9.779e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.118 (1.118)  DataTime: 0.809 (0.809)  Loss:  4.4688 (4.4688)  Acc@1:  8.5938 ( 8.5938)  Acc@5: 19.5312 (19.5312)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.020)  Loss:  4.4766 (4.4753)  Acc@1:  6.2500 ( 7.0619)  Acc@5: 19.5312 (23.2537)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.013)  Loss:  4.4766 (4.4778)  Acc@1:  0.0000 ( 6.8200)  Acc@5: 25.0000 (22.7400)
tb
loss  0 : 4.6049125 19
loss  1 : 4.4183375 19
loss  2 : 4.4716125 19
loss  3 : 4.43784375 19
{'loss_different_0': 4.6049125, 'loss_different_1': 4.4183375, 'loss_different_2': 4.4716125, 'loss_different_3': 4.43784375}
Top1:  6.82
Train: 20 [   0/390 (  0%)]  Loss:  4.530548 (4.5305)  Time: 1.638s,   78.14/s  (1.638s,   78.14/s)  LR: 9.755e-05  Data: 0.592 (0.592)
Train: 20 [  50/390 ( 13%)]  Loss:  4.537114 (4.5338)  Time: 0.748s,  171.15/s  (0.766s,  167.11/s)  LR: 9.755e-05  Data: 0.000 (0.012)
Train: 20 [ 100/390 ( 26%)]  Loss:  4.560212 (4.5426)  Time: 0.749s,  170.84/s  (0.757s,  169.04/s)  LR: 9.755e-05  Data: 0.001 (0.006)
Train: 20 [ 150/390 ( 39%)]  Loss:  4.524017 (4.5380)  Time: 0.748s,  171.08/s  (0.754s,  169.71/s)  LR: 9.755e-05  Data: 0.000 (0.004)
Train: 20 [ 200/390 ( 51%)]  Loss:  4.557163 (4.5418)  Time: 0.749s,  170.90/s  (0.753s,  170.03/s)  LR: 9.755e-05  Data: 0.001 (0.003)
Train: 20 [ 250/390 ( 64%)]  Loss:  4.521124 (4.5384)  Time: 0.749s,  170.86/s  (0.752s,  170.24/s)  LR: 9.755e-05  Data: 0.001 (0.003)
Train: 20 [ 300/390 ( 77%)]  Loss:  4.519954 (4.5357)  Time: 0.748s,  171.09/s  (0.751s,  170.38/s)  LR: 9.755e-05  Data: 0.000 (0.002)
Train: 20 [ 350/390 ( 90%)]  Loss:  4.523166 (4.5342)  Time: 0.750s,  170.64/s  (0.751s,  170.48/s)  LR: 9.755e-05  Data: 0.003 (0.002)
Train: 20 [ 389/390 (100%)]  Loss:  4.514917 (4.5320)  Time: 0.747s,  171.25/s  (0.751s,  170.34/s)  LR: 9.755e-05  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.280 (1.280)  DataTime: 0.785 (0.785)  Loss:  4.4570 (4.4570)  Acc@1:  9.3750 ( 9.3750)  Acc@5: 25.7812 (25.7812)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.016)  Loss:  4.4727 (4.4683)  Acc@1:  7.8125 ( 7.0772)  Acc@5: 21.8750 (24.1115)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.011)  Loss:  4.4766 (4.4709)  Acc@1:  0.0000 ( 7.0200)  Acc@5: 18.7500 (23.4200)
tb
loss  0 : 4.60461875 20
loss  1 : 4.4083125 20
loss  2 : 4.463275 20
loss  3 : 4.43119375 20
{'loss_different_0': 4.60461875, 'loss_different_1': 4.4083125, 'loss_different_2': 4.463275, 'loss_different_3': 4.43119375}
Top1:  7.02
Saving model to ./output/train/20250828-175014-pretrain_cifar_100_using_resuming_cifar_10_lr_1-4/pretrain_cifar_100_using_resuming_cifar_10_lr_1-4_20.pt
Train: 21 [   0/390 (  0%)]  Loss:  4.524292 (4.5243)  Time: 2.157s,   59.35/s  (2.157s,   59.35/s)  LR: 9.730e-05  Data: 1.408 (1.408)
Train: 21 [  50/390 ( 13%)]  Loss:  4.535437 (4.5299)  Time: 0.748s,  171.05/s  (0.776s,  165.02/s)  LR: 9.730e-05  Data: 0.001 (0.028)
Train: 21 [ 100/390 ( 26%)]  Loss:  4.527424 (4.5291)  Time: 0.749s,  170.87/s  (0.762s,  167.98/s)  LR: 9.730e-05  Data: 0.001 (0.014)
Train: 21 [ 150/390 ( 39%)]  Loss:  4.504186 (4.5228)  Time: 0.748s,  171.24/s  (0.757s,  169.00/s)  LR: 9.730e-05  Data: 0.001 (0.010)
Train: 21 [ 200/390 ( 51%)]  Loss:  4.518524 (4.5220)  Time: 0.748s,  171.04/s  (0.755s,  169.53/s)  LR: 9.730e-05  Data: 0.000 (0.007)
Train: 21 [ 250/390 ( 64%)]  Loss:  4.533482 (4.5239)  Time: 0.748s,  171.11/s  (0.754s,  169.85/s)  LR: 9.730e-05  Data: 0.000 (0.006)
Train: 21 [ 300/390 ( 77%)]  Loss:  4.501709 (4.5207)  Time: 0.748s,  171.05/s  (0.753s,  170.07/s)  LR: 9.730e-05  Data: 0.001 (0.005)
Train: 21 [ 350/390 ( 90%)]  Loss:  4.494812 (4.5175)  Time: 0.751s,  170.49/s  (0.752s,  170.22/s)  LR: 9.730e-05  Data: 0.003 (0.004)
Train: 21 [ 389/390 (100%)]  Loss:  4.498489 (4.5154)  Time: 0.748s,  171.21/s  (0.752s,  170.32/s)  LR: 9.730e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.045 (1.045)  DataTime: 0.748 (0.748)  Loss:  4.4492 (4.4492)  Acc@1: 10.9375 (10.9375)  Acc@5: 25.7812 (25.7812)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.015)  Loss:  4.4648 (4.4606)  Acc@1: 10.1562 ( 7.2151)  Acc@5: 23.4375 (24.5251)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.010)  Loss:  4.4766 (4.4630)  Acc@1:  0.0000 ( 7.0200)  Acc@5: 18.7500 (24.0300)
tb
loss  0 : 4.6050125 21
loss  1 : 4.39683125 21
loss  2 : 4.459925 21
loss  3 : 4.4171125 21
{'loss_different_0': 4.6050125, 'loss_different_1': 4.39683125, 'loss_different_2': 4.459925, 'loss_different_3': 4.4171125}
Top1:  7.02
Train: 22 [   0/390 (  0%)]  Loss:  4.498551 (4.4986)  Time: 1.990s,   64.31/s  (1.990s,   64.31/s)  LR: 9.704e-05  Data: 1.163 (1.163)
Train: 22 [  50/390 ( 13%)]  Loss:  4.513430 (4.5060)  Time: 0.748s,  171.12/s  (0.772s,  165.74/s)  LR: 9.704e-05  Data: 0.000 (0.023)
Train: 22 [ 100/390 ( 26%)]  Loss:  4.494289 (4.5021)  Time: 0.748s,  171.09/s  (0.760s,  168.36/s)  LR: 9.704e-05  Data: 0.000 (0.012)
Train: 22 [ 150/390 ( 39%)]  Loss:  4.509792 (4.5040)  Time: 0.748s,  171.08/s  (0.756s,  169.26/s)  LR: 9.704e-05  Data: 0.001 (0.008)
Train: 22 [ 200/390 ( 51%)]  Loss:  4.503988 (4.5040)  Time: 0.748s,  171.17/s  (0.754s,  169.73/s)  LR: 9.704e-05  Data: 0.000 (0.006)
Train: 22 [ 250/390 ( 64%)]  Loss:  4.502091 (4.5037)  Time: 0.748s,  171.20/s  (0.753s,  170.01/s)  LR: 9.704e-05  Data: 0.000 (0.005)
Train: 22 [ 300/390 ( 77%)]  Loss:  4.505222 (4.5039)  Time: 0.748s,  171.05/s  (0.753s,  169.97/s)  LR: 9.704e-05  Data: 0.000 (0.004)
Train: 22 [ 350/390 ( 90%)]  Loss:  4.550359 (4.5097)  Time: 0.751s,  170.43/s  (0.752s,  170.14/s)  LR: 9.704e-05  Data: 0.003 (0.004)
Train: 22 [ 389/390 (100%)]  Loss:  4.494665 (4.5080)  Time: 0.748s,  171.10/s  (0.752s,  170.24/s)  LR: 9.704e-05  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.542 (1.542)  DataTime: 1.104 (1.104)  Loss:  4.4375 (4.4375)  Acc@1: 10.9375 (10.9375)  Acc@5: 26.5625 (26.5625)
Test: [  50/78]  Time: 0.259 (0.284)  DataTime: 0.000 (0.022)  Loss:  4.4570 (4.4524)  Acc@1:  9.3750 ( 7.4449)  Acc@5: 21.8750 (24.7243)
Test: [  78/78]  Time: 0.035 (0.272)  DataTime: 0.000 (0.014)  Loss:  4.4648 (4.4553)  Acc@1:  0.0000 ( 7.2200)  Acc@5: 18.7500 (24.3800)
tb
loss  0 : 4.6045125 22
loss  1 : 4.38615 22
loss  2 : 4.44965625 22
loss  3 : 4.40900625 22
{'loss_different_0': 4.6045125, 'loss_different_1': 4.38615, 'loss_different_2': 4.44965625, 'loss_different_3': 4.40900625}
Top1:  7.22
Train: 23 [   0/390 (  0%)]  Loss:  4.552035 (4.5520)  Time: 2.184s,   58.60/s  (2.184s,   58.60/s)  LR: 9.677e-05  Data: 1.435 (1.435)
Train: 23 [  50/390 ( 13%)]  Loss:  4.488098 (4.5201)  Time: 0.748s,  171.14/s  (0.776s,  164.92/s)  LR: 9.677e-05  Data: 0.000 (0.029)
Train: 23 [ 100/390 ( 26%)]  Loss:  4.495775 (4.5120)  Time: 0.749s,  170.93/s  (0.762s,  167.93/s)  LR: 9.677e-05  Data: 0.000 (0.015)
Train: 23 [ 150/390 ( 39%)]  Loss:  4.500973 (4.5092)  Time: 0.747s,  171.29/s  (0.758s,  168.97/s)  LR: 9.677e-05  Data: 0.001 (0.010)
Train: 23 [ 200/390 ( 51%)]  Loss:  4.485340 (4.5044)  Time: 0.748s,  171.18/s  (0.755s,  169.51/s)  LR: 9.677e-05  Data: 0.000 (0.008)
Train: 23 [ 250/390 ( 64%)]  Loss:  4.486908 (4.5015)  Time: 0.748s,  171.01/s  (0.754s,  169.84/s)  LR: 9.677e-05  Data: 0.001 (0.006)
Train: 23 [ 300/390 ( 77%)]  Loss:  4.499274 (4.5012)  Time: 0.747s,  171.28/s  (0.753s,  170.05/s)  LR: 9.677e-05  Data: 0.001 (0.005)
Train: 23 [ 350/390 ( 90%)]  Loss:  4.501813 (4.5013)  Time: 0.750s,  170.56/s  (0.752s,  170.21/s)  LR: 9.677e-05  Data: 0.003 (0.005)
Train: 23 [ 389/390 (100%)]  Loss:  4.521183 (4.5035)  Time: 0.747s,  171.33/s  (0.752s,  170.31/s)  LR: 9.677e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.486 (1.486)  DataTime: 1.204 (1.204)  Loss:  4.4297 (4.4297)  Acc@1: 12.5000 (12.5000)  Acc@5: 28.1250 (28.1250)
Test: [  50/78]  Time: 0.259 (0.283)  DataTime: 0.000 (0.024)  Loss:  4.4492 (4.4436)  Acc@1:  7.8125 ( 7.4142)  Acc@5: 22.6562 (25.4136)
Test: [  78/78]  Time: 0.035 (0.272)  DataTime: 0.000 (0.016)  Loss:  4.4648 (4.4468)  Acc@1:  0.0000 ( 7.1500)  Acc@5: 18.7500 (25.0400)
tb
loss  0 : 4.60445625 23
loss  1 : 4.3734125 23
loss  2 : 4.44300625 23
loss  3 : 4.3967375 23
{'loss_different_0': 4.60445625, 'loss_different_1': 4.3734125, 'loss_different_2': 4.44300625, 'loss_different_3': 4.3967375}
Top1:  7.15
Train: 24 [   0/390 (  0%)]  Loss:  4.495641 (4.4956)  Time: 1.562s,   81.95/s  (1.562s,   81.95/s)  LR: 9.649e-05  Data: 0.644 (0.644)
Train: 24 [  50/390 ( 13%)]  Loss:  4.492323 (4.4940)  Time: 0.748s,  171.14/s  (0.771s,  165.97/s)  LR: 9.649e-05  Data: 0.000 (0.013)
Train: 24 [ 100/390 ( 26%)]  Loss:  4.511603 (4.4999)  Time: 0.748s,  171.07/s  (0.760s,  168.49/s)  LR: 9.649e-05  Data: 0.000 (0.007)
Train: 24 [ 150/390 ( 39%)]  Loss:  4.489182 (4.4972)  Time: 0.749s,  170.92/s  (0.756s,  169.37/s)  LR: 9.649e-05  Data: 0.000 (0.005)
Train: 24 [ 200/390 ( 51%)]  Loss:  4.488022 (4.4954)  Time: 0.748s,  171.09/s  (0.754s,  169.81/s)  LR: 9.649e-05  Data: 0.001 (0.004)
Train: 24 [ 250/390 ( 64%)]  Loss:  4.528522 (4.5009)  Time: 0.748s,  171.22/s  (0.753s,  170.08/s)  LR: 9.649e-05  Data: 0.000 (0.003)
Train: 24 [ 300/390 ( 77%)]  Loss:  4.490655 (4.4994)  Time: 0.748s,  171.21/s  (0.752s,  170.26/s)  LR: 9.649e-05  Data: 0.000 (0.003)
Train: 24 [ 350/390 ( 90%)]  Loss:  4.511241 (4.5009)  Time: 0.751s,  170.47/s  (0.751s,  170.38/s)  LR: 9.649e-05  Data: 0.003 (0.002)
Train: 24 [ 389/390 (100%)]  Loss:  4.484985 (4.4991)  Time: 0.747s,  171.29/s  (0.751s,  170.47/s)  LR: 9.649e-05  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.401 (1.401)  DataTime: 1.130 (1.130)  Loss:  4.4219 (4.4219)  Acc@1: 11.7188 (11.7188)  Acc@5: 25.7812 (25.7812)
Test: [  50/78]  Time: 0.259 (0.281)  DataTime: 0.000 (0.023)  Loss:  4.4453 (4.4359)  Acc@1:  9.3750 ( 7.3223)  Acc@5: 25.0000 (26.3940)
Test: [  78/78]  Time: 0.035 (0.271)  DataTime: 0.000 (0.015)  Loss:  4.4648 (4.4391)  Acc@1:  6.2500 ( 7.2600)  Acc@5: 18.7500 (25.7300)
tb
loss  0 : 4.60405625 24
loss  1 : 4.3650625 24
loss  2 : 4.43361875 24
loss  3 : 4.3866375 24
{'loss_different_0': 4.60405625, 'loss_different_1': 4.3650625, 'loss_different_2': 4.43361875, 'loss_different_3': 4.3866375}
Top1:  7.26
Train: 25 [   0/390 (  0%)]  Loss:  4.488800 (4.4888)  Time: 1.693s,   75.60/s  (1.693s,   75.60/s)  LR: 9.619e-05  Data: 0.845 (0.845)
Train: 25 [  50/390 ( 13%)]  Loss:  4.491807 (4.4903)  Time: 0.748s,  171.01/s  (0.767s,  166.99/s)  LR: 9.619e-05  Data: 0.000 (0.017)
Train: 25 [ 100/390 ( 26%)]  Loss:  4.499100 (4.4932)  Time: 0.747s,  171.24/s  (0.757s,  168.99/s)  LR: 9.619e-05  Data: 0.000 (0.009)
Train: 25 [ 150/390 ( 39%)]  Loss:  4.539333 (4.5048)  Time: 0.748s,  171.12/s  (0.754s,  169.69/s)  LR: 9.619e-05  Data: 0.001 (0.006)
Train: 25 [ 200/390 ( 51%)]  Loss:  4.490295 (4.5019)  Time: 0.748s,  171.15/s  (0.753s,  170.05/s)  LR: 9.619e-05  Data: 0.001 (0.005)
Train: 25 [ 250/390 ( 64%)]  Loss:  4.503902 (4.5022)  Time: 0.748s,  171.16/s  (0.752s,  170.26/s)  LR: 9.619e-05  Data: 0.001 (0.004)
Train: 25 [ 300/390 ( 77%)]  Loss:  4.518635 (4.5046)  Time: 0.748s,  171.08/s  (0.751s,  170.40/s)  LR: 9.619e-05  Data: 0.000 (0.003)
Train: 25 [ 350/390 ( 90%)]  Loss:  4.505234 (4.5046)  Time: 0.751s,  170.43/s  (0.752s,  170.31/s)  LR: 9.619e-05  Data: 0.003 (0.003)
Train: 25 [ 389/390 (100%)]  Loss:  4.490523 (4.5031)  Time: 0.747s,  171.26/s  (0.751s,  170.39/s)  LR: 9.619e-05  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.344 (1.344)  DataTime: 0.918 (0.918)  Loss:  4.4141 (4.4141)  Acc@1: 11.7188 (11.7188)  Acc@5: 25.0000 (25.0000)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.018)  Loss:  4.4336 (4.4288)  Acc@1:  6.2500 ( 7.4602)  Acc@5: 24.2188 (26.7157)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.012)  Loss:  4.4453 (4.4325)  Acc@1:  0.0000 ( 7.2400)  Acc@5: 18.7500 (25.9000)
tb
loss  0 : 4.60405625 25
loss  1 : 4.35525 25
loss  2 : 4.4257 25
loss  3 : 4.37850625 25
{'loss_different_0': 4.60405625, 'loss_different_1': 4.35525, 'loss_different_2': 4.4257, 'loss_different_3': 4.37850625}
Top1:  7.24
Train: 26 [   0/390 (  0%)]  Loss:  4.528752 (4.5288)  Time: 2.180s,   58.71/s  (2.180s,   58.71/s)  LR: 9.589e-05  Data: 1.431 (1.431)
Train: 26 [  50/390 ( 13%)]  Loss:  4.500900 (4.5148)  Time: 0.748s,  171.02/s  (0.776s,  164.89/s)  LR: 9.589e-05  Data: 0.001 (0.028)
Train: 26 [ 100/390 ( 26%)]  Loss:  4.495565 (4.5084)  Time: 0.748s,  171.14/s  (0.762s,  167.91/s)  LR: 9.589e-05  Data: 0.000 (0.015)
Train: 26 [ 150/390 ( 39%)]  Loss:  4.465390 (4.4977)  Time: 0.749s,  170.94/s  (0.758s,  168.95/s)  LR: 9.589e-05  Data: 0.000 (0.010)
Train: 26 [ 200/390 ( 51%)]  Loss:  4.497468 (4.4976)  Time: 0.748s,  171.22/s  (0.755s,  169.49/s)  LR: 9.589e-05  Data: 0.000 (0.008)
Train: 26 [ 250/390 ( 64%)]  Loss:  4.543804 (4.5053)  Time: 0.748s,  171.22/s  (0.754s,  169.82/s)  LR: 9.589e-05  Data: 0.000 (0.006)
Train: 26 [ 300/390 ( 77%)]  Loss:  4.502858 (4.5050)  Time: 0.747s,  171.27/s  (0.753s,  170.04/s)  LR: 9.589e-05  Data: 0.000 (0.005)
Train: 26 [ 350/390 ( 90%)]  Loss:  4.465134 (4.5000)  Time: 0.752s,  170.32/s  (0.752s,  170.20/s)  LR: 9.589e-05  Data: 0.004 (0.005)
Train: 26 [ 389/390 (100%)]  Loss:  4.447052 (4.4941)  Time: 0.747s,  171.43/s  (0.752s,  170.30/s)  LR: 9.589e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.144 (1.144)  DataTime: 0.753 (0.753)  Loss:  4.4023 (4.4023)  Acc@1: 11.7188 (11.7188)  Acc@5: 29.6875 (29.6875)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.015)  Loss:  4.4219 (4.4199)  Acc@1:  7.0312 ( 7.4142)  Acc@5: 26.5625 (27.2059)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.010)  Loss:  4.4453 (4.4234)  Acc@1:  0.0000 ( 7.2400)  Acc@5: 18.7500 (26.4400)
tb
loss  0 : 4.6037 26
loss  1 : 4.3425 26
loss  2 : 4.4180125 26
loss  3 : 4.3657375 26
{'loss_different_0': 4.6037, 'loss_different_1': 4.3425, 'loss_different_2': 4.4180125, 'loss_different_3': 4.3657375}
Top1:  7.24
Train: 27 [   0/390 (  0%)]  Loss:  4.466673 (4.4667)  Time: 2.054s,   62.33/s  (2.054s,   62.33/s)  LR: 9.557e-05  Data: 1.299 (1.299)
Train: 27 [  50/390 ( 13%)]  Loss:  4.454733 (4.4607)  Time: 0.747s,  171.26/s  (0.773s,  165.49/s)  LR: 9.557e-05  Data: 0.000 (0.026)
Train: 27 [ 100/390 ( 26%)]  Loss:  4.529605 (4.4837)  Time: 0.747s,  171.38/s  (0.761s,  168.26/s)  LR: 9.557e-05  Data: 0.000 (0.013)
Train: 27 [ 150/390 ( 39%)]  Loss:  4.467963 (4.4797)  Time: 0.748s,  171.21/s  (0.756s,  169.22/s)  LR: 9.557e-05  Data: 0.001 (0.009)
Train: 27 [ 200/390 ( 51%)]  Loss:  4.480103 (4.4798)  Time: 0.748s,  171.21/s  (0.754s,  169.70/s)  LR: 9.557e-05  Data: 0.000 (0.007)
Train: 27 [ 250/390 ( 64%)]  Loss:  4.554729 (4.4923)  Time: 0.748s,  171.17/s  (0.754s,  169.74/s)  LR: 9.557e-05  Data: 0.001 (0.006)
Train: 27 [ 300/390 ( 77%)]  Loss:  4.471893 (4.4894)  Time: 0.748s,  171.09/s  (0.753s,  169.97/s)  LR: 9.557e-05  Data: 0.001 (0.005)
Train: 27 [ 350/390 ( 90%)]  Loss:  4.483541 (4.4887)  Time: 0.750s,  170.68/s  (0.752s,  170.14/s)  LR: 9.557e-05  Data: 0.003 (0.004)
Train: 27 [ 389/390 (100%)]  Loss:  4.494659 (4.4893)  Time: 0.747s,  171.31/s  (0.752s,  170.25/s)  LR: 9.557e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.104 (1.104)  DataTime: 0.777 (0.777)  Loss:  4.3945 (4.3945)  Acc@1: 13.2812 (13.2812)  Acc@5: 28.9062 (28.9062)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.016)  Loss:  4.4180 (4.4114)  Acc@1:  9.3750 ( 7.8278)  Acc@5: 23.4375 (27.2825)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.010)  Loss:  4.4414 (4.4152)  Acc@1:  0.0000 ( 7.5100)  Acc@5: 18.7500 (26.4900)
tb
loss  0 : 4.6037 27
loss  1 : 4.33051875 27
loss  2 : 4.4094875 27
loss  3 : 4.35548125 27
{'loss_different_0': 4.6037, 'loss_different_1': 4.33051875, 'loss_different_2': 4.4094875, 'loss_different_3': 4.35548125}
Top1:  7.51
Train: 28 [   0/390 (  0%)]  Loss:  4.479589 (4.4796)  Time: 2.067s,   61.94/s  (2.067s,   61.94/s)  LR: 9.524e-05  Data: 1.317 (1.317)
Train: 28 [  50/390 ( 13%)]  Loss:  4.436344 (4.4580)  Time: 0.748s,  171.03/s  (0.774s,  165.44/s)  LR: 9.524e-05  Data: 0.000 (0.026)
Train: 28 [ 100/390 ( 26%)]  Loss:  4.481079 (4.4657)  Time: 0.749s,  171.00/s  (0.761s,  168.24/s)  LR: 9.524e-05  Data: 0.001 (0.013)
Train: 28 [ 150/390 ( 39%)]  Loss:  4.513041 (4.4775)  Time: 0.749s,  171.00/s  (0.756s,  169.20/s)  LR: 9.524e-05  Data: 0.001 (0.009)
Train: 28 [ 200/390 ( 51%)]  Loss:  4.464402 (4.4749)  Time: 0.747s,  171.24/s  (0.754s,  169.69/s)  LR: 9.524e-05  Data: 0.000 (0.007)
Train: 28 [ 250/390 ( 64%)]  Loss:  4.467394 (4.4736)  Time: 0.747s,  171.25/s  (0.753s,  169.98/s)  LR: 9.524e-05  Data: 0.000 (0.006)
Train: 28 [ 300/390 ( 77%)]  Loss:  4.491624 (4.4762)  Time: 0.748s,  171.12/s  (0.752s,  170.18/s)  LR: 9.524e-05  Data: 0.001 (0.005)
Train: 28 [ 350/390 ( 90%)]  Loss:  4.523625 (4.4821)  Time: 0.749s,  170.80/s  (0.752s,  170.32/s)  LR: 9.524e-05  Data: 0.003 (0.004)
Train: 28 [ 389/390 (100%)]  Loss:  4.479996 (4.4819)  Time: 0.747s,  171.27/s  (0.751s,  170.41/s)  LR: 9.524e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.476 (1.476)  DataTime: 1.177 (1.177)  Loss:  4.3828 (4.3828)  Acc@1: 12.5000 (12.5000)  Acc@5: 28.9062 (28.9062)
Test: [  50/78]  Time: 0.259 (0.283)  DataTime: 0.000 (0.023)  Loss:  4.4062 (4.4047)  Acc@1: 10.9375 ( 7.7359)  Acc@5: 25.7812 (26.8382)
Test: [  78/78]  Time: 0.035 (0.271)  DataTime: 0.000 (0.015)  Loss:  4.4375 (4.4088)  Acc@1:  0.0000 ( 7.5200)  Acc@5: 18.7500 (26.4800)
tb
loss  0 : 4.60355 28
loss  1 : 4.3226375 28
loss  2 : 4.40505625 28
loss  3 : 4.3439625 28
{'loss_different_0': 4.60355, 'loss_different_1': 4.3226375, 'loss_different_2': 4.40505625, 'loss_different_3': 4.3439625}
Top1:  7.52
Train: 29 [   0/390 (  0%)]  Loss:  4.474197 (4.4742)  Time: 2.075s,   61.69/s  (2.075s,   61.69/s)  LR: 9.490e-05  Data: 1.325 (1.325)
Train: 29 [  50/390 ( 13%)]  Loss:  4.449341 (4.4618)  Time: 0.748s,  171.09/s  (0.780s,  164.00/s)  LR: 9.490e-05  Data: 0.000 (0.026)
Train: 29 [ 100/390 ( 26%)]  Loss:  4.475983 (4.4665)  Time: 0.748s,  171.17/s  (0.764s,  167.48/s)  LR: 9.490e-05  Data: 0.000 (0.013)
Train: 29 [ 150/390 ( 39%)]  Loss:  4.490566 (4.4725)  Time: 0.748s,  171.15/s  (0.759s,  168.69/s)  LR: 9.490e-05  Data: 0.000 (0.009)
Train: 29 [ 200/390 ( 51%)]  Loss:  4.456733 (4.4694)  Time: 0.747s,  171.39/s  (0.756s,  169.30/s)  LR: 9.490e-05  Data: 0.001 (0.007)
Train: 29 [ 250/390 ( 64%)]  Loss:  4.430121 (4.4628)  Time: 0.747s,  171.30/s  (0.754s,  169.68/s)  LR: 9.490e-05  Data: 0.000 (0.006)
Train: 29 [ 300/390 ( 77%)]  Loss:  4.445849 (4.4604)  Time: 0.748s,  171.11/s  (0.753s,  169.92/s)  LR: 9.490e-05  Data: 0.000 (0.005)
Train: 29 [ 350/390 ( 90%)]  Loss:  4.429779 (4.4566)  Time: 0.750s,  170.74/s  (0.752s,  170.10/s)  LR: 9.490e-05  Data: 0.003 (0.004)
Train: 29 [ 389/390 (100%)]  Loss:  4.444740 (4.4553)  Time: 0.747s,  171.37/s  (0.752s,  170.22/s)  LR: 9.490e-05  Data: 0.000 (0.004)
Test: [   0/78]  Time: 1.119 (1.119)  DataTime: 0.757 (0.757)  Loss:  4.3789 (4.3789)  Acc@1: 10.1562 (10.1562)  Acc@5: 29.6875 (29.6875)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.015)  Loss:  4.4023 (4.3948)  Acc@1:  9.3750 ( 7.9504)  Acc@5: 25.0000 (27.5123)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.010)  Loss:  4.4180 (4.3984)  Acc@1:  0.0000 ( 7.6600)  Acc@5: 18.7500 (27.1000)
tb
loss  0 : 4.6034 29
loss  1 : 4.307775 29
loss  2 : 4.39505 29
loss  3 : 4.33116875 29
{'loss_different_0': 4.6034, 'loss_different_1': 4.307775, 'loss_different_2': 4.39505, 'loss_different_3': 4.33116875}
Top1:  7.66
Train: 30 [   0/390 (  0%)]  Loss:  4.440767 (4.4408)  Time: 1.986s,   64.46/s  (1.986s,   64.46/s)  LR: 9.455e-05  Data: 1.204 (1.204)
Train: 30 [  50/390 ( 13%)]  Loss:  4.466853 (4.4538)  Time: 0.748s,  171.16/s  (0.772s,  165.76/s)  LR: 9.455e-05  Data: 0.000 (0.024)
Train: 30 [ 100/390 ( 26%)]  Loss:  4.462387 (4.4567)  Time: 0.748s,  171.22/s  (0.760s,  168.38/s)  LR: 9.455e-05  Data: 0.000 (0.012)
Train: 30 [ 150/390 ( 39%)]  Loss:  4.444122 (4.4535)  Time: 0.748s,  171.12/s  (0.756s,  169.29/s)  LR: 9.455e-05  Data: 0.001 (0.008)
Train: 30 [ 200/390 ( 51%)]  Loss:  4.544512 (4.4717)  Time: 0.748s,  171.02/s  (0.754s,  169.74/s)  LR: 9.455e-05  Data: 0.000 (0.006)
Train: 30 [ 250/390 ( 64%)]  Loss:  4.480576 (4.4732)  Time: 0.748s,  171.17/s  (0.753s,  170.01/s)  LR: 9.455e-05  Data: 0.001 (0.005)
Train: 30 [ 300/390 ( 77%)]  Loss:  4.445450 (4.4692)  Time: 0.748s,  171.15/s  (0.752s,  170.20/s)  LR: 9.455e-05  Data: 0.000 (0.004)
Train: 30 [ 350/390 ( 90%)]  Loss:  4.501527 (4.4733)  Time: 0.750s,  170.60/s  (0.752s,  170.15/s)  LR: 9.455e-05  Data: 0.003 (0.004)
Train: 30 [ 389/390 (100%)]  Loss:  4.452203 (4.4709)  Time: 0.748s,  171.23/s  (0.752s,  170.26/s)  LR: 9.455e-05  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.957 (0.957)  DataTime: 0.686 (0.686)  Loss:  4.3711 (4.3711)  Acc@1: 10.9375 (10.9375)  Acc@5: 28.1250 (28.1250)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.014)  Loss:  4.3906 (4.3886)  Acc@1:  9.3750 ( 7.9197)  Acc@5: 28.9062 (28.1556)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  4.4180 (4.3929)  Acc@1:  0.0000 ( 7.6100)  Acc@5: 12.5000 (27.5900)
tb
loss  0 : 4.6034 30
loss  1 : 4.30216875 30
loss  2 : 4.38655 30
loss  3 : 4.3254375 30
{'loss_different_0': 4.6034, 'loss_different_1': 4.30216875, 'loss_different_2': 4.38655, 'loss_different_3': 4.3254375}
Top1:  7.61
Saving model to ./output/train/20250828-175014-pretrain_cifar_100_using_resuming_cifar_10_lr_1-4/pretrain_cifar_100_using_resuming_cifar_10_lr_1-4_30.pt
Train: 31 [   0/390 (  0%)]  Loss:  4.515684 (4.5157)  Time: 1.791s,   71.45/s  (1.791s,   71.45/s)  LR: 9.419e-05  Data: 0.901 (0.901)
Train: 31 [  50/390 ( 13%)]  Loss:  4.448700 (4.4822)  Time: 0.748s,  171.12/s  (0.769s,  166.55/s)  LR: 9.419e-05  Data: 0.000 (0.018)
Train: 31 [ 100/390 ( 26%)]  Loss:  4.516000 (4.4935)  Time: 0.748s,  171.21/s  (0.758s,  168.80/s)  LR: 9.419e-05  Data: 0.000 (0.009)
Train: 31 [ 150/390 ( 39%)]  Loss:  4.454376 (4.4837)  Time: 0.748s,  171.14/s  (0.755s,  169.57/s)  LR: 9.419e-05  Data: 0.000 (0.006)
Train: 31 [ 200/390 ( 51%)]  Loss:  4.485358 (4.4840)  Time: 0.748s,  171.11/s  (0.753s,  169.95/s)  LR: 9.419e-05  Data: 0.000 (0.005)
Train: 31 [ 250/390 ( 64%)]  Loss:  4.442430 (4.4771)  Time: 0.748s,  171.09/s  (0.752s,  170.18/s)  LR: 9.419e-05  Data: 0.000 (0.004)