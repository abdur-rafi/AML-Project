2025-08-29 09:21:05.036624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1756459265.060099     115 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1756459265.067074     115 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
create dataset:  cifar100
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Creating model..
, number of params: 27599012
SGD!!!
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Train: 0 [   0/390 (  0%)]  Loss:  4.620230 (4.6202)  Time: 3.303s,   38.75/s  (3.303s,   38.75/s)  LR: 1.000e-02  Data: 1.313 (1.313)
Train: 0 [  50/390 ( 13%)]  Loss:  4.547287 (4.5838)  Time: 0.750s,  170.67/s  (0.800s,  160.06/s)  LR: 1.000e-02  Data: 0.000 (0.026)
Train: 0 [ 100/390 ( 26%)]  Loss:  4.420311 (4.5293)  Time: 0.749s,  170.87/s  (0.775s,  165.25/s)  LR: 1.000e-02  Data: 0.000 (0.013)
Train: 0 [ 150/390 ( 39%)]  Loss:  4.312146 (4.4750)  Time: 0.749s,  170.86/s  (0.766s,  167.09/s)  LR: 1.000e-02  Data: 0.000 (0.009)
Train: 0 [ 200/390 ( 51%)]  Loss:  4.228627 (4.4257)  Time: 0.749s,  170.83/s  (0.762s,  168.03/s)  LR: 1.000e-02  Data: 0.000 (0.007)
Train: 0 [ 250/390 ( 64%)]  Loss:  4.197632 (4.3877)  Time: 0.748s,  171.07/s  (0.759s,  168.60/s)  LR: 1.000e-02  Data: 0.000 (0.006)
Train: 0 [ 300/390 ( 77%)]  Loss:  4.359242 (4.3836)  Time: 0.749s,  170.86/s  (0.758s,  168.98/s)  LR: 1.000e-02  Data: 0.000 (0.005)
Train: 0 [ 350/390 ( 90%)]  Loss:  4.027890 (4.3392)  Time: 0.751s,  170.43/s  (0.756s,  169.25/s)  LR: 1.000e-02  Data: 0.003 (0.004)
Train: 0 [ 389/390 (100%)]  Loss:  3.843529 (4.2841)  Time: 0.750s,  170.76/s  (0.755s,  169.43/s)  LR: 1.000e-02  Data: 0.000 (0.004)
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Test: [   0/78]  Time: 1.376 (1.376)  DataTime: 0.979 (0.979)  Loss:  3.5410 (3.5410)  Acc@1: 21.0938 (21.0938)  Acc@5: 48.4375 (48.4375)
Test: [  50/78]  Time: 0.259 (0.281)  DataTime: 0.000 (0.020)  Loss:  3.7227 (3.6677)  Acc@1: 11.7188 (13.9246)  Acc@5: 41.4062 (42.6164)
Test: [  78/78]  Time: 0.043 (0.271)  DataTime: 0.000 (0.013)  Loss:  3.7734 (3.6775)  Acc@1:  6.2500 (13.6200)  Acc@5: 37.5000 (42.2800)
tb
loss  0 : 4.52036875 0
loss  1 : 3.52234375 0
loss  2 : 3.83064375 0
loss  3 : 3.54080625 0
{'loss_different_0': 4.52036875, 'loss_different_1': 3.52234375, 'loss_different_2': 3.83064375, 'loss_different_3': 3.54080625}
Top1:  13.62
Saving model to ./output/train/20250829-092114-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_3/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_3_0.pt
Train: 1 [   0/390 (  0%)]  Loss:  3.744178 (3.7442)  Time: 1.617s,   79.16/s  (1.617s,   79.16/s)  LR: 9.999e-03  Data: 0.868 (0.868)
Train: 1 [  50/390 ( 13%)]  Loss:  3.858139 (3.8012)  Time: 0.749s,  171.00/s  (0.773s,  165.52/s)  LR: 9.999e-03  Data: 0.000 (0.017)
Train: 1 [ 100/390 ( 26%)]  Loss:  3.715298 (3.7725)  Time: 0.749s,  170.85/s  (0.761s,  168.14/s)  LR: 9.999e-03  Data: 0.000 (0.009)
Train: 1 [ 150/390 ( 39%)]  Loss:  3.842659 (3.7901)  Time: 0.750s,  170.69/s  (0.757s,  169.05/s)  LR: 9.999e-03  Data: 0.000 (0.006)
Train: 1 [ 200/390 ( 51%)]  Loss:  3.400863 (3.7122)  Time: 0.749s,  171.00/s  (0.755s,  169.51/s)  LR: 9.999e-03  Data: 0.000 (0.005)
Train: 1 [ 250/390 ( 64%)]  Loss:  3.555350 (3.6861)  Time: 0.749s,  170.98/s  (0.754s,  169.78/s)  LR: 9.999e-03  Data: 0.000 (0.004)
Train: 1 [ 300/390 ( 77%)]  Loss:  3.460178 (3.6538)  Time: 0.748s,  171.17/s  (0.753s,  169.97/s)  LR: 9.999e-03  Data: 0.000 (0.003)
Train: 1 [ 350/390 ( 90%)]  Loss:  3.282106 (3.6073)  Time: 0.752s,  170.32/s  (0.752s,  170.11/s)  LR: 9.999e-03  Data: 0.003 (0.003)
Train: 1 [ 389/390 (100%)]  Loss:  3.846013 (3.6339)  Time: 0.748s,  171.05/s  (0.752s,  170.19/s)  LR: 9.999e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.067 (1.067)  DataTime: 0.739 (0.739)  Loss:  2.8555 (2.8555)  Acc@1: 27.3438 (27.3438)  Acc@5: 62.5000 (62.5000)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.015)  Loss:  3.0586 (2.9755)  Acc@1: 24.2188 (23.6979)  Acc@5: 60.1562 (60.7077)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.010)  Loss:  3.0840 (2.9872)  Acc@1:  6.2500 (23.3200)  Acc@5: 56.2500 (60.1900)
tb
loss  0 : 4.0115375 1
loss  1 : 2.88825 1
loss  2 : 3.378359375 1
loss  3 : 2.958434375 1
{'loss_different_0': 4.0115375, 'loss_different_1': 2.88825, 'loss_different_2': 3.378359375, 'loss_different_3': 2.958434375}
Top1:  23.32
Train: 2 [   0/390 (  0%)]  Loss:  3.288182 (3.2882)  Time: 1.301s,   98.37/s  (1.301s,   98.37/s)  LR: 9.998e-03  Data: 0.431 (0.431)
Train: 2 [  50/390 ( 13%)]  Loss:  3.090017 (3.1891)  Time: 0.748s,  171.16/s  (0.760s,  168.49/s)  LR: 9.998e-03  Data: 0.000 (0.009)
Train: 2 [ 100/390 ( 26%)]  Loss:  3.271347 (3.2165)  Time: 0.749s,  170.93/s  (0.754s,  169.66/s)  LR: 9.998e-03  Data: 0.000 (0.005)
Train: 2 [ 150/390 ( 39%)]  Loss:  3.138275 (3.1970)  Time: 0.749s,  170.91/s  (0.753s,  170.07/s)  LR: 9.998e-03  Data: 0.000 (0.003)
Train: 2 [ 200/390 ( 51%)]  Loss:  3.005197 (3.1586)  Time: 0.749s,  170.84/s  (0.753s,  169.92/s)  LR: 9.998e-03  Data: 0.000 (0.003)
Train: 2 [ 250/390 ( 64%)]  Loss:  3.009497 (3.1338)  Time: 0.750s,  170.61/s  (0.752s,  170.11/s)  LR: 9.998e-03  Data: 0.000 (0.002)
Train: 2 [ 300/390 ( 77%)]  Loss:  3.280342 (3.1547)  Time: 0.748s,  171.03/s  (0.752s,  170.24/s)  LR: 9.998e-03  Data: 0.000 (0.002)
Train: 2 [ 350/390 ( 90%)]  Loss:  3.011707 (3.1368)  Time: 0.752s,  170.25/s  (0.751s,  170.33/s)  LR: 9.998e-03  Data: 0.003 (0.002)
Train: 2 [ 389/390 (100%)]  Loss:  3.143046 (3.1375)  Time: 0.749s,  170.85/s  (0.751s,  170.39/s)  LR: 9.998e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.001 (1.001)  DataTime: 0.723 (0.723)  Loss:  2.4551 (2.4551)  Acc@1: 36.7188 (36.7188)  Acc@5: 75.7812 (75.7812)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.015)  Loss:  2.6816 (2.5800)  Acc@1: 32.0312 (33.0576)  Acc@5: 64.8438 (69.9908)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.010)  Loss:  2.7402 (2.5875)  Acc@1: 25.0000 (32.6300)  Acc@5: 68.7500 (69.6900)
tb
loss  0 : 3.38044375 2
loss  1 : 2.538765625 2
loss  2 : 2.992084375 2
loss  3 : 2.671275 2
{'loss_different_0': 3.38044375, 'loss_different_1': 2.538765625, 'loss_different_2': 2.992084375, 'loss_different_3': 2.671275}
Top1:  32.63
Train: 3 [   0/390 (  0%)]  Loss:  2.931662 (2.9317)  Time: 1.654s,   77.38/s  (1.654s,   77.38/s)  LR: 9.994e-03  Data: 0.903 (0.903)
Train: 3 [  50/390 ( 13%)]  Loss:  2.825569 (2.8786)  Time: 0.749s,  170.85/s  (0.767s,  166.92/s)  LR: 9.994e-03  Data: 0.000 (0.018)
Train: 3 [ 100/390 ( 26%)]  Loss:  2.787550 (2.8483)  Time: 0.749s,  170.80/s  (0.758s,  168.88/s)  LR: 9.994e-03  Data: 0.000 (0.009)
Train: 3 [ 150/390 ( 39%)]  Loss:  2.723084 (2.8170)  Time: 0.749s,  170.94/s  (0.755s,  169.54/s)  LR: 9.994e-03  Data: 0.000 (0.006)
Train: 3 [ 200/390 ( 51%)]  Loss:  2.757343 (2.8050)  Time: 0.749s,  170.91/s  (0.754s,  169.87/s)  LR: 9.994e-03  Data: 0.001 (0.005)
Train: 3 [ 250/390 ( 64%)]  Loss:  2.698348 (2.7873)  Time: 0.749s,  170.84/s  (0.753s,  170.08/s)  LR: 9.994e-03  Data: 0.000 (0.004)
Train: 3 [ 300/390 ( 77%)]  Loss:  2.642313 (2.7666)  Time: 0.748s,  171.07/s  (0.752s,  170.22/s)  LR: 9.994e-03  Data: 0.000 (0.003)
Train: 3 [ 350/390 ( 90%)]  Loss:  2.853835 (2.7775)  Time: 0.752s,  170.23/s  (0.752s,  170.31/s)  LR: 9.994e-03  Data: 0.003 (0.003)
Train: 3 [ 389/390 (100%)]  Loss:  2.672795 (2.7658)  Time: 0.747s,  171.36/s  (0.751s,  170.38/s)  LR: 9.994e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.225 (1.225)  DataTime: 0.951 (0.951)  Loss:  2.1992 (2.1992)  Acc@1: 39.8438 (39.8438)  Acc@5: 77.3438 (77.3438)
Test: [  50/78]  Time: 0.260 (0.278)  DataTime: 0.000 (0.019)  Loss:  2.4082 (2.3168)  Acc@1: 39.8438 (38.0055)  Acc@5: 66.4062 (74.1268)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.012)  Loss:  2.4746 (2.3168)  Acc@1: 25.0000 (38.2000)  Acc@5: 75.0000 (74.2200)
tb
loss  0 : 3.00423125 3
loss  1 : 2.308209375 3
loss  2 : 2.78141875 3
loss  3 : 2.44776875 3
{'loss_different_0': 3.00423125, 'loss_different_1': 2.308209375, 'loss_different_2': 2.78141875, 'loss_different_3': 2.44776875}
Top1:  38.2
Train: 4 [   0/390 (  0%)]  Loss:  2.736918 (2.7369)  Time: 1.653s,   77.42/s  (1.653s,   77.42/s)  LR: 9.990e-03  Data: 0.904 (0.904)
Train: 4 [  50/390 ( 13%)]  Loss:  3.015391 (2.8762)  Time: 0.749s,  170.97/s  (0.767s,  166.95/s)  LR: 9.990e-03  Data: 0.000 (0.018)
Train: 4 [ 100/390 ( 26%)]  Loss:  2.519209 (2.7572)  Time: 0.749s,  170.84/s  (0.758s,  168.89/s)  LR: 9.990e-03  Data: 0.000 (0.009)
Train: 4 [ 150/390 ( 39%)]  Loss:  2.513585 (2.6963)  Time: 0.748s,  171.20/s  (0.756s,  169.29/s)  LR: 9.990e-03  Data: 0.000 (0.006)
Train: 4 [ 200/390 ( 51%)]  Loss:  2.526523 (2.6623)  Time: 0.747s,  171.32/s  (0.754s,  169.79/s)  LR: 9.990e-03  Data: 0.000 (0.005)
Train: 4 [ 250/390 ( 64%)]  Loss:  2.583562 (2.6492)  Time: 0.747s,  171.43/s  (0.753s,  170.09/s)  LR: 9.990e-03  Data: 0.000 (0.004)
Train: 4 [ 300/390 ( 77%)]  Loss:  2.658582 (2.6505)  Time: 0.747s,  171.30/s  (0.752s,  170.29/s)  LR: 9.990e-03  Data: 0.000 (0.003)
Train: 4 [ 350/390 ( 90%)]  Loss:  2.557815 (2.6389)  Time: 0.750s,  170.77/s  (0.751s,  170.43/s)  LR: 9.990e-03  Data: 0.003 (0.003)
Train: 4 [ 389/390 (100%)]  Loss:  3.379538 (2.7212)  Time: 0.747s,  171.39/s  (0.751s,  170.52/s)  LR: 9.990e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.269 (1.269)  DataTime: 0.867 (0.867)  Loss:  2.1211 (2.1211)  Acc@1: 47.6562 (47.6562)  Acc@5: 75.0000 (75.0000)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.017)  Loss:  2.2266 (2.1763)  Acc@1: 43.7500 (42.0956)  Acc@5: 77.3438 (76.0570)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.011)  Loss:  2.4121 (2.1773)  Acc@1: 43.7500 (42.0200)  Acc@5: 68.7500 (76.1200)
tb
loss  0 : 2.803096875 4
loss  1 : 2.220640625 4
loss  2 : 2.711590625 4
loss  3 : 2.4038875 4
{'loss_different_0': 2.803096875, 'loss_different_1': 2.220640625, 'loss_different_2': 2.711590625, 'loss_different_3': 2.4038875}
Top1:  42.02
Train: 5 [   0/390 (  0%)]  Loss:  2.565916 (2.5659)  Time: 1.489s,   85.98/s  (1.489s,   85.98/s)  LR: 9.985e-03  Data: 0.700 (0.700)
Train: 5 [  50/390 ( 13%)]  Loss:  2.464797 (2.5154)  Time: 0.747s,  171.25/s  (0.762s,  168.02/s)  LR: 9.985e-03  Data: 0.000 (0.014)
Train: 5 [ 100/390 ( 26%)]  Loss:  2.441514 (2.4907)  Time: 0.746s,  171.50/s  (0.755s,  169.63/s)  LR: 9.985e-03  Data: 0.000 (0.007)
Train: 5 [ 150/390 ( 39%)]  Loss:  2.423081 (2.4738)  Time: 0.748s,  171.24/s  (0.752s,  170.18/s)  LR: 9.985e-03  Data: 0.000 (0.005)
Train: 5 [ 200/390 ( 51%)]  Loss:  2.438129 (2.4667)  Time: 0.748s,  171.14/s  (0.751s,  170.45/s)  LR: 9.985e-03  Data: 0.001 (0.004)
Train: 5 [ 250/390 ( 64%)]  Loss:  2.453991 (2.4646)  Time: 0.748s,  171.21/s  (0.750s,  170.62/s)  LR: 9.985e-03  Data: 0.000 (0.003)
Train: 5 [ 300/390 ( 77%)]  Loss:  2.358045 (2.4494)  Time: 0.747s,  171.46/s  (0.751s,  170.49/s)  LR: 9.985e-03  Data: 0.000 (0.003)
Train: 5 [ 350/390 ( 90%)]  Loss:  2.656216 (2.4752)  Time: 0.750s,  170.69/s  (0.750s,  170.61/s)  LR: 9.985e-03  Data: 0.003 (0.002)
Train: 5 [ 389/390 (100%)]  Loss:  2.658178 (2.4955)  Time: 0.747s,  171.30/s  (0.750s,  170.68/s)  LR: 9.985e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.353 (1.353)  DataTime: 1.091 (1.091)  Loss:  1.8613 (1.8613)  Acc@1: 50.7812 (50.7812)  Acc@5: 81.2500 (81.2500)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.022)  Loss:  1.9785 (1.9486)  Acc@1: 53.1250 (46.6759)  Acc@5: 78.9062 (80.2543)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.014)  Loss:  2.3301 (1.9571)  Acc@1: 31.2500 (46.6700)  Acc@5: 68.7500 (80.1600)
tb
loss  0 : 2.681221875 5
loss  1 : 2.0312875 5
loss  2 : 2.513175 5
loss  3 : 2.209334375 5
{'loss_different_0': 2.681221875, 'loss_different_1': 2.0312875, 'loss_different_2': 2.513175, 'loss_different_3': 2.209334375}
Top1:  46.67
Train: 6 [   0/390 (  0%)]  Loss:  2.421751 (2.4218)  Time: 1.078s,  118.73/s  (1.078s,  118.73/s)  LR: 9.978e-03  Data: 0.226 (0.226)
Train: 6 [  50/390 ( 13%)]  Loss:  2.389039 (2.4054)  Time: 0.748s,  171.21/s  (0.754s,  169.85/s)  LR: 9.978e-03  Data: 0.000 (0.005)
Train: 6 [ 100/390 ( 26%)]  Loss:  2.400543 (2.4038)  Time: 0.747s,  171.32/s  (0.750s,  170.57/s)  LR: 9.978e-03  Data: 0.000 (0.003)
Train: 6 [ 150/390 ( 39%)]  Loss:  2.467318 (2.4197)  Time: 0.747s,  171.25/s  (0.749s,  170.81/s)  LR: 9.978e-03  Data: 0.000 (0.002)
Train: 6 [ 200/390 ( 51%)]  Loss:  3.785473 (2.6928)  Time: 0.747s,  171.26/s  (0.749s,  170.92/s)  LR: 9.978e-03  Data: 0.000 (0.002)
Train: 6 [ 250/390 ( 64%)]  Loss:  2.266933 (2.6218)  Time: 0.748s,  171.20/s  (0.749s,  171.00/s)  LR: 9.978e-03  Data: 0.000 (0.001)
Train: 6 [ 300/390 ( 77%)]  Loss:  2.252002 (2.5690)  Time: 0.747s,  171.25/s  (0.748s,  171.05/s)  LR: 9.978e-03  Data: 0.000 (0.001)
Train: 6 [ 350/390 ( 90%)]  Loss:  2.369591 (2.5441)  Time: 0.750s,  170.62/s  (0.748s,  171.08/s)  LR: 9.978e-03  Data: 0.003 (0.001)
Train: 6 [ 389/390 (100%)]  Loss:  2.260520 (2.5126)  Time: 0.747s,  171.26/s  (0.748s,  171.11/s)  LR: 9.978e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 0.915 (0.915)  DataTime: 0.622 (0.622)  Loss:  1.8076 (1.8076)  Acc@1: 45.3125 (45.3125)  Acc@5: 82.8125 (82.8125)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.013)  Loss:  1.9219 (1.8639)  Acc@1: 46.8750 (49.0502)  Acc@5: 79.6875 (81.3266)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  2.2559 (1.8653)  Acc@1: 31.2500 (49.3700)  Acc@5: 62.5000 (81.6400)
tb
loss  0 : 2.5155375 6
loss  1 : 1.968884375 6
loss  2 : 2.39688125 6
loss  3 : 2.1783 6
{'loss_different_0': 2.5155375, 'loss_different_1': 1.968884375, 'loss_different_2': 2.39688125, 'loss_different_3': 2.1783}
Top1:  49.37
Train: 7 [   0/390 (  0%)]  Loss:  2.249096 (2.2491)  Time: 1.402s,   91.30/s  (1.402s,   91.30/s)  LR: 9.970e-03  Data: 0.640 (0.640)
Train: 7 [  50/390 ( 13%)]  Loss:  2.319584 (2.2843)  Time: 0.748s,  171.20/s  (0.760s,  168.36/s)  LR: 9.970e-03  Data: 0.000 (0.013)
Train: 7 [ 100/390 ( 26%)]  Loss:  2.327219 (2.2986)  Time: 0.747s,  171.44/s  (0.754s,  169.79/s)  LR: 9.970e-03  Data: 0.000 (0.007)
Train: 7 [ 150/390 ( 39%)]  Loss:  2.331077 (2.3067)  Time: 0.747s,  171.28/s  (0.752s,  170.27/s)  LR: 9.970e-03  Data: 0.000 (0.005)
Train: 7 [ 200/390 ( 51%)]  Loss:  2.170774 (2.2795)  Time: 0.747s,  171.28/s  (0.752s,  170.28/s)  LR: 9.970e-03  Data: 0.000 (0.004)
Train: 7 [ 250/390 ( 64%)]  Loss:  2.231829 (2.2716)  Time: 0.748s,  171.23/s  (0.751s,  170.47/s)  LR: 9.970e-03  Data: 0.000 (0.003)
Train: 7 [ 300/390 ( 77%)]  Loss:  2.213698 (2.2633)  Time: 0.747s,  171.25/s  (0.750s,  170.60/s)  LR: 9.970e-03  Data: 0.000 (0.002)
Train: 7 [ 350/390 ( 90%)]  Loss:  2.092657 (2.2420)  Time: 0.751s,  170.52/s  (0.750s,  170.69/s)  LR: 9.970e-03  Data: 0.003 (0.002)
Train: 7 [ 389/390 (100%)]  Loss:  2.034242 (2.2189)  Time: 0.747s,  171.31/s  (0.750s,  170.75/s)  LR: 9.970e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.096 (1.096)  DataTime: 0.821 (0.821)  Loss:  1.6377 (1.6377)  Acc@1: 53.1250 (53.1250)  Acc@5: 84.3750 (84.3750)
Test: [  50/78]  Time: 0.259 (0.275)  DataTime: 0.000 (0.016)  Loss:  1.8486 (1.7660)  Acc@1: 45.3125 (50.3983)  Acc@5: 83.5938 (83.1495)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  2.0527 (1.7699)  Acc@1: 43.7500 (50.6800)  Acc@5: 75.0000 (83.0600)
tb
loss  0 : 2.357240625 7
loss  1 : 1.926 7
loss  2 : 2.283778125 7
loss  3 : 2.079025 7
{'loss_different_0': 2.357240625, 'loss_different_1': 1.926, 'loss_different_2': 2.283778125, 'loss_different_3': 2.079025}
Top1:  50.68
Train: 8 [   0/390 (  0%)]  Loss:  2.545368 (2.5454)  Time: 1.628s,   78.65/s  (1.628s,   78.65/s)  LR: 9.961e-03  Data: 0.879 (0.879)
Train: 8 [  50/390 ( 13%)]  Loss:  2.417444 (2.4814)  Time: 0.747s,  171.33/s  (0.765s,  167.40/s)  LR: 9.961e-03  Data: 0.000 (0.018)
Train: 8 [ 100/390 ( 26%)]  Loss:  2.296081 (2.4196)  Time: 0.747s,  171.30/s  (0.756s,  169.30/s)  LR: 9.961e-03  Data: 0.000 (0.009)
Train: 8 [ 150/390 ( 39%)]  Loss:  2.220328 (2.3698)  Time: 0.747s,  171.25/s  (0.753s,  169.94/s)  LR: 9.961e-03  Data: 0.000 (0.006)
Train: 8 [ 200/390 ( 51%)]  Loss:  2.300767 (2.3560)  Time: 0.747s,  171.28/s  (0.752s,  170.26/s)  LR: 9.961e-03  Data: 0.000 (0.005)
Train: 8 [ 250/390 ( 64%)]  Loss:  2.064239 (2.3074)  Time: 0.747s,  171.34/s  (0.751s,  170.47/s)  LR: 9.961e-03  Data: 0.001 (0.004)
Train: 8 [ 300/390 ( 77%)]  Loss:  2.017878 (2.2660)  Time: 0.747s,  171.31/s  (0.750s,  170.60/s)  LR: 9.961e-03  Data: 0.001 (0.003)
Train: 8 [ 350/390 ( 90%)]  Loss:  2.132387 (2.2493)  Time: 0.750s,  170.75/s  (0.751s,  170.51/s)  LR: 9.961e-03  Data: 0.003 (0.003)
Train: 8 [ 389/390 (100%)]  Loss:  3.902273 (2.4330)  Time: 0.747s,  171.41/s  (0.750s,  170.59/s)  LR: 9.961e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.008 (1.008)  DataTime: 0.742 (0.742)  Loss:  1.6641 (1.6641)  Acc@1: 56.2500 (56.2500)  Acc@5: 83.5938 (83.5938)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.015)  Loss:  1.7480 (1.7116)  Acc@1: 47.6562 (53.2475)  Acc@5: 83.5938 (83.8082)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.010)  Loss:  1.9668 (1.7159)  Acc@1: 43.7500 (53.1500)  Acc@5: 75.0000 (83.7700)
tb
loss  0 : 2.30425 8
loss  1 : 1.887375 8
loss  2 : 2.246659375 8
loss  3 : 2.0513 8
{'loss_different_0': 2.30425, 'loss_different_1': 1.887375, 'loss_different_2': 2.246659375, 'loss_different_3': 2.0513}
Top1:  53.15
Train: 9 [   0/390 (  0%)]  Loss:  1.941370 (1.9414)  Time: 1.609s,   79.55/s  (1.609s,   79.55/s)  LR: 9.950e-03  Data: 0.860 (0.860)
Train: 9 [  50/390 ( 13%)]  Loss:  1.926108 (1.9337)  Time: 0.747s,  171.40/s  (0.764s,  167.52/s)  LR: 9.950e-03  Data: 0.000 (0.017)
Train: 9 [ 100/390 ( 26%)]  Loss:  1.916731 (1.9281)  Time: 0.747s,  171.32/s  (0.756s,  169.38/s)  LR: 9.950e-03  Data: 0.000 (0.009)
Train: 9 [ 150/390 ( 39%)]  Loss:  3.534959 (2.3298)  Time: 0.747s,  171.25/s  (0.753s,  170.01/s)  LR: 9.950e-03  Data: 0.000 (0.006)
Train: 9 [ 200/390 ( 51%)]  Loss:  2.129855 (2.2898)  Time: 0.747s,  171.33/s  (0.751s,  170.33/s)  LR: 9.950e-03  Data: 0.000 (0.005)
Train: 9 [ 250/390 ( 64%)]  Loss:  2.158136 (2.2679)  Time: 0.747s,  171.38/s  (0.751s,  170.53/s)  LR: 9.950e-03  Data: 0.000 (0.004)
Train: 9 [ 300/390 ( 77%)]  Loss:  2.001243 (2.2298)  Time: 0.747s,  171.24/s  (0.750s,  170.65/s)  LR: 9.950e-03  Data: 0.000 (0.003)
Train: 9 [ 350/390 ( 90%)]  Loss:  1.960518 (2.1961)  Time: 0.750s,  170.76/s  (0.750s,  170.75/s)  LR: 9.950e-03  Data: 0.003 (0.003)
Train: 9 [ 389/390 (100%)]  Loss:  1.828353 (2.1553)  Time: 0.747s,  171.44/s  (0.749s,  170.81/s)  LR: 9.950e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.291 (1.291)  DataTime: 1.010 (1.010)  Loss:  1.6094 (1.6094)  Acc@1: 57.8125 (57.8125)  Acc@5: 87.5000 (87.5000)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.020)  Loss:  1.6523 (1.6169)  Acc@1: 57.8125 (55.7598)  Acc@5: 87.5000 (85.6311)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.013)  Loss:  2.1348 (1.6160)  Acc@1: 43.7500 (55.8600)  Acc@5: 68.7500 (85.4200)
tb
loss  0 : 2.25461875 9
loss  1 : 1.786515625 9
loss  2 : 2.142328125 9
loss  3 : 1.937271875 9
{'loss_different_0': 2.25461875, 'loss_different_1': 1.786515625, 'loss_different_2': 2.142328125, 'loss_different_3': 1.937271875}
Top1:  55.86
Train: 10 [   0/390 (  0%)]  Loss:  1.821192 (1.8212)  Time: 1.329s,   96.28/s  (1.329s,   96.28/s)  LR: 9.938e-03  Data: 0.500 (0.500)
Train: 10 [  50/390 ( 13%)]  Loss:  1.979554 (1.9004)  Time: 0.747s,  171.40/s  (0.759s,  168.73/s)  LR: 9.938e-03  Data: 0.000 (0.010)
Train: 10 [ 100/390 ( 26%)]  Loss:  2.041627 (1.9475)  Time: 0.748s,  171.23/s  (0.753s,  169.99/s)  LR: 9.938e-03  Data: 0.000 (0.005)
Train: 10 [ 150/390 ( 39%)]  Loss:  2.018217 (1.9651)  Time: 0.747s,  171.24/s  (0.751s,  170.45/s)  LR: 9.938e-03  Data: 0.000 (0.004)
Train: 10 [ 200/390 ( 51%)]  Loss:  1.883429 (1.9488)  Time: 0.748s,  171.21/s  (0.750s,  170.66/s)  LR: 9.938e-03  Data: 0.000 (0.003)
Train: 10 [ 250/390 ( 64%)]  Loss:  1.896318 (1.9401)  Time: 0.747s,  171.28/s  (0.750s,  170.61/s)  LR: 9.938e-03  Data: 0.000 (0.002)
Train: 10 [ 300/390 ( 77%)]  Loss:  1.761936 (1.9146)  Time: 0.747s,  171.44/s  (0.750s,  170.73/s)  LR: 9.938e-03  Data: 0.000 (0.002)
Train: 10 [ 350/390 ( 90%)]  Loss:  2.287914 (1.9613)  Time: 0.749s,  170.90/s  (0.749s,  170.81/s)  LR: 9.938e-03  Data: 0.003 (0.002)
Train: 10 [ 389/390 (100%)]  Loss:  2.034492 (1.9694)  Time: 0.747s,  171.45/s  (0.749s,  170.87/s)  LR: 9.938e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.946 (0.946)  DataTime: 0.676 (0.676)  Loss:  1.5059 (1.5059)  Acc@1: 56.2500 (56.2500)  Acc@5: 88.2812 (88.2812)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.014)  Loss:  1.6279 (1.5677)  Acc@1: 60.9375 (56.6330)  Acc@5: 85.9375 (85.8762)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  1.5635 (1.5724)  Acc@1: 50.0000 (56.7400)  Acc@5: 87.5000 (85.9100)
tb
loss  0 : 2.304325 10
loss  1 : 1.727896875 10
loss  2 : 2.144496875 10
loss  3 : 1.884403125 10
{'loss_different_0': 2.304325, 'loss_different_1': 1.727896875, 'loss_different_2': 2.144496875, 'loss_different_3': 1.884403125}
Top1:  56.74
Saving model to ./output/train/20250829-092114-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_3/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_3_10.pt
Train: 11 [   0/390 (  0%)]  Loss:  1.922867 (1.9229)  Time: 1.566s,   81.76/s  (1.566s,   81.76/s)  LR: 9.926e-03  Data: 0.818 (0.818)
Train: 11 [  50/390 ( 13%)]  Loss:  1.815913 (1.8694)  Time: 0.747s,  171.37/s  (0.763s,  167.69/s)  LR: 9.926e-03  Data: 0.000 (0.016)
Train: 11 [ 100/390 ( 26%)]  Loss:  2.371598 (2.0368)  Time: 0.747s,  171.34/s  (0.755s,  169.47/s)  LR: 9.926e-03  Data: 0.000 (0.008)
Train: 11 [ 150/390 ( 39%)]  Loss:  1.926586 (2.0092)  Time: 0.747s,  171.35/s  (0.753s,  170.07/s)  LR: 9.926e-03  Data: 0.000 (0.006)
Train: 11 [ 200/390 ( 51%)]  Loss:  1.876344 (1.9827)  Time: 0.747s,  171.35/s  (0.751s,  170.37/s)  LR: 9.926e-03  Data: 0.000 (0.004)
Train: 11 [ 250/390 ( 64%)]  Loss:  2.499911 (2.0689)  Time: 0.748s,  171.19/s  (0.750s,  170.56/s)  LR: 9.926e-03  Data: 0.000 (0.004)
Train: 11 [ 300/390 ( 77%)]  Loss:  3.137069 (2.2215)  Time: 0.747s,  171.39/s  (0.750s,  170.68/s)  LR: 9.926e-03  Data: 0.000 (0.003)
Train: 11 [ 350/390 ( 90%)]  Loss:  2.025679 (2.1970)  Time: 0.749s,  170.90/s  (0.750s,  170.77/s)  LR: 9.926e-03  Data: 0.003 (0.003)
Train: 11 [ 389/390 (100%)]  Loss:  1.969383 (2.1717)  Time: 0.747s,  171.32/s  (0.749s,  170.83/s)  LR: 9.926e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.175 (1.175)  DataTime: 0.895 (0.895)  Loss:  1.4443 (1.4443)  Acc@1: 58.5938 (58.5938)  Acc@5: 89.0625 (89.0625)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.018)  Loss:  1.6445 (1.5672)  Acc@1: 55.4688 (57.9504)  Acc@5: 80.4688 (86.0447)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.5283 (1.5731)  Acc@1: 56.2500 (57.7800)  Acc@5: 87.5000 (85.9700)
tb
loss  0 : 2.2457890625 11
loss  1 : 1.7381484375 11
loss  2 : 2.1373796875 11
loss  3 : 1.90376875 11
{'loss_different_0': 2.2457890625, 'loss_different_1': 1.7381484375, 'loss_different_2': 2.1373796875, 'loss_different_3': 1.90376875}
Top1:  57.78
Train: 12 [   0/390 (  0%)]  Loss:  1.843034 (1.8430)  Time: 1.525s,   83.96/s  (1.525s,   83.96/s)  LR: 9.911e-03  Data: 0.762 (0.762)
Train: 12 [  50/390 ( 13%)]  Loss:  1.815761 (1.8294)  Time: 0.747s,  171.29/s  (0.768s,  166.56/s)  LR: 9.911e-03  Data: 0.000 (0.015)
Train: 12 [ 100/390 ( 26%)]  Loss:  3.189432 (2.2827)  Time: 0.748s,  171.18/s  (0.758s,  168.85/s)  LR: 9.911e-03  Data: 0.000 (0.008)
Train: 12 [ 150/390 ( 39%)]  Loss:  1.804185 (2.1631)  Time: 0.748s,  171.19/s  (0.755s,  169.63/s)  LR: 9.911e-03  Data: 0.000 (0.005)
Train: 12 [ 200/390 ( 51%)]  Loss:  1.833066 (2.0971)  Time: 0.747s,  171.32/s  (0.753s,  170.04/s)  LR: 9.911e-03  Data: 0.000 (0.004)
Train: 12 [ 250/390 ( 64%)]  Loss:  1.859607 (2.0575)  Time: 0.748s,  171.10/s  (0.752s,  170.28/s)  LR: 9.911e-03  Data: 0.000 (0.003)
Train: 12 [ 300/390 ( 77%)]  Loss:  2.052235 (2.0568)  Time: 0.747s,  171.31/s  (0.751s,  170.44/s)  LR: 9.911e-03  Data: 0.000 (0.003)
Train: 12 [ 350/390 ( 90%)]  Loss:  2.017354 (2.0518)  Time: 0.750s,  170.77/s  (0.751s,  170.55/s)  LR: 9.911e-03  Data: 0.003 (0.003)
Train: 12 [ 389/390 (100%)]  Loss:  1.754255 (2.0188)  Time: 0.747s,  171.37/s  (0.750s,  170.63/s)  LR: 9.911e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.025 (1.025)  DataTime: 0.727 (0.727)  Loss:  1.4424 (1.4424)  Acc@1: 60.9375 (60.9375)  Acc@5: 88.2812 (88.2812)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.015)  Loss:  1.6455 (1.5104)  Acc@1: 53.9062 (58.2567)  Acc@5: 83.5938 (86.4583)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.010)  Loss:  1.4805 (1.5164)  Acc@1: 56.2500 (58.1600)  Acc@5: 93.7500 (86.6000)
tb
loss  0 : 2.16238125 12
loss  1 : 1.734109375 12
loss  2 : 2.064140625 12
loss  3 : 1.868915625 12
{'loss_different_0': 2.16238125, 'loss_different_1': 1.734109375, 'loss_different_2': 2.064140625, 'loss_different_3': 1.868915625}
Top1:  58.16
Train: 13 [   0/390 (  0%)]  Loss:  1.751789 (1.7518)  Time: 1.625s,   78.76/s  (1.625s,   78.76/s)  LR: 9.896e-03  Data: 0.876 (0.876)
Train: 13 [  50/390 ( 13%)]  Loss:  1.838981 (1.7954)  Time: 0.747s,  171.30/s  (0.765s,  167.41/s)  LR: 9.896e-03  Data: 0.000 (0.018)
Train: 13 [ 100/390 ( 26%)]  Loss:  1.779160 (1.7900)  Time: 0.748s,  171.14/s  (0.756s,  169.30/s)  LR: 9.896e-03  Data: 0.000 (0.009)
Train: 13 [ 150/390 ( 39%)]  Loss:  1.881783 (1.8129)  Time: 0.748s,  171.13/s  (0.753s,  169.94/s)  LR: 9.896e-03  Data: 0.000 (0.006)
Train: 13 [ 200/390 ( 51%)]  Loss:  2.122062 (1.8748)  Time: 0.747s,  171.31/s  (0.752s,  170.26/s)  LR: 9.896e-03  Data: 0.000 (0.005)
Train: 13 [ 250/390 ( 64%)]  Loss:  2.261318 (1.9392)  Time: 0.748s,  171.21/s  (0.751s,  170.46/s)  LR: 9.896e-03  Data: 0.000 (0.004)
Train: 13 [ 300/390 ( 77%)]  Loss:  1.622669 (1.8940)  Time: 0.747s,  171.39/s  (0.750s,  170.59/s)  LR: 9.896e-03  Data: 0.000 (0.003)
Train: 13 [ 350/390 ( 90%)]  Loss:  1.840379 (1.8873)  Time: 0.749s,  170.87/s  (0.750s,  170.56/s)  LR: 9.896e-03  Data: 0.003 (0.003)
Train: 13 [ 389/390 (100%)]  Loss:  1.607087 (1.8561)  Time: 0.746s,  171.48/s  (0.750s,  170.64/s)  LR: 9.896e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.317 (1.317)  DataTime: 1.030 (1.030)  Loss:  1.2861 (1.2861)  Acc@1: 64.0625 (64.0625)  Acc@5: 87.5000 (87.5000)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.020)  Loss:  1.6201 (1.4290)  Acc@1: 56.2500 (60.0031)  Acc@5: 85.1562 (88.1281)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.013)  Loss:  1.4561 (1.4395)  Acc@1: 62.5000 (59.6900)  Acc@5: 87.5000 (87.8000)
tb
loss  0 : 2.0526109375 13
loss  1 : 1.68231875 13
loss  2 : 1.9738578125 13
loss  3 : 1.81543125 13
{'loss_different_0': 2.0526109375, 'loss_different_1': 1.68231875, 'loss_different_2': 1.9738578125, 'loss_different_3': 1.81543125}
Top1:  59.69
Train: 14 [   0/390 (  0%)]  Loss:  1.601746 (1.6017)  Time: 1.237s,  103.47/s  (1.237s,  103.47/s)  LR: 9.880e-03  Data: 0.382 (0.382)
Train: 14 [  50/390 ( 13%)]  Loss:  1.956362 (1.7791)  Time: 0.747s,  171.27/s  (0.757s,  169.12/s)  LR: 9.880e-03  Data: 0.000 (0.008)
Train: 14 [ 100/390 ( 26%)]  Loss:  1.693760 (1.7506)  Time: 0.748s,  171.02/s  (0.752s,  170.20/s)  LR: 9.880e-03  Data: 0.000 (0.004)
Train: 14 [ 150/390 ( 39%)]  Loss:  1.701854 (1.7384)  Time: 0.748s,  171.21/s  (0.750s,  170.56/s)  LR: 9.880e-03  Data: 0.001 (0.003)
Train: 14 [ 200/390 ( 51%)]  Loss:  1.718194 (1.7344)  Time: 0.747s,  171.45/s  (0.750s,  170.75/s)  LR: 9.880e-03  Data: 0.000 (0.002)
Train: 14 [ 250/390 ( 64%)]  Loss:  2.208802 (1.8135)  Time: 0.747s,  171.32/s  (0.749s,  170.86/s)  LR: 9.880e-03  Data: 0.000 (0.002)
Train: 14 [ 300/390 ( 77%)]  Loss:  3.822917 (2.1005)  Time: 0.747s,  171.35/s  (0.749s,  170.94/s)  LR: 9.880e-03  Data: 0.000 (0.002)
Train: 14 [ 350/390 ( 90%)]  Loss:  1.615458 (2.0399)  Time: 0.750s,  170.64/s  (0.749s,  170.99/s)  LR: 9.880e-03  Data: 0.003 (0.001)
Train: 14 [ 389/390 (100%)]  Loss:  1.789400 (2.0121)  Time: 0.747s,  171.35/s  (0.748s,  171.02/s)  LR: 9.880e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.050 (1.050)  DataTime: 0.773 (0.773)  Loss:  1.3125 (1.3125)  Acc@1: 63.2812 (63.2812)  Acc@5: 86.7188 (86.7188)
Test: [  50/78]  Time: 0.259 (0.275)  DataTime: 0.000 (0.016)  Loss:  1.6719 (1.4318)  Acc@1: 55.4688 (59.7886)  Acc@5: 82.8125 (88.1740)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.010)  Loss:  1.2646 (1.4310)  Acc@1: 68.7500 (59.8800)  Acc@5: 87.5000 (88.1900)
tb
loss  0 : 1.991228125 14
loss  1 : 1.669675 14
loss  2 : 1.984803125 14
loss  3 : 1.8120296875 14
{'loss_different_0': 1.991228125, 'loss_different_1': 1.669675, 'loss_different_2': 1.984803125, 'loss_different_3': 1.8120296875}
Top1:  59.88
Train: 15 [   0/390 (  0%)]  Loss:  1.628764 (1.6288)  Time: 1.258s,  101.74/s  (1.258s,  101.74/s)  LR: 9.862e-03  Data: 0.437 (0.437)
Train: 15 [  50/390 ( 13%)]  Loss:  1.859405 (1.7441)  Time: 0.747s,  171.33/s  (0.757s,  169.04/s)  LR: 9.862e-03  Data: 0.000 (0.009)
Train: 15 [ 100/390 ( 26%)]  Loss:  1.654318 (1.7142)  Time: 0.747s,  171.38/s  (0.755s,  169.46/s)  LR: 9.862e-03  Data: 0.000 (0.005)
Train: 15 [ 150/390 ( 39%)]  Loss:  1.669337 (1.7030)  Time: 0.748s,  171.21/s  (0.753s,  170.07/s)  LR: 9.862e-03  Data: 0.000 (0.003)
Train: 15 [ 200/390 ( 51%)]  Loss:  1.734424 (1.7092)  Time: 0.747s,  171.39/s  (0.751s,  170.38/s)  LR: 9.862e-03  Data: 0.000 (0.003)
Train: 15 [ 250/390 ( 64%)]  Loss:  1.802911 (1.7249)  Time: 0.747s,  171.34/s  (0.750s,  170.56/s)  LR: 9.862e-03  Data: 0.000 (0.002)
Train: 15 [ 300/390 ( 77%)]  Loss:  3.461280 (1.9729)  Time: 0.747s,  171.35/s  (0.750s,  170.68/s)  LR: 9.862e-03  Data: 0.000 (0.002)
Train: 15 [ 350/390 ( 90%)]  Loss:  1.999921 (1.9763)  Time: 0.750s,  170.68/s  (0.750s,  170.77/s)  LR: 9.862e-03  Data: 0.003 (0.002)
Train: 15 [ 389/390 (100%)]  Loss:  1.732411 (1.9492)  Time: 0.747s,  171.43/s  (0.749s,  170.83/s)  LR: 9.862e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.229 (1.229)  DataTime: 0.906 (0.906)  Loss:  1.3232 (1.3232)  Acc@1: 64.0625 (64.0625)  Acc@5: 85.1562 (85.1562)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.018)  Loss:  1.5322 (1.4084)  Acc@1: 57.0312 (61.7341)  Acc@5: 85.1562 (87.6838)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.2344 (1.4113)  Acc@1: 62.5000 (61.6500)  Acc@5: 87.5000 (87.7000)
tb
loss  0 : 2.04915625 15
loss  1 : 1.6516796875 15
loss  2 : 1.9896578125 15
loss  3 : 1.7861125 15
{'loss_different_0': 2.04915625, 'loss_different_1': 1.6516796875, 'loss_different_2': 1.9896578125, 'loss_different_3': 1.7861125}
Top1:  61.65
Train: 16 [   0/390 (  0%)]  Loss:  1.983640 (1.9836)  Time: 1.653s,   77.43/s  (1.653s,   77.43/s)  LR: 9.843e-03  Data: 0.905 (0.905)
Train: 16 [  50/390 ( 13%)]  Loss:  1.556816 (1.7702)  Time: 0.747s,  171.46/s  (0.765s,  167.30/s)  LR: 9.843e-03  Data: 0.000 (0.018)
Train: 16 [ 100/390 ( 26%)]  Loss:  1.455545 (1.6653)  Time: 0.748s,  171.15/s  (0.756s,  169.25/s)  LR: 9.843e-03  Data: 0.000 (0.009)
Train: 16 [ 150/390 ( 39%)]  Loss:  1.608203 (1.6511)  Time: 0.747s,  171.32/s  (0.753s,  169.93/s)  LR: 9.843e-03  Data: 0.000 (0.006)
Train: 16 [ 200/390 ( 51%)]  Loss:  1.689675 (1.6588)  Time: 0.748s,  171.15/s  (0.752s,  170.27/s)  LR: 9.843e-03  Data: 0.000 (0.005)
Train: 16 [ 250/390 ( 64%)]  Loss:  1.517675 (1.6353)  Time: 0.748s,  171.22/s  (0.751s,  170.47/s)  LR: 9.843e-03  Data: 0.000 (0.004)
Train: 16 [ 300/390 ( 77%)]  Loss:  1.884502 (1.6709)  Time: 0.746s,  171.54/s  (0.750s,  170.61/s)  LR: 9.843e-03  Data: 0.000 (0.003)
Train: 16 [ 350/390 ( 90%)]  Loss:  1.576969 (1.6591)  Time: 0.750s,  170.76/s  (0.750s,  170.71/s)  LR: 9.843e-03  Data: 0.003 (0.003)
Train: 16 [ 389/390 (100%)]  Loss:  1.556907 (1.6478)  Time: 0.747s,  171.33/s  (0.750s,  170.66/s)  LR: 9.843e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.034 (1.034)  DataTime: 0.757 (0.757)  Loss:  1.2344 (1.2344)  Acc@1: 66.4062 (66.4062)  Acc@5: 89.8438 (89.8438)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.017)  Loss:  1.5361 (1.3669)  Acc@1: 60.1562 (62.8676)  Acc@5: 85.9375 (88.8940)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.2490 (1.3672)  Acc@1: 68.7500 (62.7900)  Acc@5: 81.2500 (88.8600)
tb
loss  0 : 1.9979140625 16
loss  1 : 1.591684375 16
loss  2 : 1.83483125 16
loss  3 : 1.70389375 16
{'loss_different_0': 1.9979140625, 'loss_different_1': 1.591684375, 'loss_different_2': 1.83483125, 'loss_different_3': 1.70389375}
Top1:  62.79
Train: 17 [   0/390 (  0%)]  Loss:  1.439413 (1.4394)  Time: 1.433s,   89.30/s  (1.433s,   89.30/s)  LR: 9.823e-03  Data: 0.636 (0.636)
Train: 17 [  50/390 ( 13%)]  Loss:  1.714355 (1.5769)  Time: 0.747s,  171.29/s  (0.761s,  168.24/s)  LR: 9.823e-03  Data: 0.000 (0.013)
Train: 17 [ 100/390 ( 26%)]  Loss:  1.774905 (1.6429)  Time: 0.747s,  171.36/s  (0.754s,  169.72/s)  LR: 9.823e-03  Data: 0.000 (0.007)
Train: 17 [ 150/390 ( 39%)]  Loss:  1.649339 (1.6445)  Time: 0.748s,  171.12/s  (0.752s,  170.23/s)  LR: 9.823e-03  Data: 0.000 (0.005)
Train: 17 [ 200/390 ( 51%)]  Loss:  1.567497 (1.6291)  Time: 0.748s,  171.11/s  (0.751s,  170.48/s)  LR: 9.823e-03  Data: 0.000 (0.004)
Train: 17 [ 250/390 ( 64%)]  Loss:  1.535901 (1.6136)  Time: 0.747s,  171.36/s  (0.750s,  170.63/s)  LR: 9.823e-03  Data: 0.000 (0.003)
Train: 17 [ 300/390 ( 77%)]  Loss:  1.393806 (1.5822)  Time: 0.748s,  171.21/s  (0.750s,  170.73/s)  LR: 9.823e-03  Data: 0.000 (0.002)
Train: 17 [ 350/390 ( 90%)]  Loss:  1.735733 (1.6014)  Time: 0.749s,  170.82/s  (0.749s,  170.80/s)  LR: 9.823e-03  Data: 0.003 (0.002)
Train: 17 [ 389/390 (100%)]  Loss:  1.728317 (1.6155)  Time: 0.747s,  171.29/s  (0.749s,  170.85/s)  LR: 9.823e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.118 (1.118)  DataTime: 0.823 (0.823)  Loss:  1.2529 (1.2529)  Acc@1: 64.8438 (64.8438)  Acc@5: 89.8438 (89.8438)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.017)  Loss:  1.4492 (1.3337)  Acc@1: 60.1562 (63.2200)  Acc@5: 85.9375 (89.0931)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.1328 (1.3278)  Acc@1: 68.7500 (63.4500)  Acc@5: 93.7500 (89.2200)
tb
loss  0 : 1.9807671875 17
loss  1 : 1.5577859375 17
loss  2 : 1.8358609375 17
loss  3 : 1.664053125 17
{'loss_different_0': 1.9807671875, 'loss_different_1': 1.5577859375, 'loss_different_2': 1.8358609375, 'loss_different_3': 1.664053125}
Top1:  63.45
Train: 18 [   0/390 (  0%)]  Loss:  1.508120 (1.5081)  Time: 1.669s,   76.70/s  (1.669s,   76.70/s)  LR: 9.801e-03  Data: 0.919 (0.919)
Train: 18 [  50/390 ( 13%)]  Loss:  1.711342 (1.6097)  Time: 0.748s,  171.22/s  (0.765s,  167.23/s)  LR: 9.801e-03  Data: 0.000 (0.018)
Train: 18 [ 100/390 ( 26%)]  Loss:  3.514473 (2.2446)  Time: 0.747s,  171.37/s  (0.756s,  169.20/s)  LR: 9.801e-03  Data: 0.000 (0.009)
Train: 18 [ 150/390 ( 39%)]  Loss:  1.448808 (2.0457)  Time: 0.747s,  171.26/s  (0.754s,  169.87/s)  LR: 9.801e-03  Data: 0.000 (0.006)
Train: 18 [ 200/390 ( 51%)]  Loss:  2.621138 (2.1608)  Time: 0.748s,  171.17/s  (0.752s,  170.22/s)  LR: 9.801e-03  Data: 0.000 (0.005)
Train: 18 [ 250/390 ( 64%)]  Loss:  1.611366 (2.0692)  Time: 0.748s,  171.07/s  (0.751s,  170.41/s)  LR: 9.801e-03  Data: 0.000 (0.004)
Train: 18 [ 300/390 ( 77%)]  Loss:  2.829280 (2.1778)  Time: 0.748s,  171.18/s  (0.751s,  170.38/s)  LR: 9.801e-03  Data: 0.000 (0.003)
Train: 18 [ 350/390 ( 90%)]  Loss:  1.571152 (2.1020)  Time: 0.750s,  170.55/s  (0.751s,  170.50/s)  LR: 9.801e-03  Data: 0.003 (0.003)
Train: 18 [ 389/390 (100%)]  Loss:  1.420869 (2.0263)  Time: 0.747s,  171.35/s  (0.750s,  170.59/s)  LR: 9.801e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.352 (1.352)  DataTime: 1.078 (1.078)  Loss:  1.1855 (1.1855)  Acc@1: 68.7500 (68.7500)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.021)  Loss:  1.4346 (1.3112)  Acc@1: 64.0625 (64.0012)  Acc@5: 89.0625 (89.7518)
Test: [  78/78]  Time: 0.034 (0.270)  DataTime: 0.000 (0.014)  Loss:  0.9844 (1.3081)  Acc@1: 81.2500 (64.2600)  Acc@5: 93.7500 (89.5400)
tb
loss  0 : 1.962815625 18
loss  1 : 1.5546375 18
loss  2 : 1.8666671875 18
loss  3 : 1.660684375 18
{'loss_different_0': 1.962815625, 'loss_different_1': 1.5546375, 'loss_different_2': 1.8666671875, 'loss_different_3': 1.660684375}
Top1:  64.26
Train: 19 [   0/390 (  0%)]  Loss:  1.512240 (1.5122)  Time: 1.367s,   93.63/s  (1.367s,   93.63/s)  LR: 9.779e-03  Data: 0.564 (0.564)
Train: 19 [  50/390 ( 13%)]  Loss:  1.443260 (1.4778)  Time: 0.747s,  171.24/s  (0.759s,  168.56/s)  LR: 9.779e-03  Data: 0.000 (0.011)
Train: 19 [ 100/390 ( 26%)]  Loss:  1.579000 (1.5115)  Time: 0.747s,  171.35/s  (0.753s,  169.89/s)  LR: 9.779e-03  Data: 0.000 (0.006)
Train: 19 [ 150/390 ( 39%)]  Loss:  3.968980 (2.1259)  Time: 0.748s,  171.15/s  (0.751s,  170.34/s)  LR: 9.779e-03  Data: 0.000 (0.004)
Train: 19 [ 200/390 ( 51%)]  Loss:  4.037701 (2.5082)  Time: 0.748s,  171.19/s  (0.750s,  170.58/s)  LR: 9.779e-03  Data: 0.000 (0.003)
Train: 19 [ 250/390 ( 64%)]  Loss:  1.453768 (2.3325)  Time: 0.748s,  171.21/s  (0.750s,  170.72/s)  LR: 9.779e-03  Data: 0.000 (0.003)
Train: 19 [ 300/390 ( 77%)]  Loss:  3.476593 (2.4959)  Time: 0.747s,  171.40/s  (0.749s,  170.82/s)  LR: 9.779e-03  Data: 0.000 (0.002)
Train: 19 [ 350/390 ( 90%)]  Loss:  1.489804 (2.3702)  Time: 0.750s,  170.72/s  (0.749s,  170.89/s)  LR: 9.779e-03  Data: 0.003 (0.002)
Train: 19 [ 389/390 (100%)]  Loss:  1.852371 (2.3126)  Time: 0.747s,  171.37/s  (0.749s,  170.93/s)  LR: 9.779e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.234 (1.234)  DataTime: 0.962 (0.962)  Loss:  1.3447 (1.3447)  Acc@1: 64.0625 (64.0625)  Acc@5: 92.1875 (92.1875)
Test: [  50/78]  Time: 0.259 (0.278)  DataTime: 0.000 (0.019)  Loss:  1.4473 (1.3428)  Acc@1: 62.5000 (63.1127)  Acc@5: 87.5000 (89.0931)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.3213 (1.3387)  Acc@1: 62.5000 (63.5800)  Acc@5: 81.2500 (88.9500)
tb
loss  0 : 2.014659375 19
loss  1 : 1.6045890625 19
loss  2 : 1.8541234375 19
loss  3 : 1.6988265625 19
{'loss_different_0': 2.014659375, 'loss_different_1': 1.6045890625, 'loss_different_2': 1.8541234375, 'loss_different_3': 1.6988265625}
Top1:  63.58
Train: 20 [   0/390 (  0%)]  Loss:  1.595536 (1.5955)  Time: 1.618s,   79.10/s  (1.618s,   79.10/s)  LR: 9.755e-03  Data: 0.866 (0.866)
Train: 20 [ 200/390 ( 51%)]  Loss:  1.636920 (1.5561)  Time: 0.747s,  171.43/s  (0.753s,  169.98/s)  LR: 9.755e-03  Data: 0.000 (0.005)
Train: 20 [ 250/390 ( 64%)]  Loss:  1.662468 (1.5738)  Time: 0.747s,  171.25/s  (0.752s,  170.24/s)  LR: 9.755e-03  Data: 0.000 (0.004)
Train: 20 [ 300/390 ( 77%)]  Loss:  3.811627 (1.8935)  Time: 0.747s,  171.33/s  (0.751s,  170.41/s)  LR: 9.755e-03  Data: 0.000 (0.003)
Train: 20 [ 350/390 ( 90%)]  Loss:  1.481800 (1.8420)  Time: 0.749s,  170.81/s  (0.751s,  170.54/s)  LR: 9.755e-03  Data: 0.003 (0.003)
Train: 20 [ 389/390 (100%)]  Loss:  1.429781 (1.7962)  Time: 0.746s,  171.48/s  (0.750s,  170.62/s)  LR: 9.755e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.358 (1.358)  DataTime: 1.063 (1.063)  Loss:  1.1465 (1.1465)  Acc@1: 68.7500 (68.7500)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.281)  DataTime: 0.000 (0.021)  Loss:  1.4131 (1.2906)  Acc@1: 61.7188 (64.9969)  Acc@5: 87.5000 (90.0735)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.014)  Loss:  1.2832 (1.2892)  Acc@1: 56.2500 (65.0700)  Acc@5: 87.5000 (89.9700)
tb
loss  0 : 1.9366671875 20
loss  1 : 1.5378125 20
loss  2 : 1.7771 20
loss  3 : 1.67505 20
{'loss_different_0': 1.9366671875, 'loss_different_1': 1.5378125, 'loss_different_2': 1.7771, 'loss_different_3': 1.67505}
Top1:  65.07
Saving model to ./output/train/20250829-092114-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_3/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_3_20.pt
Train: 21 [   0/390 (  0%)]  Loss:  1.577318 (1.5773)  Time: 1.266s,  101.09/s  (1.266s,  101.09/s)  LR: 9.730e-03  Data: 0.484 (0.484)
Train: 21 [  50/390 ( 13%)]  Loss:  1.304779 (1.4410)  Time: 0.747s,  171.32/s  (0.757s,  168.98/s)  LR: 9.730e-03  Data: 0.000 (0.010)
Train: 21 [ 100/390 ( 26%)]  Loss:  1.353836 (1.4120)  Time: 0.746s,  171.52/s  (0.752s,  170.10/s)  LR: 9.730e-03  Data: 0.000 (0.005)
Train: 21 [ 150/390 ( 39%)]  Loss:  1.404136 (1.4100)  Time: 0.747s,  171.25/s  (0.751s,  170.48/s)  LR: 9.730e-03  Data: 0.000 (0.004)
Train: 21 [ 200/390 ( 51%)]  Loss:  1.573905 (1.4428)  Time: 0.747s,  171.29/s  (0.750s,  170.67/s)  LR: 9.730e-03  Data: 0.000 (0.003)
Train: 21 [ 250/390 ( 64%)]  Loss:  1.618104 (1.4720)  Time: 0.748s,  171.17/s  (0.749s,  170.79/s)  LR: 9.730e-03  Data: 0.000 (0.002)
Train: 21 [ 300/390 ( 77%)]  Loss:  3.451710 (1.7548)  Time: 0.748s,  171.13/s  (0.749s,  170.87/s)  LR: 9.730e-03  Data: 0.000 (0.002)
Train: 21 [ 350/390 ( 90%)]  Loss:  1.455729 (1.7174)  Time: 0.750s,  170.63/s  (0.749s,  170.79/s)  LR: 9.730e-03  Data: 0.003 (0.002)
Train: 21 [ 389/390 (100%)]  Loss:  3.210497 (1.8833)  Time: 0.747s,  171.38/s  (0.749s,  170.84/s)  LR: 9.730e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.289 (1.289)  DataTime: 1.009 (1.009)  Loss:  1.1758 (1.1758)  Acc@1: 64.8438 (64.8438)  Acc@5: 92.1875 (92.1875)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.020)  Loss:  1.3730 (1.3046)  Acc@1: 60.1562 (63.9246)  Acc@5: 92.1875 (89.4455)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.013)  Loss:  1.0889 (1.3035)  Acc@1: 68.7500 (63.9700)  Acc@5: 93.7500 (89.4700)
tb
loss  0 : 1.9980859375 21
loss  1 : 1.55419375 21
loss  2 : 1.8798734375 21
loss  3 : 1.6481609375 21
{'loss_different_0': 1.9980859375, 'loss_different_1': 1.55419375, 'loss_different_2': 1.8798734375, 'loss_different_3': 1.6481609375}
Top1:  63.97
Train: 22 [   0/390 (  0%)]  Loss:  1.549213 (1.5492)  Time: 1.620s,   79.01/s  (1.620s,   79.01/s)  LR: 9.704e-03  Data: 0.872 (0.872)
Train: 22 [  50/390 ( 13%)]  Loss:  1.570888 (1.5601)  Time: 0.748s,  171.20/s  (0.765s,  167.39/s)  LR: 9.704e-03  Data: 0.000 (0.017)
Train: 22 [ 100/390 ( 26%)]  Loss:  1.432159 (1.5174)  Time: 0.747s,  171.26/s  (0.756s,  169.26/s)  LR: 9.704e-03  Data: 0.000 (0.009)
Train: 22 [ 150/390 ( 39%)]  Loss:  1.584135 (1.5341)  Time: 0.748s,  171.23/s  (0.753s,  169.90/s)  LR: 9.704e-03  Data: 0.001 (0.006)
Train: 22 [ 200/390 ( 51%)]  Loss:  1.437372 (1.5148)  Time: 0.748s,  171.23/s  (0.752s,  170.22/s)  LR: 9.704e-03  Data: 0.001 (0.005)
Train: 22 [ 250/390 ( 64%)]  Loss:  1.388542 (1.4937)  Time: 0.748s,  171.19/s  (0.751s,  170.42/s)  LR: 9.704e-03  Data: 0.000 (0.004)
Train: 22 [ 300/390 ( 77%)]  Loss:  1.476057 (1.4912)  Time: 0.747s,  171.30/s  (0.751s,  170.55/s)  LR: 9.704e-03  Data: 0.000 (0.003)
Train: 22 [ 350/390 ( 90%)]  Loss:  2.619540 (1.6322)  Time: 0.750s,  170.56/s  (0.750s,  170.64/s)  LR: 9.704e-03  Data: 0.003 (0.003)
Train: 22 [ 389/390 (100%)]  Loss:  1.435035 (1.6103)  Time: 0.747s,  171.42/s  (0.750s,  170.71/s)  LR: 9.704e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.145 (1.145)  DataTime: 0.717 (0.717)  Loss:  1.1963 (1.1963)  Acc@1: 66.4062 (66.4062)  Acc@5: 89.8438 (89.8438)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.014)  Loss:  1.3877 (1.2420)  Acc@1: 63.2812 (65.6403)  Acc@5: 87.5000 (90.1501)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.009)  Loss:  1.0088 (1.2431)  Acc@1: 68.7500 (65.5100)  Acc@5: 87.5000 (90.1100)
tb
loss  0 : 1.8820578125 22
loss  1 : 1.5367046875 22
loss  2 : 1.78360390625 22
loss  3 : 1.6270890625 22
{'loss_different_0': 1.8820578125, 'loss_different_1': 1.5367046875, 'loss_different_2': 1.78360390625, 'loss_different_3': 1.6270890625}
Top1:  65.51
Train: 23 [   0/390 (  0%)]  Loss:  1.192190 (1.1922)  Time: 1.493s,   85.75/s  (1.493s,   85.75/s)  LR: 9.677e-03  Data: 0.577 (0.577)
Train: 23 [  50/390 ( 13%)]  Loss:  2.349483 (1.7708)  Time: 0.747s,  171.27/s  (0.762s,  167.94/s)  LR: 9.677e-03  Data: 0.000 (0.012)
Train: 23 [ 100/390 ( 26%)]  Loss:  1.365958 (1.6359)  Time: 0.748s,  171.02/s  (0.755s,  169.56/s)  LR: 9.677e-03  Data: 0.000 (0.006)
Train: 23 [ 250/390 ( 64%)]  Loss:  1.448932 (1.5345)  Time: 0.747s,  171.38/s  (0.751s,  170.55/s)  LR: 9.677e-03  Data: 0.000 (0.003)
Train: 23 [ 300/390 ( 77%)]  Loss:  1.404756 (1.5160)  Time: 0.747s,  171.27/s  (0.751s,  170.49/s)  LR: 9.677e-03  Data: 0.000 (0.002)
Train: 23 [ 350/390 ( 90%)]  Loss:  1.597493 (1.5262)  Time: 0.750s,  170.70/s  (0.750s,  170.59/s)  LR: 9.677e-03  Data: 0.003 (0.002)
Train: 23 [ 389/390 (100%)]  Loss:  1.722442 (1.5480)  Time: 0.747s,  171.33/s  (0.750s,  170.66/s)  LR: 9.677e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.171 (1.171)  DataTime: 0.899 (0.899)  Loss:  1.1777 (1.1777)  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (87.5000)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.018)  Loss:  1.4053 (1.2603)  Acc@1: 62.5000 (65.6250)  Acc@5: 89.8438 (89.6752)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.3047 (1.2633)  Acc@1: 68.7500 (65.6700)  Acc@5: 87.5000 (89.7300)
tb
loss  0 : 1.95265625 23
loss  1 : 1.540546875 23
loss  2 : 1.7640625 23
loss  3 : 1.6507953125 23
{'loss_different_0': 1.95265625, 'loss_different_1': 1.540546875, 'loss_different_2': 1.7640625, 'loss_different_3': 1.6507953125}
Top1:  65.67
Train: 24 [   0/390 (  0%)]  Loss:  1.402801 (1.4028)  Time: 1.238s,  103.38/s  (1.238s,  103.38/s)  LR: 9.649e-03  Data: 0.357 (0.357)
Train: 24 [  50/390 ( 13%)]  Loss:  1.451771 (1.4273)  Time: 0.747s,  171.41/s  (0.757s,  169.13/s)  LR: 9.649e-03  Data: 0.000 (0.007)
Train: 24 [ 100/390 ( 26%)]  Loss:  1.311769 (1.3888)  Time: 0.747s,  171.32/s  (0.752s,  170.22/s)  LR: 9.649e-03  Data: 0.000 (0.004)
Train: 24 [ 150/390 ( 39%)]  Loss:  1.457782 (1.4060)  Time: 0.747s,  171.35/s  (0.750s,  170.58/s)  LR: 9.649e-03  Data: 0.000 (0.003)
Train: 24 [ 200/390 ( 51%)]  Loss:  1.381298 (1.4011)  Time: 0.747s,  171.33/s  (0.750s,  170.76/s)  LR: 9.649e-03  Data: 0.000 (0.002)
Train: 24 [ 250/390 ( 64%)]  Loss:  1.312964 (1.3864)  Time: 0.747s,  171.32/s  (0.749s,  170.88/s)  LR: 9.649e-03  Data: 0.001 (0.002)
Train: 24 [ 300/390 ( 77%)]  Loss:  1.488166 (1.4009)  Time: 0.747s,  171.33/s  (0.749s,  170.95/s)  LR: 9.649e-03  Data: 0.000 (0.002)
Train: 24 [ 350/390 ( 90%)]  Loss:  1.552872 (1.4199)  Time: 0.750s,  170.72/s  (0.749s,  171.00/s)  LR: 9.649e-03  Data: 0.003 (0.001)
Train: 24 [ 389/390 (100%)]  Loss:  1.521201 (1.4312)  Time: 0.747s,  171.43/s  (0.748s,  171.04/s)  LR: 9.649e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.192 (1.192)  DataTime: 0.927 (0.927)  Loss:  1.2539 (1.2539)  Acc@1: 64.8438 (64.8438)  Acc@5: 90.6250 (90.6250)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.019)  Loss:  1.3232 (1.2633)  Acc@1: 67.1875 (65.5331)  Acc@5: 88.2812 (89.7059)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  1.0537 (1.2746)  Acc@1: 81.2500 (65.1800)  Acc@5: 87.5000 (89.5000)
tb
loss  0 : 1.917828125 24
loss  1 : 1.574446875 24
loss  2 : 1.77091875 24
loss  3 : 1.701340625 24
{'loss_different_0': 1.917828125, 'loss_different_1': 1.574446875, 'loss_different_2': 1.77091875, 'loss_different_3': 1.701340625}
Top1:  65.18
Train: 25 [   0/390 (  0%)]  Loss:  1.430493 (1.4305)  Time: 1.513s,   84.59/s  (1.513s,   84.59/s)  LR: 9.619e-03  Data: 0.747 (0.747)
Train: 25 [  50/390 ( 13%)]  Loss:  1.634084 (1.5323)  Time: 0.748s,  171.10/s  (0.768s,  166.60/s)  LR: 9.619e-03  Data: 0.000 (0.015)
Train: 25 [ 100/390 ( 26%)]  Loss:  1.373500 (1.4794)  Time: 0.747s,  171.38/s  (0.758s,  168.88/s)  LR: 9.619e-03  Data: 0.000 (0.008)
Train: 25 [ 150/390 ( 39%)]  Loss:  1.350975 (1.4473)  Time: 0.747s,  171.31/s  (0.754s,  169.67/s)  LR: 9.619e-03  Data: 0.001 (0.005)
Train: 25 [ 200/390 ( 51%)]  Loss:  3.629908 (1.8838)  Time: 0.748s,  171.22/s  (0.753s,  170.07/s)  LR: 9.619e-03  Data: 0.001 (0.004)
Train: 25 [ 250/390 ( 64%)]  Loss:  1.434044 (1.8088)  Time: 0.748s,  171.18/s  (0.752s,  170.30/s)  LR: 9.619e-03  Data: 0.001 (0.003)
Train: 25 [ 300/390 ( 77%)]  Loss:  1.376591 (1.7471)  Time: 0.747s,  171.28/s  (0.751s,  170.47/s)  LR: 9.619e-03  Data: 0.000 (0.003)
Train: 25 [ 350/390 ( 90%)]  Loss:  1.696414 (1.7408)  Time: 0.749s,  170.87/s  (0.750s,  170.58/s)  LR: 9.619e-03  Data: 0.002 (0.002)
Train: 25 [ 389/390 (100%)]  Loss:  1.404385 (1.7034)  Time: 0.747s,  171.45/s  (0.750s,  170.66/s)  LR: 9.619e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.334 (1.334)  DataTime: 1.065 (1.065)  Loss:  1.1328 (1.1328)  Acc@1: 68.7500 (68.7500)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.021)  Loss:  1.3467 (1.2652)  Acc@1: 65.6250 (65.0123)  Acc@5: 88.2812 (90.1808)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.014)  Loss:  1.0605 (1.2669)  Acc@1: 68.7500 (64.9400)  Acc@5: 100.0000 (90.0200)
tb
loss  0 : 1.921596875 25
loss  1 : 1.59400625 25
loss  2 : 1.8030609375 25
loss  3 : 1.7017046875 25
{'loss_different_0': 1.921596875, 'loss_different_1': 1.59400625, 'loss_different_2': 1.8030609375, 'loss_different_3': 1.7017046875}
Top1:  64.94
Train: 26 [   0/390 (  0%)]  Loss:  1.451882 (1.4519)  Time: 1.242s,  103.04/s  (1.242s,  103.04/s)  LR: 9.589e-03  Data: 0.431 (0.431)
Train: 26 [  50/390 ( 13%)]  Loss:  1.312322 (1.3821)  Time: 0.747s,  171.43/s  (0.757s,  169.10/s)  LR: 9.589e-03  Data: 0.000 (0.009)
Train: 26 [ 100/390 ( 26%)]  Loss:  1.450290 (1.4048)  Time: 0.747s,  171.41/s  (0.752s,  170.17/s)  LR: 9.589e-03  Data: 0.000 (0.005)
Train: 26 [ 150/390 ( 39%)]  Loss:  1.214294 (1.3572)  Time: 0.746s,  171.59/s  (0.751s,  170.54/s)  LR: 9.589e-03  Data: 0.000 (0.003)
Train: 26 [ 200/390 ( 51%)]  Loss:  1.293547 (1.3445)  Time: 0.748s,  171.12/s  (0.750s,  170.74/s)  LR: 9.589e-03  Data: 0.000 (0.002)
Train: 26 [ 250/390 ( 64%)]  Loss:  2.667092 (1.5649)  Time: 0.748s,  171.18/s  (0.749s,  170.84/s)  LR: 9.589e-03  Data: 0.000 (0.002)
Train: 26 [ 300/390 ( 77%)]  Loss:  1.530748 (1.5600)  Time: 0.748s,  171.24/s  (0.749s,  170.92/s)  LR: 9.589e-03  Data: 0.000 (0.002)
Train: 26 [ 350/390 ( 90%)]  Loss:  2.030956 (1.6189)  Time: 0.750s,  170.75/s  (0.749s,  170.83/s)  LR: 9.589e-03  Data: 0.003 (0.002)
Train: 26 [ 389/390 (100%)]  Loss:  1.132111 (1.5648)  Time: 0.747s,  171.28/s  (0.749s,  170.88/s)  LR: 9.589e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 0.897 (0.897)  DataTime: 0.631 (0.631)  Loss:  1.0576 (1.0576)  Acc@1: 69.5312 (69.5312)  Acc@5: 92.9688 (92.9688)
Test: [  50/78]  Time: 0.259 (0.272)  DataTime: 0.000 (0.013)  Loss:  1.2295 (1.2091)  Acc@1: 64.0625 (67.4020)  Acc@5: 90.6250 (90.6710)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.008)  Loss:  0.9097 (1.2111)  Acc@1: 75.0000 (67.1800)  Acc@5: 93.7500 (90.5000)
tb
loss  0 : 1.85383125 26
loss  1 : 1.5131625 26
loss  2 : 1.709796875 26
loss  3 : 1.5939796875 26
{'loss_different_0': 1.85383125, 'loss_different_1': 1.5131625, 'loss_different_2': 1.709796875, 'loss_different_3': 1.5939796875}
Top1:  67.18
Train: 27 [   0/390 (  0%)]  Loss:  1.817102 (1.8171)  Time: 1.241s,  103.15/s  (1.241s,  103.15/s)  LR: 9.557e-03  Data: 0.389 (0.389)
Train: 27 [  50/390 ( 13%)]  Loss:  1.356852 (1.5870)  Time: 0.748s,  171.22/s  (0.757s,  169.02/s)  LR: 9.557e-03  Data: 0.000 (0.008)
Train: 27 [ 100/390 ( 26%)]  Loss:  1.299100 (1.4910)  Time: 0.747s,  171.27/s  (0.752s,  170.12/s)  LR: 9.557e-03  Data: 0.000 (0.004)
Train: 27 [ 150/390 ( 39%)]  Loss:  1.479323 (1.4881)  Time: 0.748s,  171.21/s  (0.751s,  170.48/s)  LR: 9.557e-03  Data: 0.001 (0.003)
Train: 27 [ 200/390 ( 51%)]  Loss:  1.483950 (1.4873)  Time: 0.748s,  171.15/s  (0.750s,  170.67/s)  LR: 9.557e-03  Data: 0.000 (0.002)
Train: 27 [ 250/390 ( 64%)]  Loss:  1.370673 (1.4678)  Time: 0.747s,  171.37/s  (0.750s,  170.78/s)  LR: 9.557e-03  Data: 0.000 (0.002)
Train: 27 [ 300/390 ( 77%)]  Loss:  1.355665 (1.4518)  Time: 0.748s,  171.22/s  (0.749s,  170.85/s)  LR: 9.557e-03  Data: 0.000 (0.002)
Train: 27 [ 350/390 ( 90%)]  Loss:  1.336994 (1.4375)  Time: 0.750s,  170.68/s  (0.749s,  170.91/s)  LR: 9.557e-03  Data: 0.003 (0.001)
Train: 27 [ 389/390 (100%)]  Loss:  1.535665 (1.4484)  Time: 0.747s,  171.32/s  (0.749s,  170.95/s)  LR: 9.557e-03  Data: 0.000 (0.001)
Test: [   0/78]  Time: 0.972 (0.972)  DataTime: 0.699 (0.699)  Loss:  1.0811 (1.0811)  Acc@1: 70.3125 (70.3125)  Acc@5: 92.9688 (92.9688)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.014)  Loss:  1.2891 (1.2094)  Acc@1: 66.4062 (66.5748)  Acc@5: 89.0625 (90.6556)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  1.3076 (1.2097)  Acc@1: 68.7500 (66.8700)  Acc@5: 87.5000 (90.6200)
tb
loss  0 : 1.9188390625 27
loss  1 : 1.484746875 27
loss  2 : 1.717121875 27
loss  3 : 1.6003765625 27
{'loss_different_0': 1.9188390625, 'loss_different_1': 1.484746875, 'loss_different_2': 1.717121875, 'loss_different_3': 1.6003765625}
Top1:  66.87
Train: 28 [   0/390 (  0%)]  Loss:  1.224944 (1.2249)  Time: 1.417s,   90.33/s  (1.417s,   90.33/s)  LR: 9.524e-03  Data: 0.607 (0.607)
Train: 28 [  50/390 ( 13%)]  Loss:  1.468935 (1.3469)  Time: 0.747s,  171.39/s  (0.761s,  168.26/s)  LR: 9.524e-03  Data: 0.000 (0.012)
Train: 28 [ 100/390 ( 26%)]  Loss:  1.462568 (1.3855)  Time: 0.747s,  171.37/s  (0.757s,  169.06/s)  LR: 9.524e-03  Data: 0.000 (0.006)
Train: 28 [ 150/390 ( 39%)]  Loss:  1.564336 (1.4302)  Time: 0.746s,  171.48/s  (0.754s,  169.77/s)  LR: 9.524e-03  Data: 0.000 (0.004)
Train: 28 [ 200/390 ( 51%)]  Loss:  1.536842 (1.4515)  Time: 0.748s,  171.19/s  (0.752s,  170.12/s)  LR: 9.524e-03  Data: 0.000 (0.003)
Train: 28 [ 250/390 ( 64%)]  Loss:  1.108649 (1.3944)  Time: 0.747s,  171.45/s  (0.751s,  170.34/s)  LR: 9.524e-03  Data: 0.000 (0.003)
Train: 28 [ 300/390 ( 77%)]  Loss:  1.303033 (1.3813)  Time: 0.748s,  171.15/s  (0.751s,  170.49/s)  LR: 9.524e-03  Data: 0.001 (0.002)
Train: 28 [ 350/390 ( 90%)]  Loss:  1.542783 (1.4015)  Time: 0.750s,  170.61/s  (0.750s,  170.60/s)  LR: 9.524e-03  Data: 0.003 (0.002)
Train: 28 [ 389/390 (100%)]  Loss:  1.984715 (1.4663)  Time: 0.748s,  171.19/s  (0.750s,  170.67/s)  LR: 9.524e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.397 (1.397)  DataTime: 1.099 (1.099)  Loss:  1.0303 (1.0303)  Acc@1: 73.4375 (73.4375)  Acc@5: 93.7500 (93.7500)
Test: [  50/78]  Time: 0.259 (0.281)  DataTime: 0.000 (0.022)  Loss:  1.3564 (1.2121)  Acc@1: 60.9375 (66.9424)  Acc@5: 92.1875 (90.4259)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.014)  Loss:  1.0537 (1.2137)  Acc@1: 75.0000 (67.0100)  Acc@5: 81.2500 (90.3700)
tb
loss  0 : 1.947721875 28
loss  1 : 1.5328515625 28
loss  2 : 1.7172546875 28
loss  3 : 1.5812859375 28
{'loss_different_0': 1.947721875, 'loss_different_1': 1.5328515625, 'loss_different_2': 1.7172546875, 'loss_different_3': 1.5812859375}
Top1:  67.01
Train: 29 [   0/390 (  0%)]  Loss:  3.257177 (3.2572)  Time: 1.442s,   88.78/s  (1.442s,   88.78/s)  LR: 9.490e-03  Data: 0.636 (0.636)
Train: 29 [  50/390 ( 13%)]  Loss:  1.365312 (2.3112)  Time: 0.748s,  171.17/s  (0.761s,  168.22/s)  LR: 9.490e-03  Data: 0.000 (0.013)
Train: 29 [ 100/390 ( 26%)]  Loss:  1.120125 (1.9142)  Time: 0.748s,  171.16/s  (0.754s,  169.71/s)  LR: 9.490e-03  Data: 0.000 (0.007)
Train: 29 [ 150/390 ( 39%)]  Loss:  1.488357 (1.8077)  Time: 0.749s,  171.01/s  (0.752s,  170.22/s)  LR: 9.490e-03  Data: 0.000 (0.005)
Train: 29 [ 200/390 ( 51%)]  Loss:  1.299006 (1.7060)  Time: 0.747s,  171.27/s  (0.751s,  170.48/s)  LR: 9.490e-03  Data: 0.000 (0.004)
Train: 29 [ 250/390 ( 64%)]  Loss:  1.195393 (1.6209)  Time: 0.747s,  171.43/s  (0.750s,  170.64/s)  LR: 9.490e-03  Data: 0.000 (0.003)
Train: 29 [ 300/390 ( 77%)]  Loss:  1.323710 (1.5784)  Time: 0.747s,  171.30/s  (0.750s,  170.74/s)  LR: 9.490e-03  Data: 0.000 (0.002)
Train: 29 [ 350/390 ( 90%)]  Loss:  3.020598 (1.7587)  Time: 0.749s,  170.81/s  (0.749s,  170.81/s)  LR: 9.490e-03  Data: 0.003 (0.002)
Train: 29 [ 389/390 (100%)]  Loss:  1.272394 (1.7047)  Time: 0.747s,  171.30/s  (0.750s,  170.75/s)  LR: 9.490e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.970 (0.970)  DataTime: 0.702 (0.702)  Loss:  1.0938 (1.0938)  Acc@1: 72.6562 (72.6562)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.014)  Loss:  1.3271 (1.2158)  Acc@1: 63.2812 (66.6820)  Acc@5: 93.7500 (90.7935)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  1.2031 (1.2098)  Acc@1: 68.7500 (66.9000)  Acc@5: 93.7500 (90.7700)
tb
loss  0 : 1.92529375 29
loss  1 : 1.4923390625 29
loss  2 : 1.75333125 29
loss  3 : 1.576840625 29
{'loss_different_0': 1.92529375, 'loss_different_1': 1.4923390625, 'loss_different_2': 1.75333125, 'loss_different_3': 1.576840625}
Top1:  66.9
Train: 30 [   0/390 (  0%)]  Loss:  3.358334 (3.3583)  Time: 1.644s,   77.87/s  (1.644s,   77.87/s)  LR: 9.455e-03  Data: 0.895 (0.895)
Train: 30 [  50/390 ( 13%)]  Loss:  2.240233 (2.7993)  Time: 0.747s,  171.25/s  (0.765s,  167.36/s)  LR: 9.455e-03  Data: 0.000 (0.018)
Train: 30 [ 100/390 ( 26%)]  Loss:  3.739017 (3.1125)  Time: 0.747s,  171.28/s  (0.756s,  169.28/s)  LR: 9.455e-03  Data: 0.000 (0.009)
Train: 30 [ 150/390 ( 39%)]  Loss:  1.582343 (2.7300)  Time: 0.748s,  171.01/s  (0.753s,  169.93/s)  LR: 9.455e-03  Data: 0.000 (0.006)
Train: 30 [ 200/390 ( 51%)]  Loss:  1.346075 (2.4532)  Time: 0.747s,  171.29/s  (0.752s,  170.26/s)  LR: 9.455e-03  Data: 0.000 (0.005)
Train: 30 [ 250/390 ( 64%)]  Loss:  1.406292 (2.2787)  Time: 0.748s,  171.22/s  (0.751s,  170.46/s)  LR: 9.455e-03  Data: 0.000 (0.004)
Train: 30 [ 300/390 ( 77%)]  Loss:  1.241280 (2.1305)  Time: 0.747s,  171.27/s  (0.750s,  170.60/s)  LR: 9.455e-03  Data: 0.000 (0.003)
Train: 30 [ 350/390 ( 90%)]  Loss:  1.348850 (2.0328)  Time: 0.751s,  170.53/s  (0.750s,  170.70/s)  LR: 9.455e-03  Data: 0.003 (0.003)
Train: 30 [ 389/390 (100%)]  Loss:  1.326374 (1.9543)  Time: 0.747s,  171.44/s  (0.750s,  170.77/s)  LR: 9.455e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.127 (1.127)  DataTime: 0.856 (0.856)  Loss:  1.1758 (1.1758)  Acc@1: 67.1875 (67.1875)  Acc@5: 90.6250 (90.6250)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.017)  Loss:  1.3447 (1.2414)  Acc@1: 62.5000 (65.8854)  Acc@5: 91.4062 (90.5944)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  0.8618 (1.2323)  Acc@1: 75.0000 (66.5100)  Acc@5: 93.7500 (90.8200)
tb
loss  0 : 1.9367375 30
loss  1 : 1.55018125 30
loss  2 : 1.7523984375 30
loss  3 : 1.6068046875 30
{'loss_different_0': 1.9367375, 'loss_different_1': 1.55018125, 'loss_different_2': 1.7523984375, 'loss_different_3': 1.6068046875}
Top1:  66.51
Saving model to ./output/train/20250829-092114-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_3/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_3_30.pt
Train: 31 [   0/390 (  0%)]  Loss:  1.207839 (1.2078)  Time: 1.024s,  125.05/s  (1.024s,  125.05/s)  LR: 9.419e-03  Data: 0.267 (0.267)
Train: 31 [  50/390 ( 13%)]  Loss:  1.443596 (1.3257)  Time: 0.748s,  171.22/s  (0.753s,  170.07/s)  LR: 9.419e-03  Data: 0.000 (0.006)
Train: 31 [ 100/390 ( 26%)]  Loss:  1.207638 (1.2864)  Time: 0.747s,  171.32/s  (0.750s,  170.69/s)  LR: 9.419e-03  Data: 0.000 (0.003)
Train: 31 [ 150/390 ( 39%)]  Loss:  1.344194 (1.3008)  Time: 0.747s,  171.39/s  (0.749s,  170.89/s)  LR: 9.419e-03  Data: 0.000 (0.002)
Train: 31 [ 200/390 ( 51%)]  Loss:  1.320066 (1.3047)  Time: 0.747s,  171.43/s  (0.749s,  171.00/s)  LR: 9.419e-03  Data: 0.000 (0.002)
^C