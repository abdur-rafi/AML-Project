2025-08-31 04:37:05.476198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1756615025.673018      98 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1756615025.730024      98 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
create dataset:  cifar10
Creating model..
, number of params: 27599012
SGD!!!
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Train: 0 [   0/391 (  0%)]  Loss:  4.612122 (4.6121)  Time: 1.717s,   74.57/s  (1.717s,   74.57/s)  LR: 1.000e-03  Data: 0.065 (0.065)
Train: 0 [  50/391 ( 13%)]  Loss:  4.172299 (4.3922)  Time: 0.793s,  161.39/s  (0.810s,  158.10/s)  LR: 1.000e-03  Data: 0.045 (0.044)
Train: 0 [ 100/391 ( 26%)]  Loss:  3.726197 (4.1702)  Time: 0.792s,  161.54/s  (0.800s,  160.01/s)  LR: 1.000e-03  Data: 0.045 (0.044)
Train: 0 [ 150/391 ( 38%)]  Loss:  3.371542 (3.9705)  Time: 0.791s,  161.83/s  (0.797s,  160.58/s)  LR: 1.000e-03  Data: 0.044 (0.044)
Train: 0 [ 200/391 ( 51%)]  Loss:  3.107936 (3.7980)  Time: 0.794s,  161.24/s  (0.796s,  160.79/s)  LR: 1.000e-03  Data: 0.047 (0.044)
Train: 0 [ 250/391 ( 64%)]  Loss:  2.956440 (3.6578)  Time: 0.789s,  162.19/s  (0.795s,  161.08/s)  LR: 1.000e-03  Data: 0.042 (0.043)
Train: 0 [ 300/391 ( 77%)]  Loss:  2.856607 (3.5433)  Time: 0.793s,  161.38/s  (0.794s,  161.26/s)  LR: 1.000e-03  Data: 0.046 (0.043)
Train: 0 [ 350/391 ( 90%)]  Loss:  2.704163 (3.4384)  Time: 0.790s,  162.01/s  (0.793s,  161.38/s)  LR: 1.000e-03  Data: 0.043 (0.043)
Train: 0 [ 390/391 (100%)]  Loss:  2.627196 (3.3796)  Time: 0.510s,  156.88/s  (0.792s,  101.00/s)  LR: 1.000e-03  Data: 0.029 (0.043)
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Test: [   0/99]  Time: 0.358 (0.358)  DataTime: 0.021 (0.021)  Loss:  2.4531 (2.4531)  Acc@1: 25.0000 (25.0000)  Acc@5: 78.0000 (78.0000)
Test: [  50/99]  Time: 0.237 (0.235)  DataTime: 0.025 (0.021)  Loss:  2.4922 (2.4807)  Acc@1: 20.0000 (18.6275)  Acc@5: 64.0000 (73.2353)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.021)  Loss:  2.4395 (2.4782)  Acc@1: 18.0000 (19.3400)  Acc@5: 69.0000 (73.0600)
tb
loss  0 : 2.6559375 0
loss  1 : 2.3798046875 0
loss  2 : 2.5094140625 0
loss  3 : 2.42462890625 0
{'loss_different_0': 2.6559375, 'loss_different_1': 2.3798046875, 'loss_different_2': 2.5094140625, 'loss_different_3': 2.42462890625}
Top1:  19.34
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_0.pt
Train: 1 [   0/391 (  0%)]  Loss:  2.647873 (2.6479)  Time: 0.790s,  162.07/s  (0.790s,  162.07/s)  LR: 9.973e-04  Data: 0.044 (0.044)
Train: 1 [  50/391 ( 13%)]  Loss:  2.621987 (2.6349)  Time: 0.790s,  161.99/s  (0.789s,  162.17/s)  LR: 9.973e-04  Data: 0.043 (0.043)
Train: 1 [ 100/391 ( 26%)]  Loss:  2.556323 (2.6087)  Time: 0.791s,  161.82/s  (0.790s,  162.07/s)  LR: 9.973e-04  Data: 0.044 (0.043)
Train: 1 [ 150/391 ( 38%)]  Loss:  2.526285 (2.5881)  Time: 0.788s,  162.44/s  (0.789s,  162.14/s)  LR: 9.973e-04  Data: 0.041 (0.043)
Train: 1 [ 200/391 ( 51%)]  Loss:  2.540687 (2.5786)  Time: 0.786s,  162.89/s  (0.789s,  162.17/s)  LR: 9.973e-04  Data: 0.040 (0.043)
Train: 1 [ 250/391 ( 64%)]  Loss:  2.472528 (2.5609)  Time: 0.803s,  159.32/s  (0.789s,  162.17/s)  LR: 9.973e-04  Data: 0.057 (0.043)
Train: 1 [ 300/391 ( 77%)]  Loss:  2.470913 (2.5481)  Time: 0.788s,  162.33/s  (0.789s,  162.18/s)  LR: 9.973e-04  Data: 0.042 (0.043)
Train: 1 [ 350/391 ( 90%)]  Loss:  2.391440 (2.5285)  Time: 0.788s,  162.38/s  (0.789s,  162.20/s)  LR: 9.973e-04  Data: 0.042 (0.043)
Train: 1 [ 390/391 (100%)]  Loss:  2.408311 (2.5198)  Time: 0.511s,  156.69/s  (0.788s,  101.47/s)  LR: 9.973e-04  Data: 0.030 (0.042)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  2.1523 (2.1523)  Acc@1: 28.0000 (28.0000)  Acc@5: 82.0000 (82.0000)
Test: [  50/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  2.2207 (2.1921)  Acc@1: 22.0000 (21.8627)  Acc@5: 68.0000 (76.1765)
Test: [  99/99]  Time: 0.233 (0.232)  DataTime: 0.021 (0.020)  Loss:  2.1367 (2.1862)  Acc@1: 25.0000 (22.2700)  Acc@5: 74.0000 (76.3700)
tb
loss  0 : 2.315234375 1
loss  1 : 2.14369140625 1
loss  2 : 2.2009765625 1
loss  3 : 2.1622265625 1
{'loss_different_0': 2.315234375, 'loss_different_1': 2.14369140625, 'loss_different_2': 2.2009765625, 'loss_different_3': 2.1622265625}
Top1:  22.27
Train: 2 [   0/391 (  0%)]  Loss:  2.403149 (2.4031)  Time: 0.793s,  161.49/s  (0.793s,  161.49/s)  LR: 9.891e-04  Data: 0.046 (0.046)
Train: 2 [  50/391 ( 13%)]  Loss:  2.474786 (2.4390)  Time: 0.787s,  162.72/s  (0.792s,  161.58/s)  LR: 9.891e-04  Data: 0.040 (0.042)
Train: 2 [ 100/391 ( 26%)]  Loss:  2.407122 (2.4284)  Time: 0.787s,  162.59/s  (0.790s,  161.94/s)  LR: 9.891e-04  Data: 0.041 (0.042)
Train: 2 [ 150/391 ( 38%)]  Loss:  2.339050 (2.4060)  Time: 0.787s,  162.68/s  (0.790s,  162.02/s)  LR: 9.891e-04  Data: 0.041 (0.042)
Train: 2 [ 200/391 ( 51%)]  Loss:  2.393458 (2.4035)  Time: 0.788s,  162.53/s  (0.790s,  162.05/s)  LR: 9.891e-04  Data: 0.041 (0.042)
Train: 2 [ 250/391 ( 64%)]  Loss:  2.353191 (2.3951)  Time: 0.789s,  162.23/s  (0.790s,  162.08/s)  LR: 9.891e-04  Data: 0.042 (0.042)
Train: 2 [ 300/391 ( 77%)]  Loss:  2.375394 (2.3923)  Time: 0.788s,  162.44/s  (0.790s,  162.08/s)  LR: 9.891e-04  Data: 0.041 (0.042)
Train: 2 [ 350/391 ( 90%)]  Loss:  2.320341 (2.3833)  Time: 0.790s,  161.97/s  (0.790s,  162.10/s)  LR: 9.891e-04  Data: 0.043 (0.043)
Train: 2 [ 390/391 (100%)]  Loss:  2.400957 (2.3846)  Time: 0.510s,  156.86/s  (0.789s,  101.40/s)  LR: 9.891e-04  Data: 0.031 (0.043)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.021 (0.021)  Loss:  2.0801 (2.0801)  Acc@1: 31.0000 (31.0000)  Acc@5: 81.0000 (81.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  2.1523 (2.1117)  Acc@1: 20.0000 (24.9020)  Acc@5: 72.0000 (78.7255)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  2.0762 (2.1050)  Acc@1: 25.0000 (25.2200)  Acc@5: 76.0000 (78.8000)
tb
loss  0 : 2.22638671875 2
loss  1 : 2.0705078125 2
loss  2 : 2.120244140625 2
loss  3 : 2.090498046875 2
{'loss_different_0': 2.22638671875, 'loss_different_1': 2.0705078125, 'loss_different_2': 2.120244140625, 'loss_different_3': 2.090498046875}
Top1:  25.22
Train: 3 [   0/391 (  0%)]  Loss:  2.431625 (2.4316)  Time: 0.796s,  160.77/s  (0.796s,  160.77/s)  LR: 9.755e-04  Data: 0.049 (0.049)
Train: 3 [  50/391 ( 13%)]  Loss:  2.494772 (2.4632)  Time: 0.789s,  162.17/s  (0.789s,  162.21/s)  LR: 9.755e-04  Data: 0.043 (0.042)
Train: 3 [ 100/391 ( 26%)]  Loss:  2.376666 (2.4344)  Time: 0.792s,  161.54/s  (0.790s,  162.11/s)  LR: 9.755e-04  Data: 0.046 (0.043)
Train: 3 [ 150/391 ( 38%)]  Loss:  2.333370 (2.4091)  Time: 0.792s,  161.64/s  (0.790s,  162.08/s)  LR: 9.755e-04  Data: 0.045 (0.043)
Train: 3 [ 200/391 ( 51%)]  Loss:  2.381523 (2.4036)  Time: 0.788s,  162.51/s  (0.790s,  162.09/s)  LR: 9.755e-04  Data: 0.042 (0.043)
Train: 3 [ 250/391 ( 64%)]  Loss:  2.332774 (2.3918)  Time: 0.788s,  162.48/s  (0.791s,  161.84/s)  LR: 9.755e-04  Data: 0.042 (0.043)
Train: 3 [ 300/391 ( 77%)]  Loss:  2.470632 (2.4031)  Time: 0.791s,  161.76/s  (0.791s,  161.84/s)  LR: 9.755e-04  Data: 0.044 (0.043)
Train: 3 [ 350/391 ( 90%)]  Loss:  2.361481 (2.3979)  Time: 0.796s,  160.82/s  (0.791s,  161.82/s)  LR: 9.755e-04  Data: 0.048 (0.043)
Train: 3 [ 390/391 (100%)]  Loss:  2.348553 (2.3943)  Time: 0.511s,  156.42/s  (0.790s,  101.22/s)  LR: 9.755e-04  Data: 0.032 (0.044)
Test: [   0/99]  Time: 0.236 (0.236)  DataTime: 0.024 (0.024)  Loss:  2.0449 (2.0449)  Acc@1: 25.0000 (25.0000)  Acc@5: 82.0000 (82.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  2.1211 (2.0700)  Acc@1: 26.0000 (25.5490)  Acc@5: 77.0000 (80.5882)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  2.0410 (2.0634)  Acc@1: 22.0000 (26.1600)  Acc@5: 82.0000 (80.7800)
tb
loss  0 : 2.19353515625 3
loss  1 : 2.033056640625 3
loss  2 : 2.073037109375 3
loss  3 : 2.057314453125 3
{'loss_different_0': 2.19353515625, 'loss_different_1': 2.033056640625, 'loss_different_2': 2.073037109375, 'loss_different_3': 2.057314453125}
Top1:  26.16
Train: 4 [   0/391 (  0%)]  Loss:  2.346749 (2.3467)  Time: 0.795s,  160.98/s  (0.795s,  160.98/s)  LR: 9.568e-04  Data: 0.048 (0.048)
Train: 4 [  50/391 ( 13%)]  Loss:  2.413190 (2.3800)  Time: 0.792s,  161.67/s  (0.792s,  161.70/s)  LR: 9.568e-04  Data: 0.045 (0.045)
Train: 4 [ 100/391 ( 26%)]  Loss:  2.365312 (2.3751)  Time: 0.791s,  161.87/s  (0.792s,  161.71/s)  LR: 9.568e-04  Data: 0.044 (0.045)
Train: 4 [ 150/391 ( 38%)]  Loss:  2.348456 (2.3684)  Time: 0.791s,  161.84/s  (0.791s,  161.75/s)  LR: 9.568e-04  Data: 0.044 (0.045)
Train: 4 [ 200/391 ( 51%)]  Loss:  2.288563 (2.3525)  Time: 0.790s,  162.01/s  (0.791s,  161.74/s)  LR: 9.568e-04  Data: 0.045 (0.045)
Train: 4 [ 250/391 ( 64%)]  Loss:  2.282457 (2.3408)  Time: 0.793s,  161.47/s  (0.791s,  161.72/s)  LR: 9.568e-04  Data: 0.045 (0.045)
Train: 4 [ 300/391 ( 77%)]  Loss:  2.261101 (2.3294)  Time: 0.791s,  161.74/s  (0.791s,  161.73/s)  LR: 9.568e-04  Data: 0.045 (0.045)
Train: 4 [ 350/391 ( 90%)]  Loss:  2.345696 (2.3314)  Time: 0.790s,  161.98/s  (0.791s,  161.74/s)  LR: 9.568e-04  Data: 0.044 (0.045)
Train: 4 [ 390/391 (100%)]  Loss:  2.303370 (2.3294)  Time: 0.511s,  156.52/s  (0.791s,  101.18/s)  LR: 9.568e-04  Data: 0.031 (0.045)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.022 (0.022)  Loss:  2.0059 (2.0059)  Acc@1: 32.0000 (32.0000)  Acc@5: 83.0000 (83.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  2.0918 (2.0353)  Acc@1: 25.0000 (27.1569)  Acc@5: 83.0000 (81.3333)
Test: [  99/99]  Time: 0.234 (0.235)  DataTime: 0.021 (0.022)  Loss:  2.0195 (2.0293)  Acc@1: 30.0000 (27.6300)  Acc@5: 82.0000 (81.3200)
tb
loss  0 : 2.142021484375 4
loss  1 : 2.004443359375 4
loss  2 : 2.041611328125 4
loss  3 : 2.026123046875 4
{'loss_different_0': 2.142021484375, 'loss_different_1': 2.004443359375, 'loss_different_2': 2.041611328125, 'loss_different_3': 2.026123046875}
Top1:  27.63
Train: 5 [   0/391 (  0%)]  Loss:  2.285332 (2.2853)  Time: 0.794s,  161.20/s  (0.794s,  161.20/s)  LR: 9.330e-04  Data: 0.047 (0.047)
Train: 5 [  50/391 ( 13%)]  Loss:  2.435510 (2.3604)  Time: 0.793s,  161.37/s  (0.791s,  161.78/s)  LR: 9.330e-04  Data: 0.046 (0.044)
Train: 5 [ 100/391 ( 26%)]  Loss:  2.267678 (2.3295)  Time: 0.790s,  161.93/s  (0.791s,  161.72/s)  LR: 9.330e-04  Data: 0.043 (0.044)
Train: 5 [ 150/391 ( 38%)]  Loss:  2.260516 (2.3123)  Time: 0.792s,  161.60/s  (0.791s,  161.75/s)  LR: 9.330e-04  Data: 0.045 (0.044)
Train: 5 [ 200/391 ( 51%)]  Loss:  2.260977 (2.3020)  Time: 0.801s,  159.78/s  (0.792s,  161.53/s)  LR: 9.330e-04  Data: 0.053 (0.044)
Train: 5 [ 250/391 ( 64%)]  Loss:  2.273422 (2.2972)  Time: 0.791s,  161.75/s  (0.792s,  161.57/s)  LR: 9.330e-04  Data: 0.044 (0.044)
Train: 5 [ 300/391 ( 77%)]  Loss:  2.227538 (2.2873)  Time: 0.792s,  161.70/s  (0.792s,  161.60/s)  LR: 9.330e-04  Data: 0.044 (0.044)
Train: 5 [ 350/391 ( 90%)]  Loss:  2.292700 (2.2880)  Time: 0.791s,  161.74/s  (0.792s,  161.61/s)  LR: 9.330e-04  Data: 0.044 (0.044)
Train: 5 [ 390/391 (100%)]  Loss:  2.251850 (2.2853)  Time: 0.509s,  157.21/s  (0.791s,  101.11/s)  LR: 9.330e-04  Data: 0.029 (0.044)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.9824 (1.9824)  Acc@1: 32.0000 (32.0000)  Acc@5: 84.0000 (84.0000)
Test: [  50/99]  Time: 0.236 (0.234)  DataTime: 0.024 (0.022)  Loss:  2.0547 (2.0086)  Acc@1: 29.0000 (27.6667)  Acc@5: 82.0000 (82.0784)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  1.9854 (2.0034)  Acc@1: 31.0000 (28.2400)  Acc@5: 82.0000 (82.1300)
tb
loss  0 : 2.12267578125 5
loss  1 : 1.982900390625 5
loss  2 : 2.014462890625 5
loss  3 : 2.00369140625 5
{'loss_different_0': 2.12267578125, 'loss_different_1': 1.982900390625, 'loss_different_2': 2.014462890625, 'loss_different_3': 2.00369140625}
Top1:  28.24
Train: 6 [   0/391 (  0%)]  Loss:  2.337592 (2.3376)  Time: 0.792s,  161.69/s  (0.792s,  161.69/s)  LR: 9.045e-04  Data: 0.046 (0.046)
Train: 6 [  50/391 ( 13%)]  Loss:  2.290427 (2.3140)  Time: 0.789s,  162.19/s  (0.791s,  161.92/s)  LR: 9.045e-04  Data: 0.043 (0.044)
Train: 6 [ 100/391 ( 26%)]  Loss:  2.240195 (2.2894)  Time: 0.789s,  162.23/s  (0.791s,  161.91/s)  LR: 9.045e-04  Data: 0.043 (0.044)
Train: 6 [ 150/391 ( 38%)]  Loss:  2.338500 (2.3017)  Time: 0.789s,  162.27/s  (0.791s,  161.86/s)  LR: 9.045e-04  Data: 0.042 (0.044)
Train: 6 [ 200/391 ( 51%)]  Loss:  2.467725 (2.3349)  Time: 0.791s,  161.81/s  (0.791s,  161.86/s)  LR: 9.045e-04  Data: 0.044 (0.044)
Train: 6 [ 250/391 ( 64%)]  Loss:  2.248409 (2.3205)  Time: 0.791s,  161.78/s  (0.791s,  161.84/s)  LR: 9.045e-04  Data: 0.045 (0.044)
Train: 6 [ 300/391 ( 77%)]  Loss:  2.326263 (2.3213)  Time: 0.791s,  161.82/s  (0.791s,  161.83/s)  LR: 9.045e-04  Data: 0.043 (0.044)
Train: 6 [ 350/391 ( 90%)]  Loss:  2.173179 (2.3028)  Time: 0.790s,  162.07/s  (0.792s,  161.67/s)  LR: 9.045e-04  Data: 0.043 (0.044)
Train: 6 [ 390/391 (100%)]  Loss:  2.342119 (2.3056)  Time: 0.511s,  156.67/s  (0.791s,  101.14/s)  LR: 9.045e-04  Data: 0.031 (0.044)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.9502 (1.9502)  Acc@1: 29.0000 (29.0000)  Acc@5: 87.0000 (87.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  2.0449 (1.9846)  Acc@1: 24.0000 (28.4902)  Acc@5: 80.0000 (82.2745)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  1.9600 (1.9800)  Acc@1: 33.0000 (28.6500)  Acc@5: 84.0000 (82.2600)
tb
loss  0 : 2.08875 6
loss  1 : 1.962294921875 6
loss  2 : 1.990380859375 6
loss  3 : 1.988876953125 6
{'loss_different_0': 2.08875, 'loss_different_1': 1.962294921875, 'loss_different_2': 1.990380859375, 'loss_different_3': 1.988876953125}
Top1:  28.65
Train: 7 [   0/391 (  0%)]  Loss:  2.220045 (2.2200)  Time: 0.793s,  161.38/s  (0.793s,  161.38/s)  LR: 8.716e-04  Data: 0.046 (0.046)
Train: 7 [  50/391 ( 13%)]  Loss:  2.448623 (2.3343)  Time: 0.788s,  162.44/s  (0.791s,  161.91/s)  LR: 8.716e-04  Data: 0.041 (0.044)
Train: 7 [ 100/391 ( 26%)]  Loss:  2.235937 (2.3015)  Time: 0.790s,  161.99/s  (0.790s,  161.99/s)  LR: 8.716e-04  Data: 0.044 (0.043)
Train: 7 [ 150/391 ( 38%)]  Loss:  2.265565 (2.2925)  Time: 0.791s,  161.87/s  (0.790s,  161.99/s)  LR: 8.716e-04  Data: 0.045 (0.043)
Train: 7 [ 200/391 ( 51%)]  Loss:  2.220841 (2.2782)  Time: 0.789s,  162.20/s  (0.790s,  161.99/s)  LR: 8.716e-04  Data: 0.043 (0.043)
Train: 7 [ 250/391 ( 64%)]  Loss:  2.300514 (2.2819)  Time: 0.789s,  162.22/s  (0.790s,  162.00/s)  LR: 8.716e-04  Data: 0.042 (0.043)
Train: 7 [ 300/391 ( 77%)]  Loss:  2.260119 (2.2788)  Time: 0.794s,  161.22/s  (0.790s,  162.01/s)  LR: 8.716e-04  Data: 0.047 (0.043)
Train: 7 [ 350/391 ( 90%)]  Loss:  2.315691 (2.2834)  Time: 0.793s,  161.35/s  (0.790s,  161.99/s)  LR: 8.716e-04  Data: 0.046 (0.043)
Train: 7 [ 390/391 (100%)]  Loss:  2.268361 (2.2823)  Time: 0.509s,  157.10/s  (0.789s,  101.34/s)  LR: 8.716e-04  Data: 0.029 (0.043)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.023 (0.023)  Loss:  1.9219 (1.9219)  Acc@1: 34.0000 (34.0000)  Acc@5: 87.0000 (87.0000)
Test: [  50/99]  Time: 0.235 (0.234)  DataTime: 0.022 (0.021)  Loss:  2.0234 (1.9692)  Acc@1: 26.0000 (28.6667)  Acc@5: 80.0000 (82.9020)
Test: [  99/99]  Time: 0.236 (0.233)  DataTime: 0.024 (0.021)  Loss:  1.9365 (1.9649)  Acc@1: 31.0000 (29.3000)  Acc@5: 86.0000 (82.8100)
tb
loss  0 : 2.07529296875 7
loss  1 : 1.9501953125 7
loss  2 : 1.975263671875 7
loss  3 : 1.984013671875 7
{'loss_different_0': 2.07529296875, 'loss_different_1': 1.9501953125, 'loss_different_2': 1.975263671875, 'loss_different_3': 1.984013671875}
Top1:  29.3
Train: 8 [   0/391 (  0%)]  Loss:  2.283014 (2.2830)  Time: 0.793s,  161.43/s  (0.793s,  161.43/s)  LR: 8.346e-04  Data: 0.046 (0.046)
Train: 8 [  50/391 ( 13%)]  Loss:  2.389501 (2.3363)  Time: 0.800s,  160.00/s  (0.791s,  161.83/s)  LR: 8.346e-04  Data: 0.053 (0.044)
Train: 8 [ 100/391 ( 26%)]  Loss:  2.456943 (2.3765)  Time: 0.789s,  162.16/s  (0.791s,  161.81/s)  LR: 8.346e-04  Data: 0.042 (0.044)
Train: 8 [ 150/391 ( 38%)]  Loss:  2.233415 (2.3407)  Time: 0.789s,  162.14/s  (0.791s,  161.80/s)  LR: 8.346e-04  Data: 0.043 (0.044)
Train: 8 [ 200/391 ( 51%)]  Loss:  2.284598 (2.3295)  Time: 0.798s,  160.49/s  (0.791s,  161.79/s)  LR: 8.346e-04  Data: 0.051 (0.044)
Train: 8 [ 250/391 ( 64%)]  Loss:  2.300619 (2.3247)  Time: 0.791s,  161.80/s  (0.791s,  161.75/s)  LR: 8.346e-04  Data: 0.044 (0.044)
Train: 8 [ 300/391 ( 77%)]  Loss:  2.249092 (2.3139)  Time: 0.790s,  162.12/s  (0.792s,  161.64/s)  LR: 8.346e-04  Data: 0.043 (0.044)
Train: 8 [ 350/391 ( 90%)]  Loss:  2.234919 (2.3040)  Time: 0.789s,  162.20/s  (0.792s,  161.67/s)  LR: 8.346e-04  Data: 0.042 (0.044)
Train: 8 [ 390/391 (100%)]  Loss:  2.217535 (2.2977)  Time: 0.508s,  157.47/s  (0.791s,  101.15/s)  LR: 8.346e-04  Data: 0.028 (0.044)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.023 (0.023)  Loss:  1.9131 (1.9131)  Acc@1: 33.0000 (33.0000)  Acc@5: 87.0000 (87.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  2.0137 (1.9474)  Acc@1: 30.0000 (29.5686)  Acc@5: 82.0000 (83.6471)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.021 (0.022)  Loss:  1.9189 (1.9440)  Acc@1: 35.0000 (30.0700)  Acc@5: 82.0000 (83.4500)
tb
loss  0 : 2.059765625 8
loss  1 : 1.927568359375 8
loss  2 : 1.95607421875 8
loss  3 : 1.956025390625 8
{'loss_different_0': 2.059765625, 'loss_different_1': 1.927568359375, 'loss_different_2': 1.95607421875, 'loss_different_3': 1.956025390625}
Top1:  30.07
Train: 9 [   0/391 (  0%)]  Loss:  2.194157 (2.1942)  Time: 0.791s,  161.75/s  (0.791s,  161.75/s)  LR: 7.939e-04  Data: 0.045 (0.045)
Train: 9 [  50/391 ( 13%)]  Loss:  2.236051 (2.2151)  Time: 0.791s,  161.88/s  (0.790s,  162.07/s)  LR: 7.939e-04  Data: 0.044 (0.043)
Train: 9 [ 100/391 ( 26%)]  Loss:  2.362314 (2.2642)  Time: 0.793s,  161.41/s  (0.790s,  162.06/s)  LR: 7.939e-04  Data: 0.046 (0.043)
Train: 9 [ 150/391 ( 38%)]  Loss:  2.212364 (2.2512)  Time: 0.790s,  162.03/s  (0.790s,  162.01/s)  LR: 7.939e-04  Data: 0.043 (0.043)
Train: 9 [ 200/391 ( 51%)]  Loss:  2.440016 (2.2890)  Time: 0.789s,  162.32/s  (0.790s,  162.03/s)  LR: 7.939e-04  Data: 0.041 (0.043)
Train: 9 [ 250/391 ( 64%)]  Loss:  2.245338 (2.2817)  Time: 0.789s,  162.27/s  (0.790s,  162.01/s)  LR: 7.939e-04  Data: 0.042 (0.043)
Train: 9 [ 300/391 ( 77%)]  Loss:  2.199671 (2.2700)  Time: 0.795s,  160.99/s  (0.790s,  162.03/s)  LR: 7.939e-04  Data: 0.049 (0.043)
Train: 9 [ 350/391 ( 90%)]  Loss:  2.271947 (2.2702)  Time: 0.788s,  162.33/s  (0.790s,  162.05/s)  LR: 7.939e-04  Data: 0.042 (0.043)
Train: 9 [ 390/391 (100%)]  Loss:  2.242954 (2.2683)  Time: 0.519s,  154.00/s  (0.789s,  101.37/s)  LR: 7.939e-04  Data: 0.039 (0.043)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.9053 (1.9053)  Acc@1: 32.0000 (32.0000)  Acc@5: 87.0000 (87.0000)
Test: [  50/99]  Time: 0.237 (0.234)  DataTime: 0.025 (0.022)  Loss:  1.9922 (1.9350)  Acc@1: 27.0000 (30.5294)  Acc@5: 83.0000 (83.7255)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.021 (0.021)  Loss:  1.9121 (1.9313)  Acc@1: 32.0000 (30.6600)  Acc@5: 83.0000 (83.6300)
tb
loss  0 : 2.04876953125 9
loss  1 : 1.915703125 9
loss  2 : 1.9441796875 9
loss  3 : 1.9463671875 9
{'loss_different_0': 2.04876953125, 'loss_different_1': 1.915703125, 'loss_different_2': 1.9441796875, 'loss_different_3': 1.9463671875}
Top1:  30.66
Train: 10 [   0/391 (  0%)]  Loss:  2.299943 (2.2999)  Time: 0.794s,  161.31/s  (0.794s,  161.31/s)  LR: 7.500e-04  Data: 0.047 (0.047)
Train: 10 [  50/391 ( 13%)]  Loss:  2.210030 (2.2550)  Time: 0.791s,  161.84/s  (0.795s,  160.93/s)  LR: 7.500e-04  Data: 0.044 (0.043)
Train: 10 [ 100/391 ( 26%)]  Loss:  2.283702 (2.2646)  Time: 0.792s,  161.63/s  (0.793s,  161.42/s)  LR: 7.500e-04  Data: 0.046 (0.044)
Train: 10 [ 150/391 ( 38%)]  Loss:  2.291840 (2.2714)  Time: 0.792s,  161.55/s  (0.792s,  161.57/s)  LR: 7.500e-04  Data: 0.046 (0.044)
Train: 10 [ 200/391 ( 51%)]  Loss:  2.290365 (2.2752)  Time: 0.790s,  162.10/s  (0.792s,  161.64/s)  LR: 7.500e-04  Data: 0.043 (0.044)
Train: 10 [ 250/391 ( 64%)]  Loss:  2.341197 (2.2862)  Time: 0.791s,  161.74/s  (0.792s,  161.69/s)  LR: 7.500e-04  Data: 0.045 (0.044)
Train: 10 [ 300/391 ( 77%)]  Loss:  2.222429 (2.2771)  Time: 0.794s,  161.24/s  (0.791s,  161.74/s)  LR: 7.500e-04  Data: 0.047 (0.044)
Train: 10 [ 350/391 ( 90%)]  Loss:  2.168039 (2.2634)  Time: 0.790s,  162.12/s  (0.791s,  161.77/s)  LR: 7.500e-04  Data: 0.042 (0.044)
Train: 10 [ 390/391 (100%)]  Loss:  2.337934 (2.2688)  Time: 0.510s,  156.92/s  (0.790s,  101.22/s)  LR: 7.500e-04  Data: 0.030 (0.044)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.8691 (1.8691)  Acc@1: 31.0000 (31.0000)  Acc@5: 86.0000 (86.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.9873 (1.9225)  Acc@1: 33.0000 (30.9020)  Acc@5: 82.0000 (83.9412)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.8887 (1.9197)  Acc@1: 35.0000 (31.0100)  Acc@5: 85.0000 (83.8700)
tb
loss  0 : 2.029853515625 10
loss  1 : 1.90759765625 10
loss  2 : 1.9353515625 10
loss  3 : 1.942880859375 10
{'loss_different_0': 2.029853515625, 'loss_different_1': 1.90759765625, 'loss_different_2': 1.9353515625, 'loss_different_3': 1.942880859375}
Top1:  31.01
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_10.pt
Train: 11 [   0/391 (  0%)]  Loss:  2.258684 (2.2587)  Time: 0.792s,  161.52/s  (0.792s,  161.52/s)  LR: 7.034e-04  Data: 0.046 (0.046)
Train: 11 [  50/391 ( 13%)]  Loss:  2.236468 (2.2476)  Time: 0.788s,  162.49/s  (0.790s,  162.05/s)  LR: 7.034e-04  Data: 0.042 (0.043)
Train: 11 [ 100/391 ( 26%)]  Loss:  2.261446 (2.2522)  Time: 0.793s,  161.45/s  (0.790s,  162.08/s)  LR: 7.034e-04  Data: 0.046 (0.043)
Train: 11 [ 150/391 ( 38%)]  Loss:  2.213886 (2.2426)  Time: 0.789s,  162.23/s  (0.790s,  161.97/s)  LR: 7.034e-04  Data: 0.042 (0.044)
Train: 11 [ 200/391 ( 51%)]  Loss:  2.217019 (2.2375)  Time: 0.789s,  162.24/s  (0.790s,  161.94/s)  LR: 7.034e-04  Data: 0.042 (0.044)
Train: 11 [ 250/391 ( 64%)]  Loss:  2.186830 (2.2291)  Time: 0.787s,  162.56/s  (0.790s,  161.97/s)  LR: 7.034e-04  Data: 0.041 (0.044)
Train: 11 [ 300/391 ( 77%)]  Loss:  2.377986 (2.2503)  Time: 0.790s,  162.07/s  (0.790s,  162.01/s)  LR: 7.034e-04  Data: 0.043 (0.043)
Train: 11 [ 350/391 ( 90%)]  Loss:  2.240230 (2.2491)  Time: 0.789s,  162.21/s  (0.790s,  162.03/s)  LR: 7.034e-04  Data: 0.042 (0.043)
Train: 11 [ 390/391 (100%)]  Loss:  2.286014 (2.2517)  Time: 0.509s,  157.09/s  (0.790s,  101.31/s)  LR: 7.034e-04  Data: 0.030 (0.043)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  1.8555 (1.8555)  Acc@1: 29.0000 (29.0000)  Acc@5: 88.0000 (88.0000)
Test: [  50/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  1.9912 (1.9146)  Acc@1: 33.0000 (30.6471)  Acc@5: 85.0000 (84.2941)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.8838 (1.9113)  Acc@1: 33.0000 (30.9700)  Acc@5: 86.0000 (84.1000)
tb
loss  0 : 2.02470703125 11
loss  1 : 1.900546875 11
loss  2 : 1.925869140625 11
loss  3 : 1.93703125 11
{'loss_different_0': 2.02470703125, 'loss_different_1': 1.900546875, 'loss_different_2': 1.925869140625, 'loss_different_3': 1.93703125}
Top1:  30.97
Train: 12 [   0/391 (  0%)]  Loss:  2.192009 (2.1920)  Time: 0.794s,  161.16/s  (0.794s,  161.16/s)  LR: 6.545e-04  Data: 0.047 (0.047)
Train: 12 [  50/391 ( 13%)]  Loss:  2.188760 (2.1904)  Time: 0.789s,  162.25/s  (0.789s,  162.14/s)  LR: 6.545e-04  Data: 0.042 (0.042)
Train: 12 [ 100/391 ( 26%)]  Loss:  2.231164 (2.2040)  Time: 0.795s,  160.92/s  (0.790s,  162.05/s)  LR: 6.545e-04  Data: 0.048 (0.043)
Train: 12 [ 150/391 ( 38%)]  Loss:  2.193423 (2.2013)  Time: 0.796s,  160.84/s  (0.790s,  162.04/s)  LR: 6.545e-04  Data: 0.050 (0.043)
Train: 12 [ 200/391 ( 51%)]  Loss:  2.183437 (2.1978)  Time: 0.790s,  162.12/s  (0.790s,  161.99/s)  LR: 6.545e-04  Data: 0.042 (0.043)
Train: 12 [ 250/391 ( 64%)]  Loss:  2.271687 (2.2101)  Time: 0.789s,  162.29/s  (0.790s,  162.02/s)  LR: 6.545e-04  Data: 0.042 (0.043)
Train: 12 [ 300/391 ( 77%)]  Loss:  2.190499 (2.2073)  Time: 0.792s,  161.70/s  (0.790s,  162.04/s)  LR: 6.545e-04  Data: 0.045 (0.043)
Train: 12 [ 350/391 ( 90%)]  Loss:  2.213913 (2.2081)  Time: 0.789s,  162.27/s  (0.790s,  162.03/s)  LR: 6.545e-04  Data: 0.042 (0.043)
Train: 12 [ 390/391 (100%)]  Loss:  2.245285 (2.2108)  Time: 0.509s,  157.06/s  (0.789s,  101.36/s)  LR: 6.545e-04  Data: 0.030 (0.043)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.8516 (1.8516)  Acc@1: 37.0000 (37.0000)  Acc@5: 86.0000 (86.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  1.9648 (1.8939)  Acc@1: 29.0000 (32.4902)  Acc@5: 84.0000 (84.8627)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.021 (0.021)  Loss:  1.8672 (1.8923)  Acc@1: 34.0000 (32.0200)  Acc@5: 85.0000 (84.7300)
tb
loss  0 : 2.0297265625 12
loss  1 : 1.87822265625 12
loss  2 : 1.910830078125 12
loss  3 : 1.912099609375 12
{'loss_different_0': 2.0297265625, 'loss_different_1': 1.87822265625, 'loss_different_2': 1.910830078125, 'loss_different_3': 1.912099609375}
Top1:  32.02
Train: 13 [   0/391 (  0%)]  Loss:  2.329000 (2.3290)  Time: 0.794s,  161.15/s  (0.794s,  161.15/s)  LR: 6.040e-04  Data: 0.048 (0.048)
Train: 13 [  50/391 ( 13%)]  Loss:  2.284358 (2.3067)  Time: 0.787s,  162.59/s  (0.789s,  162.24/s)  LR: 6.040e-04  Data: 0.041 (0.042)
Train: 13 [ 100/391 ( 26%)]  Loss:  2.309957 (2.3078)  Time: 0.799s,  160.16/s  (0.789s,  162.17/s)  LR: 6.040e-04  Data: 0.051 (0.043)
Train: 13 [ 150/391 ( 38%)]  Loss:  2.169838 (2.2733)  Time: 0.791s,  161.92/s  (0.791s,  161.78/s)  LR: 6.040e-04  Data: 0.044 (0.043)
Train: 13 [ 200/391 ( 51%)]  Loss:  2.287032 (2.2760)  Time: 0.790s,  162.09/s  (0.791s,  161.84/s)  LR: 6.040e-04  Data: 0.043 (0.043)
Train: 13 [ 250/391 ( 64%)]  Loss:  2.275964 (2.2760)  Time: 0.789s,  162.25/s  (0.791s,  161.91/s)  LR: 6.040e-04  Data: 0.041 (0.043)
Train: 13 [ 300/391 ( 77%)]  Loss:  2.165342 (2.2602)  Time: 0.789s,  162.18/s  (0.790s,  161.95/s)  LR: 6.040e-04  Data: 0.043 (0.043)
Train: 13 [ 350/391 ( 90%)]  Loss:  2.160793 (2.2478)  Time: 0.788s,  162.50/s  (0.790s,  161.97/s)  LR: 6.040e-04  Data: 0.041 (0.043)
Train: 13 [ 390/391 (100%)]  Loss:  2.185718 (2.2433)  Time: 0.509s,  157.10/s  (0.789s,  101.33/s)  LR: 6.040e-04  Data: 0.029 (0.043)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.8428 (1.8428)  Acc@1: 32.0000 (32.0000)  Acc@5: 87.0000 (87.0000)
Test: [  50/99]  Time: 0.232 (0.234)  DataTime: 0.020 (0.021)  Loss:  1.9570 (1.8953)  Acc@1: 32.0000 (31.0196)  Acc@5: 85.0000 (84.7255)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.8457 (1.8916)  Acc@1: 36.0000 (31.5500)  Acc@5: 88.0000 (84.6800)
tb
loss  0 : 2.017734375 13
loss  1 : 1.881953125 13
loss  2 : 1.90736328125 13
loss  3 : 1.91728515625 13
{'loss_different_0': 2.017734375, 'loss_different_1': 1.881953125, 'loss_different_2': 1.90736328125, 'loss_different_3': 1.91728515625}
Top1:  31.55
Train: 14 [   0/391 (  0%)]  Loss:  2.141658 (2.1417)  Time: 0.794s,  161.13/s  (0.794s,  161.13/s)  LR: 5.523e-04  Data: 0.048 (0.048)
Train: 14 [  50/391 ( 13%)]  Loss:  2.137096 (2.1394)  Time: 0.789s,  162.21/s  (0.789s,  162.13/s)  LR: 5.523e-04  Data: 0.042 (0.043)
Train: 14 [ 100/391 ( 26%)]  Loss:  2.226231 (2.1683)  Time: 0.790s,  162.10/s  (0.790s,  162.11/s)  LR: 5.523e-04  Data: 0.043 (0.043)
Train: 14 [ 150/391 ( 38%)]  Loss:  2.187209 (2.1730)  Time: 0.791s,  161.83/s  (0.790s,  162.03/s)  LR: 5.523e-04  Data: 0.045 (0.043)
Train: 14 [ 200/391 ( 51%)]  Loss:  2.213326 (2.1811)  Time: 0.788s,  162.37/s  (0.790s,  162.04/s)  LR: 5.523e-04  Data: 0.042 (0.043)
Train: 14 [ 250/391 ( 64%)]  Loss:  2.198657 (2.1840)  Time: 0.790s,  162.11/s  (0.790s,  162.05/s)  LR: 5.523e-04  Data: 0.043 (0.043)
Train: 14 [ 300/391 ( 77%)]  Loss:  2.137025 (2.1773)  Time: 0.793s,  161.49/s  (0.790s,  162.06/s)  LR: 5.523e-04  Data: 0.046 (0.043)
Train: 14 [ 350/391 ( 90%)]  Loss:  2.219313 (2.1826)  Time: 0.789s,  162.16/s  (0.790s,  162.06/s)  LR: 5.523e-04  Data: 0.043 (0.043)
Train: 14 [ 390/391 (100%)]  Loss:  2.277877 (2.1895)  Time: 0.508s,  157.46/s  (0.789s,  101.38/s)  LR: 5.523e-04  Data: 0.029 (0.043)
Test: [   0/99]  Time: 0.236 (0.236)  DataTime: 0.024 (0.024)  Loss:  1.8525 (1.8525)  Acc@1: 33.0000 (33.0000)  Acc@5: 86.0000 (86.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.9531 (1.8838)  Acc@1: 32.0000 (32.2745)  Acc@5: 82.0000 (84.6667)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.020 (0.021)  Loss:  1.8438 (1.8812)  Acc@1: 32.0000 (32.2300)  Acc@5: 87.0000 (84.7600)
tb
loss  0 : 2.006826171875 14
loss  1 : 1.874794921875 14
loss  2 : 1.897705078125 14
loss  3 : 1.914814453125 14
{'loss_different_0': 2.006826171875, 'loss_different_1': 1.874794921875, 'loss_different_2': 1.897705078125, 'loss_different_3': 1.914814453125}
Top1:  32.23
Train: 15 [   0/391 (  0%)]  Loss:  2.202776 (2.2028)  Time: 0.792s,  161.71/s  (0.792s,  161.71/s)  LR: 5.000e-04  Data: 0.045 (0.045)
Train: 15 [  50/391 ( 13%)]  Loss:  2.178747 (2.1908)  Time: 0.790s,  162.09/s  (0.790s,  162.02/s)  LR: 5.000e-04  Data: 0.043 (0.043)
Train: 15 [ 100/391 ( 26%)]  Loss:  2.336851 (2.2395)  Time: 0.790s,  162.08/s  (0.792s,  161.71/s)  LR: 5.000e-04  Data: 0.043 (0.043)
Train: 15 [ 150/391 ( 38%)]  Loss:  2.213271 (2.2329)  Time: 0.788s,  162.41/s  (0.791s,  161.77/s)  LR: 5.000e-04  Data: 0.041 (0.043)
Train: 15 [ 200/391 ( 51%)]  Loss:  2.156110 (2.2176)  Time: 0.791s,  161.87/s  (0.791s,  161.85/s)  LR: 5.000e-04  Data: 0.044 (0.043)
Train: 15 [ 250/391 ( 64%)]  Loss:  2.337819 (2.2376)  Time: 0.790s,  162.10/s  (0.791s,  161.87/s)  LR: 5.000e-04  Data: 0.043 (0.043)
Train: 15 [ 300/391 ( 77%)]  Loss:  2.190277 (2.2308)  Time: 0.788s,  162.36/s  (0.791s,  161.91/s)  LR: 5.000e-04  Data: 0.042 (0.043)
Train: 15 [ 350/391 ( 90%)]  Loss:  2.174653 (2.2238)  Time: 0.790s,  161.97/s  (0.790s,  161.94/s)  LR: 5.000e-04  Data: 0.043 (0.043)
Train: 15 [ 390/391 (100%)]  Loss:  2.313809 (2.2303)  Time: 0.511s,  156.43/s  (0.790s,  101.32/s)  LR: 5.000e-04  Data: 0.031 (0.043)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.021 (0.021)  Loss:  1.8320 (1.8320)  Acc@1: 34.0000 (34.0000)  Acc@5: 86.0000 (86.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.020 (0.021)  Loss:  1.9531 (1.8771)  Acc@1: 32.0000 (32.8431)  Acc@5: 86.0000 (85.0784)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.8340 (1.8740)  Acc@1: 30.0000 (32.3000)  Acc@5: 86.0000 (85.1700)
tb
loss  0 : 2.01724609375 15
loss  1 : 1.85931640625 15
loss  2 : 1.8923828125 15
loss  3 : 1.890849609375 15
{'loss_different_0': 2.01724609375, 'loss_different_1': 1.85931640625, 'loss_different_2': 1.8923828125, 'loss_different_3': 1.890849609375}
Top1:  32.3
Train: 16 [   0/391 (  0%)]  Loss:  2.132979 (2.1330)  Time: 0.792s,  161.55/s  (0.792s,  161.55/s)  LR: 4.477e-04  Data: 0.045 (0.045)
Train: 16 [  50/391 ( 13%)]  Loss:  2.195504 (2.1642)  Time: 0.790s,  162.11/s  (0.790s,  162.04/s)  LR: 4.477e-04  Data: 0.043 (0.043)
Train: 16 [ 100/391 ( 26%)]  Loss:  2.104199 (2.1442)  Time: 0.788s,  162.38/s  (0.790s,  162.01/s)  LR: 4.477e-04  Data: 0.041 (0.043)
Train: 16 [ 150/391 ( 38%)]  Loss:  2.392095 (2.2062)  Time: 0.789s,  162.23/s  (0.790s,  162.08/s)  LR: 4.477e-04  Data: 0.042 (0.043)
Train: 16 [ 200/391 ( 51%)]  Loss:  2.385856 (2.2421)  Time: 0.789s,  162.33/s  (0.790s,  162.09/s)  LR: 4.477e-04  Data: 0.042 (0.043)
Train: 16 [ 250/391 ( 64%)]  Loss:  2.058449 (2.2115)  Time: 0.789s,  162.24/s  (0.791s,  161.88/s)  LR: 4.477e-04  Data: 0.042 (0.043)
Train: 16 [ 300/391 ( 77%)]  Loss:  2.144946 (2.2020)  Time: 0.791s,  161.82/s  (0.791s,  161.90/s)  LR: 4.477e-04  Data: 0.044 (0.043)
Train: 16 [ 350/391 ( 90%)]  Loss:  2.306849 (2.2151)  Time: 0.790s,  162.12/s  (0.791s,  161.91/s)  LR: 4.477e-04  Data: 0.043 (0.043)
Train: 16 [ 390/391 (100%)]  Loss:  2.174088 (2.2121)  Time: 0.509s,  157.18/s  (0.790s,  101.30/s)  LR: 4.477e-04  Data: 0.029 (0.043)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.8252 (1.8252)  Acc@1: 33.0000 (33.0000)  Acc@5: 86.0000 (86.0000)
Test: [  50/99]  Time: 0.235 (0.234)  DataTime: 0.023 (0.021)  Loss:  1.9268 (1.8716)  Acc@1: 34.0000 (33.3333)  Acc@5: 85.0000 (85.2157)
Test: [  99/99]  Time: 0.235 (0.233)  DataTime: 0.022 (0.021)  Loss:  1.8135 (1.8672)  Acc@1: 32.0000 (33.2400)  Acc@5: 88.0000 (85.3000)
tb
loss  0 : 1.9921484375 16
loss  1 : 1.86572265625 16
loss  2 : 1.88369140625 16
loss  3 : 1.907587890625 16
{'loss_different_0': 1.9921484375, 'loss_different_1': 1.86572265625, 'loss_different_2': 1.88369140625, 'loss_different_3': 1.907587890625}
Top1:  33.24
Train: 17 [   0/391 (  0%)]  Loss:  2.144437 (2.1444)  Time: 0.790s,  162.11/s  (0.790s,  162.11/s)  LR: 3.960e-04  Data: 0.043 (0.043)
Train: 17 [  50/391 ( 13%)]  Loss:  2.232260 (2.1883)  Time: 0.793s,  161.51/s  (0.790s,  161.96/s)  LR: 3.960e-04  Data: 0.046 (0.044)
Train: 17 [ 100/391 ( 26%)]  Loss:  2.208147 (2.1949)  Time: 0.790s,  162.12/s  (0.790s,  161.99/s)  LR: 3.960e-04  Data: 0.043 (0.043)
Train: 17 [ 150/391 ( 38%)]  Loss:  2.226577 (2.2029)  Time: 0.791s,  161.89/s  (0.790s,  162.03/s)  LR: 3.960e-04  Data: 0.044 (0.043)
Train: 17 [ 200/391 ( 51%)]  Loss:  2.244016 (2.2111)  Time: 0.789s,  162.28/s  (0.790s,  162.06/s)  LR: 3.960e-04  Data: 0.043 (0.043)
Train: 17 [ 250/391 ( 64%)]  Loss:  2.155493 (2.2018)  Time: 0.791s,  161.79/s  (0.790s,  162.05/s)  LR: 3.960e-04  Data: 0.045 (0.043)
Train: 17 [ 300/391 ( 77%)]  Loss:  2.209373 (2.2029)  Time: 0.790s,  162.06/s  (0.790s,  162.05/s)  LR: 3.960e-04  Data: 0.043 (0.043)
Train: 17 [ 350/391 ( 90%)]  Loss:  2.144876 (2.1956)  Time: 0.789s,  162.26/s  (0.790s,  162.06/s)  LR: 3.960e-04  Data: 0.042 (0.043)
Train: 17 [ 390/391 (100%)]  Loss:  2.230847 (2.1982)  Time: 0.510s,  156.78/s  (0.789s,  101.38/s)  LR: 3.960e-04  Data: 0.030 (0.043)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.8057 (1.8057)  Acc@1: 36.0000 (36.0000)  Acc@5: 85.0000 (85.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.9268 (1.8657)  Acc@1: 32.0000 (32.9412)  Acc@5: 85.0000 (85.2157)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.8174 (1.8616)  Acc@1: 33.0000 (33.2400)  Acc@5: 88.0000 (85.3400)
tb
loss  0 : 2.00404296875 17
loss  1 : 1.855576171875 17
loss  2 : 1.8815234375 17
loss  3 : 1.89529296875 17
{'loss_different_0': 2.00404296875, 'loss_different_1': 1.855576171875, 'loss_different_2': 1.8815234375, 'loss_different_3': 1.89529296875}
Top1:  33.24
Train: 18 [   0/391 (  0%)]  Loss:  2.121989 (2.1220)  Time: 0.793s,  161.50/s  (0.793s,  161.50/s)  LR: 3.455e-04  Data: 0.046 (0.046)
Train: 18 [  50/391 ( 13%)]  Loss:  2.156551 (2.1393)  Time: 0.791s,  161.89/s  (0.790s,  161.98/s)  LR: 3.455e-04  Data: 0.044 (0.043)
Train: 18 [ 100/391 ( 26%)]  Loss:  2.217694 (2.1654)  Time: 0.788s,  162.43/s  (0.790s,  161.99/s)  LR: 3.455e-04  Data: 0.041 (0.043)
Train: 18 [ 150/391 ( 38%)]  Loss:  2.157624 (2.1635)  Time: 0.809s,  158.30/s  (0.790s,  161.95/s)  LR: 3.455e-04  Data: 0.061 (0.043)
Train: 18 [ 200/391 ( 51%)]  Loss:  2.214906 (2.1738)  Time: 0.790s,  162.08/s  (0.791s,  161.78/s)  LR: 3.455e-04  Data: 0.043 (0.043)
Train: 18 [ 250/391 ( 64%)]  Loss:  2.291581 (2.1934)  Time: 0.790s,  162.11/s  (0.791s,  161.81/s)  LR: 3.455e-04  Data: 0.042 (0.043)
Train: 18 [ 300/391 ( 77%)]  Loss:  2.195080 (2.1936)  Time: 0.790s,  162.11/s  (0.791s,  161.82/s)  LR: 3.455e-04  Data: 0.043 (0.043)
Train: 18 [ 350/391 ( 90%)]  Loss:  2.048057 (2.1754)  Time: 0.787s,  162.57/s  (0.791s,  161.86/s)  LR: 3.455e-04  Data: 0.041 (0.043)
Train: 18 [ 390/391 (100%)]  Loss:  2.118929 (2.1713)  Time: 0.509s,  157.13/s  (0.790s,  101.26/s)  LR: 3.455e-04  Data: 0.029 (0.043)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  1.7939 (1.7939)  Acc@1: 35.0000 (35.0000)  Acc@5: 86.0000 (86.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.9268 (1.8597)  Acc@1: 36.0000 (32.9804)  Acc@5: 84.0000 (85.1373)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.020 (0.021)  Loss:  1.8057 (1.8562)  Acc@1: 31.0000 (33.0800)  Acc@5: 89.0000 (85.4400)
tb
loss  0 : 1.98333984375 18
loss  1 : 1.857744140625 18
loss  2 : 1.873232421875 18
loss  3 : 1.8948046875 18
{'loss_different_0': 1.98333984375, 'loss_different_1': 1.857744140625, 'loss_different_2': 1.873232421875, 'loss_different_3': 1.8948046875}
Top1:  33.08
Train: 19 [   0/391 (  0%)]  Loss:  2.147040 (2.1470)  Time: 0.791s,  161.76/s  (0.791s,  161.76/s)  LR: 2.966e-04  Data: 0.045 (0.045)
Train: 19 [  50/391 ( 13%)]  Loss:  2.166368 (2.1567)  Time: 0.792s,  161.55/s  (0.790s,  161.98/s)  LR: 2.966e-04  Data: 0.045 (0.043)
Train: 19 [ 100/391 ( 26%)]  Loss:  2.377344 (2.2303)  Time: 0.788s,  162.36/s  (0.791s,  161.90/s)  LR: 2.966e-04  Data: 0.041 (0.044)
Train: 19 [ 150/391 ( 38%)]  Loss:  2.220537 (2.2278)  Time: 0.791s,  161.85/s  (0.791s,  161.85/s)  LR: 2.966e-04  Data: 0.044 (0.044)
Train: 19 [ 200/391 ( 51%)]  Loss:  2.124033 (2.2071)  Time: 0.791s,  161.80/s  (0.791s,  161.83/s)  LR: 2.966e-04  Data: 0.044 (0.044)
Train: 19 [ 250/391 ( 64%)]  Loss:  2.216179 (2.2086)  Time: 0.789s,  162.13/s  (0.791s,  161.85/s)  LR: 2.966e-04  Data: 0.043 (0.044)
Train: 19 [ 300/391 ( 77%)]  Loss:  2.090604 (2.1917)  Time: 0.790s,  161.93/s  (0.791s,  161.89/s)  LR: 2.966e-04  Data: 0.044 (0.044)
Train: 19 [ 350/391 ( 90%)]  Loss:  2.118941 (2.1826)  Time: 0.787s,  162.60/s  (0.791s,  161.90/s)  LR: 2.966e-04  Data: 0.041 (0.044)
Train: 19 [ 390/391 (100%)]  Loss:  2.142542 (2.1797)  Time: 0.509s,  157.07/s  (0.790s,  101.21/s)  LR: 2.966e-04  Data: 0.029 (0.044)
Test: [   0/99]  Time: 0.236 (0.236)  DataTime: 0.024 (0.024)  Loss:  1.8184 (1.8184)  Acc@1: 35.0000 (35.0000)  Acc@5: 87.0000 (87.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.021)  Loss:  1.9395 (1.8627)  Acc@1: 36.0000 (33.0784)  Acc@5: 86.0000 (85.6275)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  1.8086 (1.8604)  Acc@1: 31.0000 (33.0400)  Acc@5: 89.0000 (85.5700)
tb
loss  0 : 1.975205078125 19
loss  1 : 1.85978515625 19
loss  2 : 1.88140625 19
loss  3 : 1.90744140625 19
{'loss_different_0': 1.975205078125, 'loss_different_1': 1.85978515625, 'loss_different_2': 1.88140625, 'loss_different_3': 1.90744140625}
Top1:  33.04
Train: 20 [   0/391 (  0%)]  Loss:  2.243563 (2.2436)  Time: 0.792s,  161.52/s  (0.792s,  161.52/s)  LR: 2.500e-04  Data: 0.046 (0.046)
Train: 20 [  50/391 ( 13%)]  Loss:  2.125912 (2.1847)  Time: 0.787s,  162.58/s  (0.790s,  162.02/s)  LR: 2.500e-04  Data: 0.041 (0.043)
Train: 20 [ 100/391 ( 26%)]  Loss:  2.410796 (2.2601)  Time: 0.788s,  162.44/s  (0.790s,  162.08/s)  LR: 2.500e-04  Data: 0.041 (0.043)
Train: 20 [ 150/391 ( 38%)]  Loss:  2.252062 (2.2581)  Time: 0.790s,  162.09/s  (0.790s,  162.12/s)  LR: 2.500e-04  Data: 0.043 (0.043)
Train: 20 [ 200/391 ( 51%)]  Loss:  2.188280 (2.2441)  Time: 0.790s,  162.12/s  (0.790s,  162.11/s)  LR: 2.500e-04  Data: 0.043 (0.043)
Train: 20 [ 250/391 ( 64%)]  Loss:  2.150463 (2.2285)  Time: 0.787s,  162.60/s  (0.789s,  162.14/s)  LR: 2.500e-04  Data: 0.041 (0.043)
Train: 20 [ 300/391 ( 77%)]  Loss:  2.349733 (2.2458)  Time: 0.795s,  161.03/s  (0.790s,  162.12/s)  LR: 2.500e-04  Data: 0.048 (0.043)
Train: 20 [ 350/391 ( 90%)]  Loss:  2.166221 (2.2359)  Time: 0.787s,  162.60/s  (0.790s,  162.09/s)  LR: 2.500e-04  Data: 0.041 (0.043)
Train: 20 [ 390/391 (100%)]  Loss:  2.189096 (2.2325)  Time: 0.508s,  157.43/s  (0.789s,  101.40/s)  LR: 2.500e-04  Data: 0.028 (0.043)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.7910 (1.7910)  Acc@1: 38.0000 (38.0000)  Acc@5: 89.0000 (89.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.9316 (1.8579)  Acc@1: 34.0000 (32.8235)  Acc@5: 83.0000 (85.4118)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.7988 (1.8530)  Acc@1: 33.0000 (33.1800)  Acc@5: 89.0000 (85.4400)
tb
loss  0 : 1.970517578125 20
loss  1 : 1.85197265625 20
loss  2 : 1.869404296875 20
loss  3 : 1.8942578125 20
{'loss_different_0': 1.970517578125, 'loss_different_1': 1.85197265625, 'loss_different_2': 1.869404296875, 'loss_different_3': 1.8942578125}
Top1:  33.18
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_20.pt
Train: 21 [   0/391 (  0%)]  Loss:  2.089627 (2.0896)  Time: 0.788s,  162.34/s  (0.788s,  162.34/s)  LR: 2.061e-04  Data: 0.043 (0.043)
Train: 21 [  50/391 ( 13%)]  Loss:  2.225353 (2.1575)  Time: 0.789s,  162.27/s  (0.790s,  161.95/s)  LR: 2.061e-04  Data: 0.042 (0.043)
Train: 21 [ 100/391 ( 26%)]  Loss:  2.216224 (2.1771)  Time: 0.788s,  162.40/s  (0.790s,  162.04/s)  LR: 2.061e-04  Data: 0.041 (0.043)
Train: 21 [ 150/391 ( 38%)]  Loss:  2.197976 (2.1823)  Time: 0.790s,  161.94/s  (0.791s,  161.75/s)  LR: 2.061e-04  Data: 0.043 (0.043)
Train: 21 [ 200/391 ( 51%)]  Loss:  2.114367 (2.1687)  Time: 0.791s,  161.92/s  (0.791s,  161.85/s)  LR: 2.061e-04  Data: 0.043 (0.043)
Train: 21 [ 250/391 ( 64%)]  Loss:  2.148546 (2.1653)  Time: 0.790s,  162.02/s  (0.791s,  161.86/s)  LR: 2.061e-04  Data: 0.043 (0.043)
Train: 21 [ 300/391 ( 77%)]  Loss:  2.122696 (2.1593)  Time: 0.790s,  161.94/s  (0.791s,  161.90/s)  LR: 2.061e-04  Data: 0.044 (0.043)
Train: 21 [ 350/391 ( 90%)]  Loss:  2.211411 (2.1658)  Time: 0.790s,  162.09/s  (0.791s,  161.88/s)  LR: 2.061e-04  Data: 0.042 (0.043)
Train: 21 [ 390/391 (100%)]  Loss:  2.222825 (2.1699)  Time: 0.510s,  156.89/s  (0.790s,  101.25/s)  LR: 2.061e-04  Data: 0.030 (0.043)
Test: [   0/99]  Time: 0.238 (0.238)  DataTime: 0.026 (0.026)  Loss:  1.7930 (1.7930)  Acc@1: 40.0000 (40.0000)  Acc@5: 86.0000 (86.0000)
Test: [  50/99]  Time: 0.235 (0.234)  DataTime: 0.023 (0.022)  Loss:  1.9219 (1.8504)  Acc@1: 34.0000 (33.7255)  Acc@5: 84.0000 (85.3922)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.8008 (1.8469)  Acc@1: 33.0000 (33.5400)  Acc@5: 90.0000 (85.6100)
tb
loss  0 : 1.98634765625 21
loss  1 : 1.84453125 21
loss  2 : 1.86458984375 21
loss  3 : 1.879150390625 21
{'loss_different_0': 1.98634765625, 'loss_different_1': 1.84453125, 'loss_different_2': 1.86458984375, 'loss_different_3': 1.879150390625}
Top1:  33.54
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_21.pt
Train: 22 [   0/391 (  0%)]  Loss:  2.160933 (2.1609)  Time: 0.792s,  161.55/s  (0.792s,  161.55/s)  LR: 1.654e-04  Data: 0.046 (0.046)
Train: 22 [  50/391 ( 13%)]  Loss:  2.130459 (2.1457)  Time: 0.788s,  162.53/s  (0.789s,  162.17/s)  LR: 1.654e-04  Data: 0.040 (0.042)
Train: 22 [ 100/391 ( 26%)]  Loss:  2.107177 (2.1329)  Time: 0.791s,  161.74/s  (0.789s,  162.29/s)  LR: 1.654e-04  Data: 0.044 (0.042)
Train: 22 [ 150/391 ( 38%)]  Loss:  2.138584 (2.1343)  Time: 0.788s,  162.37/s  (0.788s,  162.34/s)  LR: 1.654e-04  Data: 0.042 (0.042)
Train: 22 [ 200/391 ( 51%)]  Loss:  2.108370 (2.1291)  Time: 0.789s,  162.31/s  (0.788s,  162.36/s)  LR: 1.654e-04  Data: 0.042 (0.042)
Train: 22 [ 250/391 ( 64%)]  Loss:  2.292338 (2.1563)  Time: 0.786s,  162.84/s  (0.788s,  162.37/s)  LR: 1.654e-04  Data: 0.040 (0.042)
Train: 22 [ 300/391 ( 77%)]  Loss:  2.204094 (2.1631)  Time: 0.790s,  161.95/s  (0.788s,  162.37/s)  LR: 1.654e-04  Data: 0.044 (0.042)
Train: 22 [ 350/391 ( 90%)]  Loss:  2.247487 (2.1737)  Time: 0.790s,  161.98/s  (0.788s,  162.38/s)  LR: 1.654e-04  Data: 0.042 (0.042)
Train: 22 [ 390/391 (100%)]  Loss:  2.052204 (2.1649)  Time: 0.508s,  157.50/s  (0.788s,  101.59/s)  LR: 1.654e-04  Data: 0.028 (0.041)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.7803 (1.7803)  Acc@1: 39.0000 (39.0000)  Acc@5: 86.0000 (86.0000)
Test: [  50/99]  Time: 0.234 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.9160 (1.8482)  Acc@1: 35.0000 (33.9216)  Acc@5: 86.0000 (85.6863)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.7920 (1.8445)  Acc@1: 30.0000 (33.6300)  Acc@5: 91.0000 (85.8100)
tb
loss  0 : 1.989609375 22
loss  1 : 1.836953125 22
loss  2 : 1.863603515625 22
loss  3 : 1.87662109375 22
{'loss_different_0': 1.989609375, 'loss_different_1': 1.836953125, 'loss_different_2': 1.863603515625, 'loss_different_3': 1.87662109375}
Top1:  33.63
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_22.pt
Train: 23 [   0/391 (  0%)]  Loss:  2.218365 (2.2184)  Time: 0.790s,  162.09/s  (0.790s,  162.09/s)  LR: 1.284e-04  Data: 0.043 (0.043)
Train: 23 [  50/391 ( 13%)]  Loss:  2.222277 (2.2203)  Time: 0.788s,  162.34/s  (0.789s,  162.26/s)  LR: 1.284e-04  Data: 0.042 (0.042)
Train: 23 [ 100/391 ( 26%)]  Loss:  2.223487 (2.2214)  Time: 0.791s,  161.89/s  (0.791s,  161.73/s)  LR: 1.284e-04  Data: 0.044 (0.042)
Train: 23 [ 150/391 ( 38%)]  Loss:  2.070305 (2.1836)  Time: 0.788s,  162.40/s  (0.790s,  161.93/s)  LR: 1.284e-04  Data: 0.042 (0.042)
Train: 23 [ 200/391 ( 51%)]  Loss:  2.211358 (2.1892)  Time: 0.791s,  161.81/s  (0.790s,  162.02/s)  LR: 1.284e-04  Data: 0.044 (0.042)
Train: 23 [ 250/391 ( 64%)]  Loss:  2.181713 (2.1879)  Time: 0.787s,  162.70/s  (0.790s,  162.09/s)  LR: 1.284e-04  Data: 0.040 (0.042)
Train: 23 [ 300/391 ( 77%)]  Loss:  2.232887 (2.1943)  Time: 0.788s,  162.45/s  (0.789s,  162.16/s)  LR: 1.284e-04  Data: 0.042 (0.042)
Train: 23 [ 350/391 ( 90%)]  Loss:  2.243143 (2.2004)  Time: 0.790s,  162.10/s  (0.789s,  162.20/s)  LR: 1.284e-04  Data: 0.043 (0.042)
Train: 23 [ 390/391 (100%)]  Loss:  2.243188 (2.2035)  Time: 0.510s,  156.72/s  (0.789s,  101.46/s)  LR: 1.284e-04  Data: 0.031 (0.042)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.7861 (1.7861)  Acc@1: 36.0000 (36.0000)  Acc@5: 88.0000 (88.0000)
Test: [  50/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  1.9189 (1.8468)  Acc@1: 36.0000 (33.6863)  Acc@5: 84.0000 (85.8824)
Test: [  99/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  1.8008 (1.8442)  Acc@1: 32.0000 (33.8600)  Acc@5: 90.0000 (85.8900)
tb
loss  0 : 1.98103515625 23
loss  1 : 1.840517578125 23
loss  2 : 1.861708984375 23
loss  3 : 1.876455078125 23
{'loss_different_0': 1.98103515625, 'loss_different_1': 1.840517578125, 'loss_different_2': 1.861708984375, 'loss_different_3': 1.876455078125}
Top1:  33.86
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_23.pt
Train: 24 [   0/391 (  0%)]  Loss:  2.286384 (2.2864)  Time: 0.789s,  162.21/s  (0.789s,  162.21/s)  LR: 9.549e-05  Data: 0.042 (0.042)
Train: 24 [  50/391 ( 13%)]  Loss:  2.150837 (2.2186)  Time: 0.786s,  162.87/s  (0.788s,  162.46/s)  LR: 9.549e-05  Data: 0.040 (0.041)
Train: 24 [ 100/391 ( 26%)]  Loss:  2.022190 (2.1531)  Time: 0.787s,  162.55/s  (0.788s,  162.52/s)  LR: 9.549e-05  Data: 0.041 (0.041)
Train: 24 [ 150/391 ( 38%)]  Loss:  2.074910 (2.1336)  Time: 0.786s,  162.88/s  (0.788s,  162.45/s)  LR: 9.549e-05  Data: 0.039 (0.042)
Train: 24 [ 200/391 ( 51%)]  Loss:  2.255606 (2.1580)  Time: 0.789s,  162.23/s  (0.788s,  162.40/s)  LR: 9.549e-05  Data: 0.042 (0.042)
Train: 24 [ 250/391 ( 64%)]  Loss:  2.170393 (2.1601)  Time: 0.790s,  162.05/s  (0.788s,  162.37/s)  LR: 9.549e-05  Data: 0.043 (0.042)
Train: 24 [ 300/391 ( 77%)]  Loss:  2.148652 (2.1584)  Time: 0.787s,  162.55/s  (0.788s,  162.34/s)  LR: 9.549e-05  Data: 0.040 (0.042)
Train: 24 [ 350/391 ( 90%)]  Loss:  2.253159 (2.1703)  Time: 0.789s,  162.24/s  (0.788s,  162.37/s)  LR: 9.549e-05  Data: 0.042 (0.042)
Train: 24 [ 390/391 (100%)]  Loss:  2.237908 (2.1752)  Time: 0.509s,  157.28/s  (0.788s,  101.58/s)  LR: 9.549e-05  Data: 0.029 (0.042)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  1.7803 (1.7803)  Acc@1: 38.0000 (38.0000)  Acc@5: 87.0000 (87.0000)
Test: [  50/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  1.9248 (1.8471)  Acc@1: 36.0000 (33.7647)  Acc@5: 85.0000 (85.7059)
Test: [  99/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  1.7881 (1.8439)  Acc@1: 30.0000 (33.7300)  Acc@5: 88.0000 (85.5300)
tb
loss  0 : 1.965576171875 24
loss  1 : 1.84583984375 24
loss  2 : 1.861552734375 24
loss  3 : 1.885849609375 24
{'loss_different_0': 1.965576171875, 'loss_different_1': 1.84583984375, 'loss_different_2': 1.861552734375, 'loss_different_3': 1.885849609375}
Top1:  33.73
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_24.pt
Train: 25 [   0/391 (  0%)]  Loss:  2.162678 (2.1627)  Time: 0.790s,  162.10/s  (0.790s,  162.10/s)  LR: 6.699e-05  Data: 0.042 (0.042)
Train: 25 [  50/391 ( 13%)]  Loss:  2.151500 (2.1571)  Time: 0.789s,  162.31/s  (0.794s,  161.19/s)  LR: 6.699e-05  Data: 0.042 (0.042)
Train: 25 [ 100/391 ( 26%)]  Loss:  2.103811 (2.1393)  Time: 0.787s,  162.56/s  (0.792s,  161.64/s)  LR: 6.699e-05  Data: 0.041 (0.042)
Train: 25 [ 150/391 ( 38%)]  Loss:  2.177985 (2.1490)  Time: 0.788s,  162.41/s  (0.791s,  161.82/s)  LR: 6.699e-05  Data: 0.042 (0.042)
Train: 25 [ 200/391 ( 51%)]  Loss:  2.317003 (2.1826)  Time: 0.789s,  162.17/s  (0.791s,  161.90/s)  LR: 6.699e-05  Data: 0.042 (0.042)
Train: 25 [ 250/391 ( 64%)]  Loss:  2.215351 (2.1881)  Time: 0.787s,  162.65/s  (0.790s,  161.95/s)  LR: 6.699e-05  Data: 0.041 (0.042)
Train: 25 [ 300/391 ( 77%)]  Loss:  2.109874 (2.1769)  Time: 0.788s,  162.46/s  (0.790s,  162.01/s)  LR: 6.699e-05  Data: 0.041 (0.042)
Train: 25 [ 350/391 ( 90%)]  Loss:  2.123337 (2.1702)  Time: 0.789s,  162.25/s  (0.790s,  162.03/s)  LR: 6.699e-05  Data: 0.042 (0.042)
Train: 25 [ 390/391 (100%)]  Loss:  2.176101 (2.1706)  Time: 0.510s,  156.83/s  (0.789s,  101.38/s)  LR: 6.699e-05  Data: 0.030 (0.042)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.7793 (1.7793)  Acc@1: 37.0000 (37.0000)  Acc@5: 88.0000 (88.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.9180 (1.8436)  Acc@1: 34.0000 (33.6863)  Acc@5: 86.0000 (86.0196)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  1.7920 (1.8406)  Acc@1: 30.0000 (33.7100)  Acc@5: 90.0000 (86.0100)
tb
loss  0 : 1.978525390625 25
loss  1 : 1.83578125 25
loss  2 : 1.859453125 25
loss  3 : 1.87341796875 25
{'loss_different_0': 1.978525390625, 'loss_different_1': 1.83578125, 'loss_different_2': 1.859453125, 'loss_different_3': 1.87341796875}
Top1:  33.71
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_25.pt
Train: 26 [   0/391 (  0%)]  Loss:  2.321247 (2.3212)  Time: 0.790s,  162.10/s  (0.790s,  162.10/s)  LR: 4.323e-05  Data: 0.043 (0.043)
Train: 26 [  50/391 ( 13%)]  Loss:  2.198874 (2.2601)  Time: 0.787s,  162.67/s  (0.789s,  162.30/s)  LR: 4.323e-05  Data: 0.040 (0.042)
Train: 26 [ 100/391 ( 26%)]  Loss:  2.219141 (2.2464)  Time: 0.789s,  162.32/s  (0.789s,  162.25/s)  LR: 4.323e-05  Data: 0.042 (0.042)
Train: 26 [ 150/391 ( 38%)]  Loss:  2.129824 (2.2173)  Time: 0.792s,  161.66/s  (0.789s,  162.14/s)  LR: 4.323e-05  Data: 0.044 (0.043)
Train: 26 [ 200/391 ( 51%)]  Loss:  2.134826 (2.2008)  Time: 0.788s,  162.39/s  (0.789s,  162.16/s)  LR: 4.323e-05  Data: 0.042 (0.043)
Train: 26 [ 250/391 ( 64%)]  Loss:  2.163531 (2.1946)  Time: 0.789s,  162.32/s  (0.789s,  162.23/s)  LR: 4.323e-05  Data: 0.042 (0.042)
Train: 26 [ 300/391 ( 77%)]  Loss:  2.163643 (2.1902)  Time: 0.787s,  162.62/s  (0.789s,  162.24/s)  LR: 4.323e-05  Data: 0.041 (0.042)
Train: 26 [ 350/391 ( 90%)]  Loss:  2.129687 (2.1826)  Time: 0.787s,  162.63/s  (0.789s,  162.15/s)  LR: 4.323e-05  Data: 0.040 (0.042)
Train: 26 [ 390/391 (100%)]  Loss:  2.128898 (2.1787)  Time: 0.510s,  156.85/s  (0.789s,  101.44/s)  LR: 4.323e-05  Data: 0.030 (0.042)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.7812 (1.7812)  Acc@1: 37.0000 (37.0000)  Acc@5: 86.0000 (86.0000)
Test: [  50/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  1.9053 (1.8434)  Acc@1: 33.0000 (33.4706)  Acc@5: 84.0000 (85.8235)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  1.7822 (1.8401)  Acc@1: 35.0000 (33.5600)  Acc@5: 90.0000 (85.9000)
tb
loss  0 : 1.981220703125 26
loss  1 : 1.83517578125 26
loss  2 : 1.85845703125 26
loss  3 : 1.86986328125 26
{'loss_different_0': 1.981220703125, 'loss_different_1': 1.83517578125, 'loss_different_2': 1.85845703125, 'loss_different_3': 1.86986328125}
Top1:  33.56
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_26.pt
Train: 27 [   0/391 (  0%)]  Loss:  2.073745 (2.0737)  Time: 0.790s,  162.13/s  (0.790s,  162.13/s)  LR: 2.447e-05  Data: 0.042 (0.042)
Train: 27 [  50/391 ( 13%)]  Loss:  2.210680 (2.1422)  Time: 0.786s,  162.78/s  (0.788s,  162.39/s)  LR: 2.447e-05  Data: 0.040 (0.042)
Train: 27 [ 100/391 ( 26%)]  Loss:  2.098380 (2.1276)  Time: 0.786s,  162.85/s  (0.788s,  162.36/s)  LR: 2.447e-05  Data: 0.039 (0.042)
Train: 27 [ 150/391 ( 38%)]  Loss:  2.128778 (2.1279)  Time: 0.789s,  162.29/s  (0.788s,  162.35/s)  LR: 2.447e-05  Data: 0.041 (0.042)
Train: 27 [ 200/391 ( 51%)]  Loss:  2.124126 (2.1271)  Time: 0.787s,  162.59/s  (0.788s,  162.34/s)  LR: 2.447e-05  Data: 0.041 (0.042)
Train: 27 [ 250/391 ( 64%)]  Loss:  2.064294 (2.1167)  Time: 0.796s,  160.77/s  (0.788s,  162.35/s)  LR: 2.447e-05  Data: 0.050 (0.042)
Train: 27 [ 300/391 ( 77%)]  Loss:  2.294766 (2.1421)  Time: 0.786s,  162.84/s  (0.788s,  162.37/s)  LR: 2.447e-05  Data: 0.040 (0.042)
Train: 27 [ 350/391 ( 90%)]  Loss:  2.181208 (2.1470)  Time: 0.786s,  162.75/s  (0.788s,  162.38/s)  LR: 2.447e-05  Data: 0.040 (0.042)
Train: 27 [ 390/391 (100%)]  Loss:  2.102960 (2.1438)  Time: 0.508s,  157.62/s  (0.788s,  101.59/s)  LR: 2.447e-05  Data: 0.028 (0.042)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  1.7754 (1.7754)  Acc@1: 39.0000 (39.0000)  Acc@5: 88.0000 (88.0000)
Test: [  50/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  1.9014 (1.8406)  Acc@1: 35.0000 (33.8431)  Acc@5: 85.0000 (86.0000)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.019 (0.021)  Loss:  1.7900 (1.8380)  Acc@1: 31.0000 (33.8700)  Acc@5: 90.0000 (85.9600)
tb
loss  0 : 1.976591796875 27
loss  1 : 1.83416015625 27
loss  2 : 1.857001953125 27
loss  3 : 1.8716796875 27
{'loss_different_0': 1.976591796875, 'loss_different_1': 1.83416015625, 'loss_different_2': 1.857001953125, 'loss_different_3': 1.8716796875}
Top1:  33.87
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_27.pt
Train: 28 [   0/391 (  0%)]  Loss:  2.081171 (2.0812)  Time: 0.791s,  161.76/s  (0.791s,  161.76/s)  LR: 1.093e-05  Data: 0.044 (0.044)
Train: 28 [  50/391 ( 13%)]  Loss:  2.163392 (2.1223)  Time: 0.788s,  162.48/s  (0.788s,  162.38/s)  LR: 1.093e-05  Data: 0.040 (0.042)
Train: 28 [ 100/391 ( 26%)]  Loss:  2.106066 (2.1169)  Time: 0.788s,  162.47/s  (0.788s,  162.42/s)  LR: 1.093e-05  Data: 0.041 (0.041)
Train: 28 [ 150/391 ( 38%)]  Loss:  2.283260 (2.1585)  Time: 0.788s,  162.44/s  (0.789s,  162.18/s)  LR: 1.093e-05  Data: 0.041 (0.041)
Train: 28 [ 200/391 ( 51%)]  Loss:  2.201244 (2.1670)  Time: 0.788s,  162.34/s  (0.789s,  162.23/s)  LR: 1.093e-05  Data: 0.041 (0.041)
Train: 28 [ 250/391 ( 64%)]  Loss:  2.154173 (2.1649)  Time: 0.788s,  162.42/s  (0.789s,  162.26/s)  LR: 1.093e-05  Data: 0.041 (0.042)
Train: 28 [ 300/391 ( 77%)]  Loss:  2.419583 (2.2013)  Time: 0.789s,  162.27/s  (0.789s,  162.29/s)  LR: 1.093e-05  Data: 0.042 (0.041)
Train: 28 [ 350/391 ( 90%)]  Loss:  2.120812 (2.1912)  Time: 0.789s,  162.26/s  (0.789s,  162.30/s)  LR: 1.093e-05  Data: 0.041 (0.042)
Train: 28 [ 390/391 (100%)]  Loss:  2.186739 (2.1909)  Time: 0.508s,  157.33/s  (0.788s,  101.54/s)  LR: 1.093e-05  Data: 0.029 (0.041)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.7764 (1.7764)  Acc@1: 38.0000 (38.0000)  Acc@5: 87.0000 (87.0000)
Test: [  50/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  1.9150 (1.8418)  Acc@1: 34.0000 (33.8627)  Acc@5: 84.0000 (85.9020)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  1.7891 (1.8388)  Acc@1: 31.0000 (33.9000)  Acc@5: 90.0000 (85.8700)
tb
loss  0 : 1.980654296875 28
loss  1 : 1.834599609375 28
loss  2 : 1.8586328125 28
loss  3 : 1.867265625 28
{'loss_different_0': 1.980654296875, 'loss_different_1': 1.834599609375, 'loss_different_2': 1.8586328125, 'loss_different_3': 1.867265625}
Top1:  33.9
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_28.pt
Train: 29 [   0/391 (  0%)]  Loss:  2.542102 (2.5421)  Time: 0.791s,  161.81/s  (0.791s,  161.81/s)  LR: 2.739e-06  Data: 0.044 (0.044)
Train: 29 [  50/391 ( 13%)]  Loss:  2.181946 (2.3620)  Time: 0.787s,  162.70/s  (0.788s,  162.46/s)  LR: 2.739e-06  Data: 0.040 (0.041)
Train: 29 [ 100/391 ( 26%)]  Loss:  2.179499 (2.3012)  Time: 0.789s,  162.18/s  (0.788s,  162.44/s)  LR: 2.739e-06  Data: 0.042 (0.041)
Train: 29 [ 150/391 ( 38%)]  Loss:  2.438572 (2.3355)  Time: 0.788s,  162.46/s  (0.788s,  162.43/s)  LR: 2.739e-06  Data: 0.041 (0.041)
Train: 29 [ 200/391 ( 51%)]  Loss:  2.163825 (2.3012)  Time: 0.788s,  162.52/s  (0.788s,  162.45/s)  LR: 2.739e-06  Data: 0.041 (0.041)
Train: 29 [ 250/391 ( 64%)]  Loss:  2.064713 (2.2618)  Time: 0.787s,  162.74/s  (0.788s,  162.46/s)  LR: 2.739e-06  Data: 0.040 (0.041)
Train: 29 [ 300/391 ( 77%)]  Loss:  2.089638 (2.2372)  Time: 0.788s,  162.49/s  (0.788s,  162.46/s)  LR: 2.739e-06  Data: 0.041 (0.041)
Train: 29 [ 350/391 ( 90%)]  Loss:  2.169105 (2.2287)  Time: 0.791s,  161.83/s  (0.789s,  162.29/s)  LR: 2.739e-06  Data: 0.044 (0.041)
Train: 29 [ 390/391 (100%)]  Loss:  2.139508 (2.2222)  Time: 0.508s,  157.45/s  (0.788s,  101.53/s)  LR: 2.739e-06  Data: 0.029 (0.041)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.021 (0.021)  Loss:  1.7754 (1.7754)  Acc@1: 39.0000 (39.0000)  Acc@5: 89.0000 (89.0000)
Test: [  50/99]  Time: 0.234 (0.233)  DataTime: 0.022 (0.021)  Loss:  1.9229 (1.8447)  Acc@1: 32.0000 (33.6471)  Acc@5: 82.0000 (85.6667)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  1.7871 (1.8414)  Acc@1: 33.0000 (33.7600)  Acc@5: 89.0000 (85.7000)
tb
loss  0 : 1.97755859375 29
loss  1 : 1.839970703125 29
loss  2 : 1.861474609375 29
loss  3 : 1.876259765625 29
{'loss_different_0': 1.97755859375, 'loss_different_1': 1.839970703125, 'loss_different_2': 1.861474609375, 'loss_different_3': 1.876259765625}
Top1:  33.76
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250831-043725-pretrain_cifar_10_lr_1-3/pretrain_cifar_10_lr_1-3_29.pt