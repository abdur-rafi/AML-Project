2025-08-28 23:43:37.014885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1756424617.246358     121 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1756424617.309543     121 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
create dataset:  cifar100
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Creating model..
, number of params: 27599012
SGD!!!
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Train: 0 [   0/390 (  0%)]  Loss:  4.622955 (4.6230)  Time: 3.492s,   36.66/s  (3.492s,   36.66/s)  LR: 1.000e-02  Data: 1.640 (1.640)
Train: 0 [  50/390 ( 13%)]  Loss:  4.581110 (4.6020)  Time: 0.747s,  171.26/s  (0.802s,  159.69/s)  LR: 1.000e-02  Data: 0.000 (0.033)
Train: 0 [ 100/390 ( 26%)]  Loss:  4.490434 (4.5648)  Time: 0.747s,  171.27/s  (0.775s,  165.21/s)  LR: 1.000e-02  Data: 0.000 (0.017)
Train: 0 [ 150/390 ( 39%)]  Loss:  4.396679 (4.5228)  Time: 0.748s,  171.17/s  (0.766s,  167.17/s)  LR: 1.000e-02  Data: 0.000 (0.011)
Train: 0 [ 200/390 ( 51%)]  Loss:  4.387924 (4.4958)  Time: 0.747s,  171.40/s  (0.763s,  167.76/s)  LR: 1.000e-02  Data: 0.000 (0.009)
Train: 0 [ 250/390 ( 64%)]  Loss:  4.327362 (4.4677)  Time: 0.747s,  171.37/s  (0.760s,  168.46/s)  LR: 1.000e-02  Data: 0.000 (0.007)
Train: 0 [ 300/390 ( 77%)]  Loss:  4.179970 (4.4266)  Time: 0.747s,  171.36/s  (0.758s,  168.92/s)  LR: 1.000e-02  Data: 0.000 (0.006)
Train: 0 [ 350/390 ( 90%)]  Loss:  4.131841 (4.3898)  Time: 0.750s,  170.63/s  (0.756s,  169.26/s)  LR: 1.000e-02  Data: 0.003 (0.005)
Train: 0 [ 389/390 (100%)]  Loss:  4.100733 (4.3577)  Time: 0.747s,  171.46/s  (0.755s,  169.47/s)  LR: 1.000e-02  Data: 0.000 (0.005)
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Test: [   0/78]  Time: 1.267 (1.267)  DataTime: 0.624 (0.624)  Loss:  3.6855 (3.6855)  Acc@1: 17.1875 (17.1875)  Acc@5: 46.0938 (46.0938)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.012)  Loss:  3.8418 (3.8055)  Acc@1: 12.5000 (11.4890)  Acc@5: 32.0312 (37.2855)
Test: [  78/78]  Time: 0.041 (0.268)  DataTime: 0.000 (0.008)  Loss:  3.8613 (3.8164)  Acc@1:  6.2500 (11.2100)  Acc@5: 31.2500 (36.5400)
tb
loss  0 : 4.5371625 0
loss  1 : 3.678915625 0
loss  2 : 3.94274375 0
loss  3 : 3.6709125 0
{'loss_different_0': 4.5371625, 'loss_different_1': 3.678915625, 'loss_different_2': 3.94274375, 'loss_different_3': 3.6709125}
Top1:  11.21
Saving model to ./output/train/20250828-234358-pretrain_cifar_100_using_resuming_cifar_10_lr_1-2/pretrain_cifar_100_using_resuming_cifar_10_lr_1-2_0.pt
Train: 1 [   0/390 (  0%)]  Loss:  4.093792 (4.0938)  Time: 1.538s,   83.24/s  (1.538s,   83.24/s)  LR: 9.999e-03  Data: 0.755 (0.755)
Train: 1 [  50/390 ( 13%)]  Loss:  3.891575 (3.9927)  Time: 0.747s,  171.30/s  (0.762s,  167.89/s)  LR: 9.999e-03  Data: 0.000 (0.015)
Train: 1 [ 100/390 ( 26%)]  Loss:  3.922310 (3.9692)  Time: 0.746s,  171.53/s  (0.755s,  169.61/s)  LR: 9.999e-03  Data: 0.000 (0.008)
Train: 1 [ 150/390 ( 39%)]  Loss:  3.855971 (3.9409)  Time: 0.748s,  171.20/s  (0.752s,  170.19/s)  LR: 9.999e-03  Data: 0.000 (0.005)
Train: 1 [ 200/390 ( 51%)]  Loss:  3.670193 (3.8868)  Time: 0.747s,  171.28/s  (0.751s,  170.48/s)  LR: 9.999e-03  Data: 0.000 (0.004)
Train: 1 [ 250/390 ( 64%)]  Loss:  3.732186 (3.8610)  Time: 0.747s,  171.42/s  (0.750s,  170.66/s)  LR: 9.999e-03  Data: 0.000 (0.003)
Train: 1 [ 300/390 ( 77%)]  Loss:  4.009867 (3.8823)  Time: 0.747s,  171.41/s  (0.750s,  170.78/s)  LR: 9.999e-03  Data: 0.000 (0.003)
Train: 1 [ 350/390 ( 90%)]  Loss:  3.470625 (3.8308)  Time: 0.749s,  170.84/s  (0.749s,  170.86/s)  LR: 9.999e-03  Data: 0.002 (0.003)
Train: 1 [ 389/390 (100%)]  Loss:  3.704270 (3.8168)  Time: 0.746s,  171.53/s  (0.749s,  170.92/s)  LR: 9.999e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.208 (1.208)  DataTime: 0.940 (0.940)  Loss:  3.0527 (3.0527)  Acc@1: 22.6562 (22.6562)  Acc@5: 56.2500 (56.2500)
Test: [  50/78]  Time: 0.258 (0.277)  DataTime: 0.000 (0.019)  Loss:  3.2656 (3.1893)  Acc@1: 21.8750 (19.2862)  Acc@5: 47.6562 (53.6765)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  3.1973 (3.1991)  Acc@1: 12.5000 (18.9800)  Acc@5: 50.0000 (53.1900)
tb
loss  0 : 4.12568125 1
loss  1 : 3.10966875 1
loss  2 : 3.524475 1
loss  3 : 3.153465625 1
{'loss_different_0': 4.12568125, 'loss_different_1': 3.10966875, 'loss_different_2': 3.524475, 'loss_different_3': 3.153465625}
Top1:  18.98
Train: 2 [   0/390 (  0%)]  Loss:  3.687454 (3.6875)  Time: 1.806s,   70.89/s  (1.806s,   70.89/s)  LR: 9.998e-03  Data: 1.059 (1.059)
Train: 2 [  50/390 ( 13%)]  Loss:  3.335110 (3.5113)  Time: 0.747s,  171.33/s  (0.773s,  165.54/s)  LR: 9.998e-03  Data: 0.000 (0.021)
Train: 2 [ 100/390 ( 26%)]  Loss:  3.610682 (3.5444)  Time: 0.746s,  171.50/s  (0.760s,  168.38/s)  LR: 9.998e-03  Data: 0.000 (0.011)
Train: 2 [ 150/390 ( 39%)]  Loss:  4.322811 (3.7390)  Time: 0.748s,  171.23/s  (0.756s,  169.37/s)  LR: 9.998e-03  Data: 0.000 (0.007)
Train: 2 [ 200/390 ( 51%)]  Loss:  3.333489 (3.6579)  Time: 0.746s,  171.57/s  (0.753s,  169.87/s)  LR: 9.998e-03  Data: 0.000 (0.006)
Train: 2 [ 250/390 ( 64%)]  Loss:  3.263932 (3.5922)  Time: 0.746s,  171.65/s  (0.752s,  170.17/s)  LR: 9.998e-03  Data: 0.000 (0.005)
Train: 2 [ 300/390 ( 77%)]  Loss:  3.206800 (3.5372)  Time: 0.747s,  171.46/s  (0.751s,  170.38/s)  LR: 9.998e-03  Data: 0.000 (0.004)
Train: 2 [ 350/390 ( 90%)]  Loss:  4.164314 (3.6156)  Time: 0.749s,  170.91/s  (0.751s,  170.52/s)  LR: 9.998e-03  Data: 0.003 (0.003)
Train: 2 [ 389/390 (100%)]  Loss:  4.134834 (3.6733)  Time: 0.747s,  171.46/s  (0.750s,  170.61/s)  LR: 9.998e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.127 (1.127)  DataTime: 0.850 (0.850)  Loss:  2.6484 (2.6484)  Acc@1: 28.9062 (28.9062)  Acc@5: 65.6250 (65.6250)
Test: [  50/78]  Time: 0.258 (0.276)  DataTime: 0.000 (0.017)  Loss:  2.9043 (2.8207)  Acc@1: 28.1250 (26.5165)  Acc@5: 59.3750 (62.0864)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.011)  Loss:  2.8965 (2.8285)  Acc@1: 25.0000 (26.2200)  Acc@5: 50.0000 (61.8200)
tb
loss  0 : 3.5774125 2
loss  1 : 2.80351875 2
loss  2 : 3.226721875 2
loss  3 : 2.8931625 2
{'loss_different_0': 3.5774125, 'loss_different_1': 2.80351875, 'loss_different_2': 3.226721875, 'loss_different_3': 2.8931625}
Top1:  26.22
Train: 3 [   0/390 (  0%)]  Loss:  3.303540 (3.3035)  Time: 1.675s,   76.42/s  (1.675s,   76.42/s)  LR: 9.994e-03  Data: 0.911 (0.911)
Train: 3 [  50/390 ( 13%)]  Loss:  3.092663 (3.1981)  Time: 0.747s,  171.45/s  (0.765s,  167.31/s)  LR: 9.994e-03  Data: 0.000 (0.018)
Train: 3 [ 100/390 ( 26%)]  Loss:  3.030136 (3.1421)  Time: 0.747s,  171.27/s  (0.756s,  169.25/s)  LR: 9.994e-03  Data: 0.000 (0.009)
Train: 3 [ 150/390 ( 39%)]  Loss:  3.219718 (3.1615)  Time: 0.747s,  171.36/s  (0.755s,  169.46/s)  LR: 9.994e-03  Data: 0.001 (0.006)
Train: 3 [ 200/390 ( 51%)]  Loss:  2.921192 (3.1134)  Time: 0.747s,  171.39/s  (0.753s,  169.93/s)  LR: 9.994e-03  Data: 0.000 (0.005)
Train: 3 [ 250/390 ( 64%)]  Loss:  2.979881 (3.0912)  Time: 0.747s,  171.27/s  (0.752s,  170.20/s)  LR: 9.994e-03  Data: 0.000 (0.004)
Train: 3 [ 300/390 ( 77%)]  Loss:  2.903360 (3.0644)  Time: 0.748s,  171.19/s  (0.751s,  170.38/s)  LR: 9.994e-03  Data: 0.000 (0.003)
Train: 3 [ 350/390 ( 90%)]  Loss:  4.272228 (3.2153)  Time: 0.749s,  170.78/s  (0.751s,  170.51/s)  LR: 9.994e-03  Data: 0.003 (0.003)
Train: 3 [ 389/390 (100%)]  Loss:  2.868780 (3.1768)  Time: 0.747s,  171.31/s  (0.750s,  170.59/s)  LR: 9.994e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.265 (1.265)  DataTime: 0.985 (0.985)  Loss:  2.4316 (2.4316)  Acc@1: 35.1562 (35.1562)  Acc@5: 67.9688 (67.9688)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.020)  Loss:  2.6289 (2.5487)  Acc@1: 34.3750 (31.6483)  Acc@5: 67.9688 (67.8615)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.013)  Loss:  2.5840 (2.5546)  Acc@1: 31.2500 (31.2100)  Acc@5: 62.5000 (67.9900)
tb
loss  0 : 3.257009375 3
loss  1 : 2.553034375 3
loss  2 : 3.026846875 3
loss  3 : 2.66500625 3
{'loss_different_0': 3.257009375, 'loss_different_1': 2.553034375, 'loss_different_2': 3.026846875, 'loss_different_3': 2.66500625}
Top1:  31.21
Train: 4 [   0/390 (  0%)]  Loss:  3.399994 (3.4000)  Time: 1.927s,   66.44/s  (1.927s,   66.44/s)  LR: 9.990e-03  Data: 1.178 (1.178)
Train: 4 [  50/390 ( 13%)]  Loss:  3.049347 (3.2247)  Time: 0.747s,  171.28/s  (0.770s,  166.16/s)  LR: 9.990e-03  Data: 0.000 (0.023)
Train: 4 [ 100/390 ( 26%)]  Loss:  2.878953 (3.1094)  Time: 0.748s,  171.20/s  (0.759s,  168.65/s)  LR: 9.990e-03  Data: 0.001 (0.012)
Train: 4 [ 150/390 ( 39%)]  Loss:  2.633453 (2.9904)  Time: 0.746s,  171.47/s  (0.755s,  169.53/s)  LR: 9.990e-03  Data: 0.000 (0.008)
Train: 4 [ 200/390 ( 51%)]  Loss:  4.266334 (3.2456)  Time: 0.747s,  171.38/s  (0.753s,  169.97/s)  LR: 9.990e-03  Data: 0.000 (0.006)
Train: 4 [ 250/390 ( 64%)]  Loss:  2.888023 (3.1860)  Time: 0.747s,  171.27/s  (0.752s,  170.24/s)  LR: 9.990e-03  Data: 0.000 (0.005)
Train: 4 [ 300/390 ( 77%)]  Loss:  2.774351 (3.1272)  Time: 0.748s,  171.23/s  (0.751s,  170.42/s)  LR: 9.990e-03  Data: 0.000 (0.004)
Train: 4 [ 350/390 ( 90%)]  Loss:  2.890126 (3.0976)  Time: 0.749s,  170.87/s  (0.751s,  170.54/s)  LR: 9.990e-03  Data: 0.003 (0.004)
Train: 4 [ 389/390 (100%)]  Loss:  2.876478 (3.0730)  Time: 0.746s,  171.67/s  (0.750s,  170.63/s)  LR: 9.990e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.084 (1.084)  DataTime: 0.813 (0.813)  Loss:  2.2559 (2.2559)  Acc@1: 36.7188 (36.7188)  Acc@5: 77.3438 (77.3438)
Test: [  50/78]  Time: 0.259 (0.275)  DataTime: 0.000 (0.016)  Loss:  2.4609 (2.3646)  Acc@1: 33.5938 (35.7996)  Acc@5: 70.3125 (71.7984)
Test: [  78/78]  Time: 0.034 (0.266)  DataTime: 0.000 (0.011)  Loss:  2.5645 (2.3679)  Acc@1: 37.5000 (35.6000)  Acc@5: 68.7500 (71.8200)
tb
loss  0 : 3.086959375 4
loss  1 : 2.37803125 4
loss  2 : 2.87443125 4
loss  3 : 2.540159375 4
{'loss_different_0': 3.086959375, 'loss_different_1': 2.37803125, 'loss_different_2': 2.87443125, 'loss_different_3': 2.540159375}
Top1:  35.6
Train: 5 [   0/390 (  0%)]  Loss:  2.923723 (2.9237)  Time: 1.686s,   75.91/s  (1.686s,   75.91/s)  LR: 9.985e-03  Data: 0.875 (0.875)
Train: 5 [  50/390 ( 13%)]  Loss:  2.841068 (2.8824)  Time: 0.747s,  171.41/s  (0.766s,  167.20/s)  LR: 9.985e-03  Data: 0.000 (0.018)
Train: 5 [ 100/390 ( 26%)]  Loss:  2.667061 (2.8106)  Time: 0.747s,  171.38/s  (0.759s,  168.66/s)  LR: 9.985e-03  Data: 0.000 (0.009)
Train: 5 [ 150/390 ( 39%)]  Loss:  2.728830 (2.7902)  Time: 0.747s,  171.30/s  (0.755s,  169.52/s)  LR: 9.985e-03  Data: 0.000 (0.006)
Train: 5 [ 200/390 ( 51%)]  Loss:  2.846137 (2.8014)  Time: 0.747s,  171.33/s  (0.753s,  169.96/s)  LR: 9.985e-03  Data: 0.000 (0.005)
Train: 5 [ 250/390 ( 64%)]  Loss:  3.887203 (2.9823)  Time: 0.747s,  171.45/s  (0.752s,  170.23/s)  LR: 9.985e-03  Data: 0.000 (0.004)
Train: 5 [ 300/390 ( 77%)]  Loss:  2.592782 (2.9267)  Time: 0.747s,  171.29/s  (0.751s,  170.41/s)  LR: 9.985e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.126 (1.126)  DataTime: 0.834 (0.834)  Loss:  1.6855 (1.6855)  Acc@1: 56.2500 (56.2500)  Acc@5: 84.3750 (84.3750)
Train: 12 [ 100/390 ( 26%)]  Loss:  2.253442 (2.2741)  Time: 0.747s,  171.27/s  (0.758s,  168.93/s)  LR: 9.911e-03  Data: 0.000 (0.011)
Train: 12 [ 150/390 ( 39%)]  Loss:  3.546309 (2.5921)  Time: 0.747s,  171.24/s  (0.754s,  169.72/s)  LR: 9.911e-03  Data: 0.000 (0.007)
Train: 12 [ 200/390 ( 51%)]  Loss:  2.096697 (2.4931)  Time: 0.747s,  171.42/s  (0.752s,  170.12/s)  LR: 9.911e-03  Data: 0.000 (0.006)
Train: 12 [ 250/390 ( 64%)]  Loss:  2.236943 (2.4504)  Time: 0.747s,  171.38/s  (0.751s,  170.36/s)  LR: 9.911e-03  Data: 0.000 (0.005)
Train: 12 [ 300/390 ( 77%)]  Loss:  2.178544 (2.4115)  Time: 0.747s,  171.38/s  (0.751s,  170.53/s)  LR: 9.911e-03  Data: 0.000 (0.004)
Train: 12 [ 350/390 ( 90%)]  Loss:  2.046077 (2.3659)  Time: 0.750s,  170.72/s  (0.751s,  170.43/s)  LR: 9.911e-03  Data: 0.003 (0.003)
Train: 12 [ 389/390 (100%)]  Loss:  2.302824 (2.3589)  Time: 0.746s,  171.48/s  (0.751s,  170.52/s)  LR: 9.911e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.042 (1.042)  DataTime: 0.704 (0.704)  Loss:  1.5645 (1.5645)  Acc@1: 56.2500 (56.2500)  Acc@5: 85.1562 (85.1562)
Test: [  50/78]  Time: 0.258 (0.274)  DataTime: 0.000 (0.014)  Loss:  1.7520 (1.6758)  Acc@1: 54.6875 (53.5386)  Acc@5: 79.6875 (83.5478)
Test: [  78/78]  Time: 0.034 (0.266)  DataTime: 0.000 (0.009)  Loss:  1.8691 (1.6852)  Acc@1: 50.0000 (53.2400)  Acc@5: 75.0000 (83.4800)
tb
loss  0 : 2.438959375 12
loss  1 : 1.8254734375 12
loss  2 : 2.277840625 12
loss  3 : 1.95985 12
{'loss_different_0': 2.438959375, 'loss_different_1': 1.8254734375, 'loss_different_2': 2.277840625, 'loss_different_3': 1.95985}
Top1:  53.24
Train: 13 [   0/390 (  0%)]  Loss:  2.467166 (2.4672)  Time: 1.477s,   86.64/s  (1.477s,   86.64/s)  LR: 9.896e-03  Data: 0.718 (0.718)
Train: 13 [  50/390 ( 13%)]  Loss:  2.186191 (2.3267)  Time: 0.747s,  171.41/s  (0.761s,  168.12/s)  LR: 9.896e-03  Data: 0.000 (0.014)
Train: 13 [ 100/390 ( 26%)]  Loss:  2.171366 (2.2749)  Time: 0.747s,  171.40/s  (0.754s,  169.71/s)  LR: 9.896e-03  Data: 0.000 (0.007)
Train: 13 [ 150/390 ( 39%)]  Loss:  2.082215 (2.2267)  Time: 0.747s,  171.33/s  (0.752s,  170.24/s)  LR: 9.896e-03  Data: 0.000 (0.005)
Train: 13 [ 200/390 ( 51%)]  Loss:  3.340500 (2.4495)  Time: 0.747s,  171.41/s  (0.751s,  170.50/s)  LR: 9.896e-03  Data: 0.000 (0.004)
Train: 13 [ 250/390 ( 64%)]  Loss:  2.415049 (2.4437)  Time: 0.747s,  171.35/s  (0.750s,  170.67/s)  LR: 9.896e-03  Data: 0.000 (0.003)
Train: 13 [ 300/390 ( 77%)]  Loss:  1.983498 (2.3780)  Time: 0.747s,  171.38/s  (0.749s,  170.78/s)  LR: 9.896e-03  Data: 0.000 (0.003)
Train: 13 [ 350/390 ( 90%)]  Loss:  1.914240 (2.3200)  Time: 0.750s,  170.74/s  (0.749s,  170.86/s)  LR: 9.896e-03  Data: 0.003 (0.002)
Train: 13 [ 389/390 (100%)]  Loss:  2.106266 (2.2963)  Time: 0.746s,  171.47/s  (0.749s,  170.91/s)  LR: 9.896e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.945 (0.945)  DataTime: 0.647 (0.647)  Loss:  1.5391 (1.5391)  Acc@1: 57.0312 (57.0312)  Acc@5: 81.2500 (81.2500)
Test: [  50/78]  Time: 0.258 (0.272)  DataTime: 0.000 (0.013)  Loss:  1.7119 (1.5814)  Acc@1: 53.1250 (55.8517)  Acc@5: 82.0312 (85.4167)
Test: [  78/78]  Time: 0.034 (0.264)  DataTime: 0.000 (0.008)  Loss:  1.8965 (1.5897)  Acc@1: 50.0000 (55.7000)  Acc@5: 81.2500 (85.0700)
tb
loss  0 : 2.284284375 13
loss  1 : 1.76855625 13
loss  2 : 2.23128125 13
loss  3 : 1.912934375 13
{'loss_different_0': 2.284284375, 'loss_different_1': 1.76855625, 'loss_different_2': 2.23128125, 'loss_different_3': 1.912934375}
Top1:  55.7
Train: 14 [   0/390 (  0%)]  Loss:  1.835718 (1.8357)  Time: 1.707s,   74.97/s  (1.707s,   74.97/s)  LR: 9.880e-03  Data: 0.932 (0.932)
Train: 14 [  50/390 ( 13%)]  Loss:  2.164783 (2.0003)  Time: 0.747s,  171.29/s  (0.766s,  167.18/s)  LR: 9.880e-03  Data: 0.000 (0.019)
Train: 14 [ 100/390 ( 26%)]  Loss:  1.766181 (1.9222)  Time: 0.747s,  171.28/s  (0.756s,  169.23/s)  LR: 9.880e-03  Data: 0.000 (0.010)
Train: 14 [ 150/390 ( 39%)]  Loss:  1.793448 (1.8900)  Time: 0.748s,  171.18/s  (0.755s,  169.45/s)  LR: 9.880e-03  Data: 0.000 (0.007)
Train: 14 [ 200/390 ( 51%)]  Loss:  2.474957 (2.0070)  Time: 0.747s,  171.32/s  (0.753s,  169.93/s)  LR: 9.880e-03  Data: 0.000 (0.005)
Train: 14 [ 250/390 ( 64%)]  Loss:  4.248034 (2.3805)  Time: 0.746s,  171.51/s  (0.752s,  170.22/s)  LR: 9.880e-03  Data: 0.000 (0.004)
Train: 14 [ 300/390 ( 77%)]  Loss:  1.825205 (2.3012)  Time: 0.747s,  171.35/s  (0.751s,  170.41/s)  LR: 9.880e-03  Data: 0.000 (0.003)
Train: 14 [ 350/390 ( 90%)]  Loss:  1.886843 (2.2494)  Time: 0.749s,  170.92/s  (0.751s,  170.55/s)  LR: 9.880e-03  Data: 0.003 (0.003)
Train: 14 [ 389/390 (100%)]  Loss:  2.058941 (2.2282)  Time: 0.747s,  171.46/s  (0.750s,  170.64/s)  LR: 9.880e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.264 (1.264)  DataTime: 0.979 (0.979)  Loss:  1.4570 (1.4570)  Acc@1: 60.9375 (60.9375)  Acc@5: 88.2812 (88.2812)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.019)  Loss:  1.6914 (1.5793)  Acc@1: 51.5625 (55.3615)  Acc@5: 82.8125 (85.7077)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.013)  Loss:  1.8193 (1.5808)  Acc@1: 50.0000 (55.4000)  Acc@5: 87.5000 (85.6000)
tb
loss  0 : 2.269025 14
loss  1 : 1.75706875 14
loss  2 : 2.14031875 14
loss  3 : 1.918046875 14
{'loss_different_0': 2.269025, 'loss_different_1': 1.75706875, 'loss_different_2': 2.14031875, 'loss_different_3': 1.918046875}
Top1:  55.4
Train: 15 [   0/390 (  0%)]  Loss:  1.870823 (1.8708)  Time: 1.784s,   71.74/s  (1.784s,   71.74/s)  LR: 9.862e-03  Data: 1.036 (1.036)
Train: 15 [  50/390 ( 13%)]  Loss:  2.569293 (2.2201)  Time: 0.746s,  171.50/s  (0.767s,  166.83/s)  LR: 9.862e-03  Data: 0.000 (0.021)
Train: 15 [ 100/390 ( 26%)]  Loss:  2.534785 (2.3250)  Time: 0.747s,  171.40/s  (0.757s,  169.06/s)  LR: 9.862e-03  Data: 0.000 (0.011)
Train: 18 [ 300/390 ( 77%)]  Loss:  1.614694 (2.1119)  Time: 0.748s,  171.20/s  (0.750s,  170.75/s)  LR: 9.801e-03  Data: 0.000 (0.002)
Train: 24 [ 100/390 ( 26%)]  Loss:  3.205104 (2.2870)  Time: 0.747s,  171.28/s  (0.759s,  168.69/s)  LR: 9.649e-03  Data: 0.000 (0.012)
Train: 33 [ 389/390 (100%)]  Loss:  1.607209 (1.8363)  Time: 0.747s,  171.40/s  (0.750s,  170.70/s)  LR: 9.343e-03  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.056 (1.056)  DataTime: 0.752 (0.752)  Loss:  0.9614 (0.9614)  Acc@1: 72.6562 (72.6562)  Acc@5: 90.6250 (90.6250)
Train: 39 [  50/390 ( 13%)]  Loss:  1.633772 (1.5086)  Time: 0.748s,  171.14/s  (0.769s,  166.52/s)  LR: 9.091e-03  Data: 0.000 (0.022)
Train: 39 [ 100/390 ( 26%)]  Loss:  1.856824 (1.6247)  Time: 0.747s,  171.33/s  (0.758s,  168.87/s)  LR: 9.091e-03  Data: 0.001 (0.011)
Train: 41 [ 200/390 ( 51%)]  Loss:  1.318897 (1.7028)  Time: 0.746s,  171.52/s  (0.751s,  170.35/s)  LR: 8.998e-03  Data: 0.000 (0.005)
Train: 41 [ 250/390 ( 64%)]  Loss:  2.082921 (1.7661)  Time: 0.747s,  171.43/s  (0.751s,  170.55/s)  LR: 8.998e-03  Data: 0.000 (0.004)
Train: 41 [ 300/390 ( 77%)]  Loss:  3.595453 (2.0275)  Time: 0.747s,  171.33/s  (0.750s,  170.68/s)  LR: 8.998e-03  Data: 0.000 (0.003)
Train: 41 [ 350/390 ( 90%)]  Loss:  1.467806 (1.9575)  Time: 0.749s,  170.83/s  (0.750s,  170.77/s)  LR: 8.998e-03  Data: 0.003 (0.003)
Train: 41 [ 389/390 (100%)]  Loss:  1.452983 (1.9015)  Time: 0.747s,  171.41/s  (0.749s,  170.84/s)  LR: 8.998e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.179 (1.179)  DataTime: 0.871 (0.871)  Loss:  1.0410 (1.0410)  Acc@1: 67.1875 (67.1875)  Acc@5: 92.9688 (92.9688)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.017)  Loss:  1.3174 (1.1155)  Acc@1: 61.7188 (67.8615)  Acc@5: 88.2812 (91.7433)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.011)  Loss:  1.2295 (1.1200)  Acc@1: 62.5000 (67.7600)  Acc@5: 93.7500 (91.6400)
tb
loss  0 : 1.8267171875 41
loss  1 : 1.4098671875 41
loss  2 : 1.66618125 41
loss  3 : 1.5147921875 41
{'loss_different_0': 1.8267171875, 'loss_different_1': 1.4098671875, 'loss_different_2': 1.66618125, 'loss_different_3': 1.5147921875}
Top1:  67.76
Train: 42 [   0/390 (  0%)]  Loss:  3.131425 (3.1314)  Time: 1.794s,   71.35/s  (1.794s,   71.35/s)  LR: 8.951e-03  Data: 1.044 (1.044)
Train: 42 [  50/390 ( 13%)]  Loss:  1.451311 (2.2914)  Time: 0.747s,  171.31/s  (0.768s,  166.77/s)  LR: 8.951e-03  Data: 0.000 (0.021)
Train: 42 [ 100/390 ( 26%)]  Loss:  1.310965 (1.9646)  Time: 0.748s,  171.18/s  (0.757s,  169.00/s)  LR: 8.951e-03  Data: 0.000 (0.011)
Train: 42 [ 150/390 ( 39%)]  Loss:  1.504710 (1.8496)  Time: 0.747s,  171.46/s  (0.756s,  169.27/s)  LR: 8.951e-03  Data: 0.000 (0.007)
Train: 42 [ 200/390 ( 51%)]  Loss:  1.431093 (1.7659)  Time: 0.747s,  171.43/s  (0.754s,  169.78/s)  LR: 8.951e-03  Data: 0.000 (0.006)
Train: 42 [ 250/390 ( 64%)]  Loss:  1.451095 (1.7134)  Time: 0.747s,  171.35/s  (0.753s,  170.09/s)  LR: 8.951e-03  Data: 0.000 (0.005)
Train: 42 [ 300/390 ( 77%)]  Loss:  3.234787 (1.9308)  Time: 0.747s,  171.30/s  (0.752s,  170.30/s)  LR: 8.951e-03  Data: 0.000 (0.004)
Train: 42 [ 350/390 ( 90%)]  Loss:  1.291947 (1.8509)  Time: 0.749s,  170.79/s  (0.751s,  170.45/s)  LR: 8.951e-03  Data: 0.003 (0.003)
Train: 42 [ 389/390 (100%)]  Loss:  1.497220 (1.8116)  Time: 0.747s,  171.43/s  (0.751s,  170.54/s)  LR: 8.951e-03  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.054 (1.054)  DataTime: 0.683 (0.683)  Loss:  1.0947 (1.0947)  Acc@1: 71.0938 (71.0938)  Acc@5: 91.4062 (91.4062)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.014)  Loss:  1.2520 (1.1472)  Acc@1: 63.2812 (67.1415)  Acc@5: 89.0625 (91.2071)
^C