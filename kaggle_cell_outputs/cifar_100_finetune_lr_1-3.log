2025-08-28 13:49:23.213403: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1756388963.236803    2277 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1756388963.244094    2277 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
create dataset:  cifar100
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Creating model..
, number of params: 27599012
SGD!!!
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Train: 0 [   0/390 (  0%)]  Loss:  4.622955 (4.6230)  Time: 2.895s,   44.21/s  (2.895s,   44.21/s)  LR: 1.000e-03  Data: 0.984 (0.984)
Train: 0 [  50/390 ( 13%)]  Loss:  4.608321 (4.6156)  Time: 0.748s,  171.20/s  (0.790s,  162.05/s)  LR: 1.000e-03  Data: 0.000 (0.020)
Train: 0 [ 100/390 ( 26%)]  Loss:  4.595556 (4.6089)  Time: 0.748s,  171.04/s  (0.769s,  166.46/s)  LR: 1.000e-03  Data: 0.000 (0.010)
Train: 0 [ 150/390 ( 39%)]  Loss:  4.597381 (4.6061)  Time: 0.747s,  171.35/s  (0.762s,  168.01/s)  LR: 1.000e-03  Data: 0.000 (0.007)
Train: 0 [ 200/390 ( 51%)]  Loss:  4.596780 (4.6042)  Time: 0.747s,  171.36/s  (0.758s,  168.80/s)  LR: 1.000e-03  Data: 0.000 (0.005)
Train: 0 [ 250/390 ( 64%)]  Loss:  4.589386 (4.6017)  Time: 0.748s,  171.14/s  (0.756s,  169.28/s)  LR: 1.000e-03  Data: 0.000 (0.004)
Train: 0 [ 300/390 ( 77%)]  Loss:  4.584917 (4.5993)  Time: 0.747s,  171.24/s  (0.755s,  169.60/s)  LR: 1.000e-03  Data: 0.000 (0.004)
Train: 0 [ 350/390 ( 90%)]  Loss:  4.587342 (4.5978)  Time: 0.751s,  170.40/s  (0.754s,  169.84/s)  LR: 1.000e-03  Data: 0.003 (0.003)
Train: 0 [ 389/390 (100%)]  Loss:  4.572584 (4.5950)  Time: 0.747s,  171.33/s  (0.753s,  169.99/s)  LR: 1.000e-03  Data: 0.000 (0.003)
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Test: [   0/78]  Time: 1.118 (1.118)  DataTime: 0.608 (0.608)  Loss:  4.5469 (4.5469)  Acc@1:  4.6875 ( 4.6875)  Acc@5: 12.5000 (12.5000)
Test: [  50/78]  Time: 0.258 (0.276)  DataTime: 0.000 (0.012)  Loss:  4.5469 (4.5485)  Acc@1:  4.6875 ( 4.0441)  Acc@5: 11.7188 (14.5221)
Test: [  78/78]  Time: 0.041 (0.267)  DataTime: 0.000 (0.008)  Loss:  4.5547 (4.5497)  Acc@1:  0.0000 ( 3.8500)  Acc@5: 18.7500 (14.0700)
tb
loss  0 : 4.60663125 0
loss  1 : 4.52300625 0
loss  2 : 4.54901875 0
loss  3 : 4.53446875 0
{'loss_different_0': 4.60663125, 'loss_different_1': 4.52300625, 'loss_different_2': 4.54901875, 'loss_different_3': 4.53446875}
Top1:  3.85
Saving model to ./output/train/20250828-134932-pretrain_cifar_100_using_resuming_cifar_10_lr_1-3/pretrain_cifar_100_using_resuming_cifar_10_lr_1-3_0.pt
Train: 1 [   0/390 (  0%)]  Loss:  4.573511 (4.5735)  Time: 1.640s,   78.03/s  (1.640s,   78.03/s)  LR: 9.999e-04  Data: 0.877 (0.877)
Train: 1 [  50/390 ( 13%)]  Loss:  4.568571 (4.5710)  Time: 0.747s,  171.26/s  (0.772s,  165.82/s)  LR: 9.999e-04  Data: 0.000 (0.018)
Train: 1 [ 100/390 ( 26%)]  Loss:  4.550232 (4.5641)  Time: 0.747s,  171.29/s  (0.760s,  168.47/s)  LR: 9.999e-04  Data: 0.000 (0.009)
Train: 1 [ 150/390 ( 39%)]  Loss:  4.559002 (4.5628)  Time: 0.747s,  171.36/s  (0.756s,  169.39/s)  LR: 9.999e-04  Data: 0.000 (0.006)
Train: 1 [ 200/390 ( 51%)]  Loss:  4.540468 (4.5584)  Time: 0.747s,  171.37/s  (0.754s,  169.86/s)  LR: 9.999e-04  Data: 0.000 (0.005)
Train: 1 [ 250/390 ( 64%)]  Loss:  4.519012 (4.5518)  Time: 0.748s,  171.21/s  (0.752s,  170.14/s)  LR: 9.999e-04  Data: 0.000 (0.004)
Train: 1 [ 300/390 ( 77%)]  Loss:  4.552652 (4.5519)  Time: 0.747s,  171.45/s  (0.751s,  170.33/s)  LR: 9.999e-04  Data: 0.000 (0.003)
Train: 1 [ 350/390 ( 90%)]  Loss:  4.502778 (4.5458)  Time: 0.749s,  170.88/s  (0.751s,  170.47/s)  LR: 9.999e-04  Data: 0.003 (0.003)
Train: 1 [ 389/390 (100%)]  Loss:  4.544006 (4.5456)  Time: 0.747s,  171.44/s  (0.750s,  170.56/s)  LR: 9.999e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.151 (1.151)  DataTime: 0.887 (0.887)  Loss:  4.4648 (4.4648)  Acc@1: 10.1562 (10.1562)  Acc@5: 22.6562 (22.6562)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.018)  Loss:  4.4766 (4.4736)  Acc@1:  4.6875 ( 7.0925)  Acc@5: 21.8750 (23.2230)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.011)  Loss:  4.4883 (4.4762)  Acc@1:  0.0000 ( 6.8100)  Acc@5: 18.7500 (22.6100)
tb
loss  0 : 4.60516875 1
loss  1 : 4.41551875 1
loss  2 : 4.47095 1
loss  3 : 4.43636875 1
{'loss_different_0': 4.60516875, 'loss_different_1': 4.41551875, 'loss_different_2': 4.47095, 'loss_different_3': 4.43636875}
Top1:  6.81
Train: 2 [   0/390 (  0%)]  Loss:  4.521753 (4.5218)  Time: 1.752s,   73.05/s  (1.752s,   73.05/s)  LR: 9.998e-04  Data: 0.999 (0.999)
Train: 2 [  50/390 ( 13%)]  Loss:  4.493098 (4.5074)  Time: 0.748s,  171.18/s  (0.767s,  166.92/s)  LR: 9.998e-04  Data: 0.000 (0.020)
Train: 2 [ 100/390 ( 26%)]  Loss:  4.494567 (4.5031)  Time: 0.747s,  171.42/s  (0.757s,  169.08/s)  LR: 9.998e-04  Data: 0.000 (0.010)
Train: 2 [ 150/390 ( 39%)]  Loss:  4.561537 (4.5177)  Time: 0.748s,  171.19/s  (0.754s,  169.81/s)  LR: 9.998e-04  Data: 0.000 (0.007)
Train: 2 [ 200/390 ( 51%)]  Loss:  4.482735 (4.5107)  Time: 0.747s,  171.35/s  (0.754s,  169.87/s)  LR: 9.998e-04  Data: 0.000 (0.005)
Train: 2 [ 250/390 ( 64%)]  Loss:  4.486069 (4.5066)  Time: 0.748s,  171.15/s  (0.752s,  170.15/s)  LR: 9.998e-04  Data: 0.000 (0.004)
Train: 2 [ 300/390 ( 77%)]  Loss:  4.481574 (4.5030)  Time: 0.747s,  171.26/s  (0.751s,  170.34/s)  LR: 9.998e-04  Data: 0.000 (0.004)
Train: 2 [ 350/390 ( 90%)]  Loss:  4.531441 (4.5066)  Time: 0.750s,  170.66/s  (0.751s,  170.48/s)  LR: 9.998e-04  Data: 0.003 (0.003)
Train: 2 [ 389/390 (100%)]  Loss:  4.518504 (4.5079)  Time: 0.747s,  171.41/s  (0.750s,  170.57/s)  LR: 9.998e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.250 (1.250)  DataTime: 0.950 (0.950)  Loss:  4.3750 (4.3750)  Acc@1: 10.9375 (10.9375)  Acc@5: 26.5625 (26.5625)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.019)  Loss:  4.4023 (4.3930)  Acc@1:  8.5938 ( 7.7206)  Acc@5: 24.2188 (27.4510)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  4.4297 (4.3971)  Acc@1:  0.0000 ( 7.4600)  Acc@5: 12.5000 (27.0400)
tb
loss  0 : 4.60315 2
loss  1 : 4.3066125 2
loss  2 : 4.39225 2
loss  3 : 4.33125 2
{'loss_different_0': 4.60315, 'loss_different_1': 4.3066125, 'loss_different_2': 4.39225, 'loss_different_3': 4.33125}
Top1:  7.46
Train: 3 [   0/390 (  0%)]  Loss:  4.452780 (4.4528)  Time: 1.265s,  101.15/s  (1.265s,  101.15/s)  LR: 9.994e-04  Data: 0.453 (0.453)
Train: 3 [  50/390 ( 13%)]  Loss:  4.477859 (4.4653)  Time: 0.747s,  171.29/s  (0.757s,  169.03/s)  LR: 9.994e-04  Data: 0.000 (0.009)
Train: 3 [ 100/390 ( 26%)]  Loss:  4.439069 (4.4566)  Time: 0.747s,  171.40/s  (0.752s,  170.15/s)  LR: 9.994e-04  Data: 0.000 (0.005)
Train: 3 [ 150/390 ( 39%)]  Loss:  4.433030 (4.4507)  Time: 0.747s,  171.32/s  (0.751s,  170.53/s)  LR: 9.994e-04  Data: 0.000 (0.003)
Train: 3 [ 200/390 ( 51%)]  Loss:  4.396637 (4.4399)  Time: 0.747s,  171.41/s  (0.750s,  170.72/s)  LR: 9.994e-04  Data: 0.000 (0.003)
Train: 3 [ 250/390 ( 64%)]  Loss:  4.455968 (4.4426)  Time: 0.747s,  171.36/s  (0.749s,  170.83/s)  LR: 9.994e-04  Data: 0.000 (0.002)
Train: 3 [ 300/390 ( 77%)]  Loss:  4.374515 (4.4328)  Time: 0.747s,  171.29/s  (0.749s,  170.90/s)  LR: 9.994e-04  Data: 0.000 (0.002)
Train: 3 [ 350/390 ( 90%)]  Loss:  4.502377 (4.4415)  Time: 0.749s,  170.85/s  (0.749s,  170.96/s)  LR: 9.994e-04  Data: 0.003 (0.002)
Train: 3 [ 389/390 (100%)]  Loss:  4.346573 (4.4310)  Time: 0.747s,  171.42/s  (0.749s,  171.00/s)  LR: 9.994e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.133 (1.133)  DataTime: 0.761 (0.761)  Loss:  4.2656 (4.2656)  Acc@1: 12.5000 (12.5000)  Acc@5: 35.1562 (35.1562)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.015)  Loss:  4.3086 (4.2992)  Acc@1: 10.1562 ( 8.2721)  Acc@5: 26.5625 (30.6066)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.010)  Loss:  4.3516 (4.3058)  Acc@1:  0.0000 ( 7.9100)  Acc@5:  6.2500 (29.7400)
tb
loss  0 : 4.60028125 3
loss  1 : 4.18966875 3
loss  2 : 4.30879375 3
loss  3 : 4.21083125 3
{'loss_different_0': 4.60028125, 'loss_different_1': 4.18966875, 'loss_different_2': 4.30879375, 'loss_different_3': 4.21083125}
Top1:  7.91
Train: 4 [   0/390 (  0%)]  Loss:  4.428426 (4.4284)  Time: 1.511s,   84.69/s  (1.511s,   84.69/s)  LR: 9.990e-04  Data: 0.743 (0.743)
Train: 4 [  50/390 ( 13%)]  Loss:  4.399226 (4.4138)  Time: 0.747s,  171.25/s  (0.762s,  167.96/s)  LR: 9.990e-04  Data: 0.000 (0.015)
Train: 4 [ 100/390 ( 26%)]  Loss:  4.366623 (4.3981)  Time: 0.747s,  171.31/s  (0.755s,  169.62/s)  LR: 9.990e-04  Data: 0.000 (0.008)
Train: 4 [ 150/390 ( 39%)]  Loss:  4.309332 (4.3759)  Time: 0.747s,  171.33/s  (0.753s,  169.88/s)  LR: 9.990e-04  Data: 0.000 (0.005)
Train: 4 [ 200/390 ( 51%)]  Loss:  4.469449 (4.3946)  Time: 0.747s,  171.34/s  (0.752s,  170.24/s)  LR: 9.990e-04  Data: 0.000 (0.004)
Train: 4 [ 250/390 ( 64%)]  Loss:  4.377642 (4.3918)  Time: 0.747s,  171.36/s  (0.751s,  170.45/s)  LR: 9.990e-04  Data: 0.001 (0.003)
Train: 4 [ 300/390 ( 77%)]  Loss:  4.302872 (4.3791)  Time: 0.747s,  171.32/s  (0.750s,  170.60/s)  LR: 9.990e-04  Data: 0.000 (0.003)
Train: 4 [ 350/390 ( 90%)]  Loss:  4.351427 (4.3756)  Time: 0.749s,  170.90/s  (0.750s,  170.70/s)  LR: 9.990e-04  Data: 0.003 (0.002)
Train: 4 [ 389/390 (100%)]  Loss:  4.330734 (4.3706)  Time: 0.747s,  171.38/s  (0.750s,  170.76/s)  LR: 9.990e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.384 (1.384)  DataTime: 1.116 (1.116)  Loss:  4.1680 (4.1680)  Acc@1: 10.1562 (10.1562)  Acc@5: 32.0312 (32.0312)
Test: [  50/78]  Time: 0.259 (0.281)  DataTime: 0.000 (0.022)  Loss:  4.2188 (4.2080)  Acc@1:  7.8125 ( 8.3793)  Acc@5: 27.3438 (31.0202)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.014)  Loss:  4.2734 (4.2149)  Acc@1:  0.0000 ( 8.0500)  Acc@5: 18.7500 (30.6800)
tb
loss  0 : 4.59489375 4
loss  1 : 4.084225 4
loss  2 : 4.22971875 4
loss  3 : 4.101925 4
{'loss_different_0': 4.59489375, 'loss_different_1': 4.084225, 'loss_different_2': 4.22971875, 'loss_different_3': 4.101925}
Top1:  8.05
Train: 5 [   0/390 (  0%)]  Loss:  4.410895 (4.4109)  Time: 1.173s,  109.12/s  (1.173s,  109.12/s)  LR: 9.985e-04  Data: 0.369 (0.369)
Train: 5 [  50/390 ( 13%)]  Loss:  4.336287 (4.3736)  Time: 0.747s,  171.32/s  (0.756s,  169.41/s)  LR: 9.985e-04  Data: 0.000 (0.008)
Train: 5 [ 100/390 ( 26%)]  Loss:  4.303455 (4.3502)  Time: 0.747s,  171.36/s  (0.751s,  170.36/s)  LR: 9.985e-04  Data: 0.000 (0.004)
Train: 5 [ 150/390 ( 39%)]  Loss:  4.282984 (4.3334)  Time: 0.747s,  171.36/s  (0.750s,  170.69/s)  LR: 9.985e-04  Data: 0.000 (0.003)
Train: 5 [ 200/390 ( 51%)]  Loss:  4.273688 (4.3215)  Time: 0.747s,  171.28/s  (0.749s,  170.84/s)  LR: 9.985e-04  Data: 0.001 (0.002)
Train: 5 [ 250/390 ( 64%)]  Loss:  4.434060 (4.3402)  Time: 0.747s,  171.39/s  (0.749s,  170.94/s)  LR: 9.985e-04  Data: 0.000 (0.002)
Train: 5 [ 300/390 ( 77%)]  Loss:  4.237640 (4.3256)  Time: 0.747s,  171.37/s  (0.750s,  170.78/s)  LR: 9.985e-04  Data: 0.000 (0.002)
Train: 5 [ 350/390 ( 90%)]  Loss:  4.313811 (4.3241)  Time: 0.749s,  170.82/s  (0.749s,  170.85/s)  LR: 9.985e-04  Data: 0.003 (0.001)
Train: 5 [ 389/390 (100%)]  Loss:  4.197383 (4.3100)  Time: 0.747s,  171.33/s  (0.749s,  170.90/s)  LR: 9.985e-04  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.088 (1.088)  DataTime: 0.824 (0.824)  Loss:  4.0664 (4.0664)  Acc@1: 12.5000 (12.5000)  Acc@5: 35.1562 (35.1562)
Test: [  50/78]  Time: 0.259 (0.275)  DataTime: 0.000 (0.017)  Loss:  4.1289 (4.1117)  Acc@1: 11.7188 ( 9.5895)  Acc@5: 27.3438 (32.3376)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.011)  Loss:  4.1914 (4.1207)  Acc@1:  0.0000 ( 9.2600)  Acc@5: 12.5000 (31.8200)
tb
loss  0 : 4.588925 5
loss  1 : 3.98315 5
loss  2 : 4.15665 5
loss  3 : 3.989025 5
{'loss_different_0': 4.588925, 'loss_different_1': 3.98315, 'loss_different_2': 4.15665, 'loss_different_3': 3.989025}
Top1:  9.26
Train: 6 [   0/390 (  0%)]  Loss:  4.313386 (4.3134)  Time: 1.882s,   68.03/s  (1.882s,   68.03/s)  LR: 9.978e-04  Data: 1.127 (1.127)
Train: 6 [  50/390 ( 13%)]  Loss:  4.251896 (4.2826)  Time: 0.747s,  171.29/s  (0.770s,  166.32/s)  LR: 9.978e-04  Data: 0.000 (0.022)
Train: 6 [ 100/390 ( 26%)]  Loss:  4.384110 (4.3165)  Time: 0.747s,  171.46/s  (0.759s,  168.75/s)  LR: 9.978e-04  Data: 0.000 (0.012)
Train: 6 [ 150/390 ( 39%)]  Loss:  4.322876 (4.3181)  Time: 0.747s,  171.27/s  (0.755s,  169.57/s)  LR: 9.978e-04  Data: 0.001 (0.008)
Train: 6 [ 200/390 ( 51%)]  Loss:  4.246296 (4.3037)  Time: 0.747s,  171.40/s  (0.753s,  170.00/s)  LR: 9.978e-04  Data: 0.000 (0.006)
Train: 6 [ 250/390 ( 64%)]  Loss:  4.417712 (4.3227)  Time: 0.748s,  171.22/s  (0.752s,  170.26/s)  LR: 9.978e-04  Data: 0.001 (0.005)
Train: 6 [ 300/390 ( 77%)]  Loss:  4.206425 (4.3061)  Time: 0.748s,  171.13/s  (0.751s,  170.43/s)  LR: 9.978e-04  Data: 0.000 (0.004)
Train: 6 [ 350/390 ( 90%)]  Loss:  4.142814 (4.2857)  Time: 0.749s,  170.99/s  (0.751s,  170.55/s)  LR: 9.978e-04  Data: 0.003 (0.004)
Train: 6 [ 389/390 (100%)]  Loss:  4.291194 (4.2863)  Time: 0.747s,  171.42/s  (0.750s,  170.64/s)  LR: 9.978e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.961 (0.961)  DataTime: 0.687 (0.687)  Loss:  3.9570 (3.9570)  Acc@1: 13.2812 (13.2812)  Acc@5: 34.3750 (34.3750)
Test: [  50/78]  Time: 0.258 (0.273)  DataTime: 0.000 (0.014)  Loss:  4.0547 (4.0228)  Acc@1:  9.3750 (10.1869)  Acc@5: 31.2500 (33.8848)
Test: [  78/78]  Time: 0.034 (0.265)  DataTime: 0.000 (0.009)  Loss:  4.1289 (4.0320)  Acc@1:  0.0000 ( 9.9400)  Acc@5: 18.7500 (33.2300)
tb
loss  0 : 4.57769375 6
loss  1 : 3.88844375 6
loss  2 : 4.08764375 6
loss  3 : 3.8961625 6
{'loss_different_0': 4.57769375, 'loss_different_1': 3.88844375, 'loss_different_2': 4.08764375, 'loss_different_3': 3.8961625}
Top1:  9.94
Train: 7 [   0/390 (  0%)]  Loss:  4.273105 (4.2731)  Time: 1.423s,   89.96/s  (1.423s,   89.96/s)  LR: 9.970e-04  Data: 0.637 (0.637)
Train: 7 [  50/390 ( 13%)]  Loss:  4.162564 (4.2178)  Time: 0.748s,  171.04/s  (0.761s,  168.26/s)  LR: 9.970e-04  Data: 0.000 (0.013)
Train: 7 [ 100/390 ( 26%)]  Loss:  4.199128 (4.2116)  Time: 0.747s,  171.36/s  (0.754s,  169.74/s)  LR: 9.970e-04  Data: 0.000 (0.007)
Train: 7 [ 150/390 ( 39%)]  Loss:  4.171646 (4.2016)  Time: 0.748s,  171.22/s  (0.752s,  170.24/s)  LR: 9.970e-04  Data: 0.000 (0.005)
Train: 7 [ 200/390 ( 51%)]  Loss:  4.110443 (4.1834)  Time: 0.748s,  171.16/s  (0.752s,  170.27/s)  LR: 9.970e-04  Data: 0.001 (0.004)
Train: 7 [ 250/390 ( 64%)]  Loss:  4.292279 (4.2015)  Time: 0.747s,  171.30/s  (0.751s,  170.47/s)  LR: 9.970e-04  Data: 0.000 (0.003)
Train: 7 [ 300/390 ( 77%)]  Loss:  4.189181 (4.1998)  Time: 0.748s,  171.16/s  (0.750s,  170.61/s)  LR: 9.970e-04  Data: 0.000 (0.002)
Train: 7 [ 350/390 ( 90%)]  Loss:  4.176663 (4.1969)  Time: 0.750s,  170.56/s  (0.750s,  170.70/s)  LR: 9.970e-04  Data: 0.003 (0.002)
Train: 7 [ 389/390 (100%)]  Loss:  4.122848 (4.1887)  Time: 0.747s,  171.34/s  (0.750s,  170.76/s)  LR: 9.970e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.187 (1.187)  DataTime: 0.896 (0.896)  Loss:  3.8535 (3.8535)  Acc@1: 11.7188 (11.7188)  Acc@5: 37.5000 (37.5000)
Test: [  50/78]  Time: 0.259 (0.277)  DataTime: 0.000 (0.018)  Loss:  3.9766 (3.9390)  Acc@1: 10.1562 (11.0754)  Acc@5: 31.2500 (35.9528)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.012)  Loss:  4.0312 (3.9484)  Acc@1:  6.2500 (10.7000)  Acc@5: 18.7500 (35.1600)
tb
loss  0 : 4.563475 7
loss  1 : 3.803740625 7
loss  2 : 4.03583125 7
loss  3 : 3.802859375 7
{'loss_different_0': 4.563475, 'loss_different_1': 3.803740625, 'loss_different_2': 4.03583125, 'loss_different_3': 3.802859375}
Top1:  10.7
Train: 8 [   0/390 (  0%)]  Loss:  4.172122 (4.1721)  Time: 1.659s,   77.14/s  (1.659s,   77.14/s)  LR: 9.961e-04  Data: 0.906 (0.906)
Train: 8 [  50/390 ( 13%)]  Loss:  4.173437 (4.1728)  Time: 0.747s,  171.31/s  (0.765s,  167.30/s)  LR: 9.961e-04  Data: 0.000 (0.018)
Train: 8 [ 100/390 ( 26%)]  Loss:  4.111572 (4.1524)  Time: 0.747s,  171.24/s  (0.756s,  169.25/s)  LR: 9.961e-04  Data: 0.000 (0.009)
Train: 8 [ 150/390 ( 39%)]  Loss:  4.366513 (4.2059)  Time: 0.747s,  171.24/s  (0.753s,  169.91/s)  LR: 9.961e-04  Data: 0.000 (0.006)
Train: 8 [ 200/390 ( 51%)]  Loss:  4.054902 (4.1757)  Time: 0.747s,  171.36/s  (0.752s,  170.25/s)  LR: 9.961e-04  Data: 0.000 (0.005)
Train: 8 [ 250/390 ( 64%)]  Loss:  4.028260 (4.1511)  Time: 0.748s,  171.18/s  (0.751s,  170.45/s)  LR: 9.961e-04  Data: 0.000 (0.004)
Train: 8 [ 300/390 ( 77%)]  Loss:  4.188448 (4.1565)  Time: 0.748s,  171.18/s  (0.750s,  170.59/s)  LR: 9.961e-04  Data: 0.000 (0.003)
Train: 8 [ 350/390 ( 90%)]  Loss:  4.073502 (4.1461)  Time: 0.752s,  170.23/s  (0.751s,  170.49/s)  LR: 9.961e-04  Data: 0.004 (0.003)
Train: 8 [ 389/390 (100%)]  Loss:  4.177218 (4.1496)  Time: 0.747s,  171.27/s  (0.750s,  170.57/s)  LR: 9.961e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.241 (1.241)  DataTime: 0.937 (0.937)  Loss:  3.7656 (3.7656)  Acc@1: 13.2812 (13.2812)  Acc@5: 39.8438 (39.8438)
Test: [  50/78]  Time: 0.259 (0.278)  DataTime: 0.000 (0.019)  Loss:  3.9062 (3.8564)  Acc@1: 12.5000 (11.1213)  Acc@5: 39.0625 (37.4387)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.012)  Loss:  3.9199 (3.8674)  Acc@1:  6.2500 (11.1300)  Acc@5: 18.7500 (36.5900)
tb
loss  0 : 4.53765625 8
loss  1 : 3.727928125 8
loss  2 : 3.98780625 8
loss  3 : 3.718490625 8
{'loss_different_0': 4.53765625, 'loss_different_1': 3.727928125, 'loss_different_2': 3.98780625, 'loss_different_3': 3.718490625}
Top1:  11.13
Train: 9 [   0/390 (  0%)]  Loss:  4.085330 (4.0853)  Time: 1.425s,   89.82/s  (1.425s,   89.82/s)  LR: 9.950e-04  Data: 0.634 (0.634)
Train: 9 [  50/390 ( 13%)]  Loss:  4.236949 (4.1611)  Time: 0.748s,  171.22/s  (0.760s,  168.32/s)  LR: 9.950e-04  Data: 0.000 (0.013)
Train: 9 [ 100/390 ( 26%)]  Loss:  4.074974 (4.1324)  Time: 0.747s,  171.35/s  (0.754s,  169.80/s)  LR: 9.950e-04  Data: 0.000 (0.007)
Train: 9 [ 150/390 ( 39%)]  Loss:  4.128372 (4.1314)  Time: 0.747s,  171.25/s  (0.752s,  170.30/s)  LR: 9.950e-04  Data: 0.000 (0.005)
Train: 9 [ 200/390 ( 51%)]  Loss:  4.371070 (4.1793)  Time: 0.748s,  171.20/s  (0.751s,  170.55/s)  LR: 9.950e-04  Data: 0.000 (0.003)
Train: 9 [ 250/390 ( 64%)]  Loss:  3.937439 (4.1390)  Time: 0.747s,  171.28/s  (0.750s,  170.71/s)  LR: 9.950e-04  Data: 0.000 (0.003)
Train: 9 [ 300/390 ( 77%)]  Loss:  3.979752 (4.1163)  Time: 0.747s,  171.29/s  (0.749s,  170.81/s)  LR: 9.950e-04  Data: 0.000 (0.002)
Train: 9 [ 350/390 ( 90%)]  Loss:  3.980156 (4.0993)  Time: 0.749s,  170.90/s  (0.749s,  170.89/s)  LR: 9.950e-04  Data: 0.003 (0.002)
Train: 9 [ 389/390 (100%)]  Loss:  3.901070 (4.0772)  Time: 0.746s,  171.48/s  (0.749s,  170.95/s)  LR: 9.950e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.203 (1.203)  DataTime: 0.906 (0.906)  Loss:  3.6855 (3.6855)  Acc@1: 14.8438 (14.8438)  Acc@5: 39.8438 (39.8438)
Test: [  50/78]  Time: 0.258 (0.277)  DataTime: 0.000 (0.018)  Loss:  3.8301 (3.7820)  Acc@1: 13.2812 (12.1936)  Acc@5: 37.5000 (39.0472)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.012)  Loss:  3.8652 (3.7931)  Acc@1:  6.2500 (11.9700)  Acc@5: 18.7500 (38.5200)
tb
loss  0 : 4.526975 9
loss  1 : 3.65036875 9
loss  2 : 3.93335 9
loss  3 : 3.64115625 9
{'loss_different_0': 4.526975, 'loss_different_1': 3.65036875, 'loss_different_2': 3.93335, 'loss_different_3': 3.64115625}
Top1:  11.97
Train: 10 [   0/390 (  0%)]  Loss:  3.923035 (3.9230)  Time: 1.369s,   93.50/s  (1.369s,   93.50/s)  LR: 9.938e-04  Data: 0.612 (0.612)
Train: 10 [  50/390 ( 13%)]  Loss:  4.185607 (4.0543)  Time: 0.747s,  171.38/s  (0.759s,  168.61/s)  LR: 9.938e-04  Data: 0.000 (0.012)
Train: 10 [ 100/390 ( 26%)]  Loss:  4.096675 (4.0684)  Time: 0.747s,  171.36/s  (0.753s,  169.98/s)  LR: 9.938e-04  Data: 0.000 (0.006)
Train: 10 [ 150/390 ( 39%)]  Loss:  3.969528 (4.0437)  Time: 0.746s,  171.48/s  (0.753s,  170.02/s)  LR: 9.938e-04  Data: 0.000 (0.004)
Train: 10 [ 200/390 ( 51%)]  Loss:  4.172053 (4.0694)  Time: 0.747s,  171.37/s  (0.751s,  170.33/s)  LR: 9.938e-04  Data: 0.000 (0.003)
Train: 10 [ 250/390 ( 64%)]  Loss:  3.928146 (4.0458)  Time: 0.747s,  171.40/s  (0.751s,  170.53/s)  LR: 9.938e-04  Data: 0.000 (0.003)
Train: 10 [ 300/390 ( 77%)]  Loss:  3.895665 (4.0244)  Time: 0.748s,  171.12/s  (0.750s,  170.66/s)  LR: 9.938e-04  Data: 0.000 (0.002)
Train: 10 [ 350/390 ( 90%)]  Loss:  4.024963 (4.0245)  Time: 0.750s,  170.77/s  (0.750s,  170.75/s)  LR: 9.938e-04  Data: 0.003 (0.002)
Train: 10 [ 389/390 (100%)]  Loss:  3.925513 (4.0135)  Time: 0.747s,  171.41/s  (0.749s,  170.81/s)  LR: 9.938e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.169 (1.169)  DataTime: 0.889 (0.889)  Loss:  3.5820 (3.5820)  Acc@1: 16.4062 (16.4062)  Acc@5: 46.8750 (46.8750)
Test: [  50/78]  Time: 0.258 (0.277)  DataTime: 0.000 (0.018)  Loss:  3.7578 (3.7006)  Acc@1: 15.6250 (13.1434)  Acc@5: 39.8438 (41.0080)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.012)  Loss:  3.7715 (3.7123)  Acc@1:  6.2500 (12.8500)  Acc@5: 31.2500 (40.5700)
tb
loss  0 : 4.49673125 10
loss  1 : 3.57106875 10
loss  2 : 3.884028125 10
loss  3 : 3.567403125 10
{'loss_different_0': 4.49673125, 'loss_different_1': 3.57106875, 'loss_different_2': 3.884028125, 'loss_different_3': 3.567403125}
Top1:  12.85
Saving model to ./output/train/20250828-134932-pretrain_cifar_100_using_resuming_cifar_10_lr_1-3/pretrain_cifar_100_using_resuming_cifar_10_lr_1-3_10.pt
Train: 11 [   0/390 (  0%)]  Loss:  3.995377 (3.9954)  Time: 1.838s,   69.62/s  (1.838s,   69.62/s)  LR: 9.926e-04  Data: 1.090 (1.090)
Train: 11 [  50/390 ( 13%)]  Loss:  3.956345 (3.9759)  Time: 0.748s,  171.20/s  (0.769s,  166.53/s)  LR: 9.926e-04  Data: 0.000 (0.022)
Train: 11 [ 100/390 ( 26%)]  Loss:  3.988448 (3.9801)  Time: 0.747s,  171.39/s  (0.758s,  168.86/s)  LR: 9.926e-04  Data: 0.000 (0.011)
Train: 11 [ 150/390 ( 39%)]  Loss:  3.918532 (3.9647)  Time: 0.747s,  171.44/s  (0.754s,  169.70/s)  LR: 9.926e-04  Data: 0.000 (0.008)
Train: 11 [ 200/390 ( 51%)]  Loss:  4.180111 (4.0078)  Time: 0.746s,  171.51/s  (0.752s,  170.12/s)  LR: 9.926e-04  Data: 0.000 (0.006)
Train: 11 [ 250/390 ( 64%)]  Loss:  4.162848 (4.0336)  Time: 0.747s,  171.41/s  (0.751s,  170.38/s)  LR: 9.926e-04  Data: 0.000 (0.005)
Train: 11 [ 300/390 ( 77%)]  Loss:  3.871193 (4.0104)  Time: 0.747s,  171.32/s  (0.751s,  170.55/s)  LR: 9.926e-04  Data: 0.000 (0.004)
Train: 11 [ 350/390 ( 90%)]  Loss:  3.876932 (3.9937)  Time: 0.749s,  170.91/s  (0.750s,  170.67/s)  LR: 9.926e-04  Data: 0.003 (0.003)
Train: 11 [ 389/390 (100%)]  Loss:  4.252456 (4.0225)  Time: 0.747s,  171.38/s  (0.750s,  170.75/s)  LR: 9.926e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.141 (1.141)  DataTime: 0.866 (0.866)  Loss:  3.5195 (3.5195)  Acc@1: 17.9688 (17.9688)  Acc@5: 45.3125 (45.3125)
Test: [  50/78]  Time: 0.258 (0.276)  DataTime: 0.000 (0.017)  Loss:  3.6855 (3.6234)  Acc@1: 14.0625 (14.5527)  Acc@5: 43.7500 (43.2751)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.011)  Loss:  3.6895 (3.6343)  Acc@1:  6.2500 (14.1400)  Acc@5: 31.2500 (42.7600)
tb
loss  0 : 4.4636125 11
loss  1 : 3.49713125 11
loss  2 : 3.818240625 11
loss  3 : 3.501371875 11
{'loss_different_0': 4.4636125, 'loss_different_1': 3.49713125, 'loss_different_2': 3.818240625, 'loss_different_3': 3.501371875}
Top1:  14.14
Train: 12 [   0/390 (  0%)]  Loss:  3.875757 (3.8758)  Time: 1.137s,  112.53/s  (1.137s,  112.53/s)  LR: 9.911e-04  Data: 0.347 (0.347)
Train: 12 [  50/390 ( 13%)]  Loss:  3.871136 (3.8734)  Time: 0.747s,  171.44/s  (0.758s,  168.81/s)  LR: 9.911e-04  Data: 0.000 (0.007)
Train: 12 [ 100/390 ( 26%)]  Loss:  3.860504 (3.8691)  Time: 0.747s,  171.42/s  (0.753s,  170.09/s)  LR: 9.911e-04  Data: 0.000 (0.004)
Train: 12 [ 150/390 ( 39%)]  Loss:  4.184223 (3.9479)  Time: 0.747s,  171.40/s  (0.751s,  170.52/s)  LR: 9.911e-04  Data: 0.000 (0.003)
Train: 12 [ 200/390 ( 51%)]  Loss:  3.798977 (3.9181)  Time: 0.747s,  171.38/s  (0.750s,  170.73/s)  LR: 9.911e-04  Data: 0.000 (0.002)
Train: 12 [ 250/390 ( 64%)]  Loss:  3.985625 (3.9294)  Time: 0.747s,  171.44/s  (0.749s,  170.86/s)  LR: 9.911e-04  Data: 0.000 (0.002)
Train: 12 [ 300/390 ( 77%)]  Loss:  3.928817 (3.9293)  Time: 0.747s,  171.38/s  (0.749s,  170.96/s)  LR: 9.911e-04  Data: 0.000 (0.001)
Train: 12 [ 350/390 ( 90%)]  Loss:  3.758591 (3.9080)  Time: 0.749s,  170.85/s  (0.748s,  171.02/s)  LR: 9.911e-04  Data: 0.003 (0.001)
Train: 12 [ 389/390 (100%)]  Loss:  3.782472 (3.8940)  Time: 0.747s,  171.42/s  (0.748s,  171.06/s)  LR: 9.911e-04  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.000 (1.000)  DataTime: 0.690 (0.690)  Loss:  3.4375 (3.4375)  Acc@1: 19.5312 (19.5312)  Acc@5: 48.4375 (48.4375)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.014)  Loss:  3.6270 (3.5566)  Acc@1: 14.0625 (15.1348)  Acc@5: 45.3125 (45.1440)
Test: [  78/78]  Time: 0.034 (0.265)  DataTime: 0.000 (0.009)  Loss:  3.6309 (3.5686)  Acc@1:  6.2500 (14.7900)  Acc@5: 37.5000 (44.6200)
tb
loss  0 : 4.42995 12
loss  1 : 3.43465 12
loss  2 : 3.7775125 12
loss  3 : 3.442425 12
{'loss_different_0': 4.42995, 'loss_different_1': 3.43465, 'loss_different_2': 3.7775125, 'loss_different_3': 3.442425}
Top1:  14.79
Train: 13 [   0/390 (  0%)]  Loss:  3.897583 (3.8976)  Time: 1.782s,   71.82/s  (1.782s,   71.82/s)  LR: 9.896e-04  Data: 1.034 (1.034)
Train: 13 [  50/390 ( 13%)]  Loss:  3.864499 (3.8810)  Time: 0.747s,  171.29/s  (0.767s,  166.86/s)  LR: 9.896e-04  Data: 0.000 (0.021)
Train: 13 [ 100/390 ( 26%)]  Loss:  3.864611 (3.8756)  Time: 0.747s,  171.42/s  (0.757s,  169.09/s)  LR: 9.896e-04  Data: 0.000 (0.011)
Train: 13 [ 150/390 ( 39%)]  Loss:  3.767121 (3.8485)  Time: 0.747s,  171.30/s  (0.754s,  169.85/s)  LR: 9.896e-04  Data: 0.000 (0.007)
Train: 13 [ 200/390 ( 51%)]  Loss:  4.138919 (3.9065)  Time: 0.747s,  171.39/s  (0.753s,  169.92/s)  LR: 9.896e-04  Data: 0.000 (0.005)
Train: 13 [ 250/390 ( 64%)]  Loss:  3.949763 (3.9137)  Time: 0.746s,  171.52/s  (0.752s,  170.22/s)  LR: 9.896e-04  Data: 0.000 (0.004)
Train: 13 [ 300/390 ( 77%)]  Loss:  3.795074 (3.8968)  Time: 0.748s,  171.20/s  (0.751s,  170.41/s)  LR: 9.896e-04  Data: 0.000 (0.004)
Train: 13 [ 350/390 ( 90%)]  Loss:  3.745575 (3.8779)  Time: 0.749s,  170.94/s  (0.751s,  170.55/s)  LR: 9.896e-04  Data: 0.002 (0.003)
Train: 13 [ 389/390 (100%)]  Loss:  3.827744 (3.8723)  Time: 0.746s,  171.59/s  (0.750s,  170.64/s)  LR: 9.896e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.213 (1.213)  DataTime: 0.916 (0.916)  Loss:  3.3672 (3.3672)  Acc@1: 16.4062 (16.4062)  Acc@5: 47.6562 (47.6562)
Test: [  50/78]  Time: 0.258 (0.277)  DataTime: 0.000 (0.018)  Loss:  3.5605 (3.4869)  Acc@1: 16.4062 (15.8701)  Acc@5: 44.5312 (46.8444)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.012)  Loss:  3.5234 (3.4984)  Acc@1: 12.5000 (15.4700)  Acc@5: 37.5000 (46.4000)
tb
loss  0 : 4.38333125 13
loss  1 : 3.367109375 13
loss  2 : 3.7312875 13
loss  3 : 3.381890625 13
{'loss_different_0': 4.38333125, 'loss_different_1': 3.367109375, 'loss_different_2': 3.7312875, 'loss_different_3': 3.381890625}
Top1:  15.47
Train: 14 [   0/390 (  0%)]  Loss:  3.755297 (3.7553)  Time: 1.842s,   69.48/s  (1.842s,   69.48/s)  LR: 9.880e-04  Data: 1.094 (1.094)
Train: 14 [  50/390 ( 13%)]  Loss:  3.793694 (3.7745)  Time: 0.747s,  171.42/s  (0.768s,  166.58/s)  LR: 9.880e-04  Data: 0.000 (0.022)
Train: 14 [ 100/390 ( 26%)]  Loss:  3.614684 (3.7212)  Time: 0.746s,  171.56/s  (0.758s,  168.93/s)  LR: 9.880e-04  Data: 0.000 (0.011)
Train: 14 [ 150/390 ( 39%)]  Loss:  3.745584 (3.7273)  Time: 0.746s,  171.56/s  (0.754s,  169.74/s)  LR: 9.880e-04  Data: 0.000 (0.008)
Train: 14 [ 200/390 ( 51%)]  Loss:  3.860432 (3.7539)  Time: 0.746s,  171.53/s  (0.752s,  170.15/s)  LR: 9.880e-04  Data: 0.000 (0.006)
Train: 14 [ 250/390 ( 64%)]  Loss:  4.336932 (3.8511)  Time: 0.746s,  171.49/s  (0.751s,  170.40/s)  LR: 9.880e-04  Data: 0.000 (0.005)
Train: 14 [ 300/390 ( 77%)]  Loss:  3.723618 (3.8329)  Time: 0.746s,  171.53/s  (0.750s,  170.57/s)  LR: 9.880e-04  Data: 0.000 (0.004)
Train: 14 [ 350/390 ( 90%)]  Loss:  3.625094 (3.8069)  Time: 0.749s,  170.95/s  (0.750s,  170.68/s)  LR: 9.880e-04  Data: 0.003 (0.003)
Train: 14 [ 389/390 (100%)]  Loss:  3.776053 (3.8035)  Time: 0.747s,  171.42/s  (0.750s,  170.76/s)  LR: 9.880e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.835 (0.835)  DataTime: 0.569 (0.569)  Loss:  3.3145 (3.3145)  Acc@1: 19.5312 (19.5312)  Acc@5: 50.0000 (50.0000)
Test: [  50/78]  Time: 0.258 (0.270)  DataTime: 0.000 (0.011)  Loss:  3.5039 (3.4234)  Acc@1: 14.0625 (17.0343)  Acc@5: 46.0938 (48.1158)
Test: [  78/78]  Time: 0.034 (0.263)  DataTime: 0.000 (0.007)  Loss:  3.4512 (3.4351)  Acc@1: 12.5000 (16.6300)  Acc@5: 37.5000 (47.9000)
tb
loss  0 : 4.35241875 14
loss  1 : 3.304371875 14
loss  2 : 3.682240625 14
loss  3 : 3.3312625 14
{'loss_different_0': 4.35241875, 'loss_different_1': 3.304371875, 'loss_different_2': 3.682240625, 'loss_different_3': 3.3312625}
Top1:  16.63
Train: 15 [   0/390 (  0%)]  Loss:  3.803949 (3.8039)  Time: 1.346s,   95.07/s  (1.346s,   95.07/s)  LR: 9.862e-04  Data: 0.546 (0.546)
Train: 15 [  50/390 ( 13%)]  Loss:  3.761885 (3.7829)  Time: 0.747s,  171.38/s  (0.759s,  168.68/s)  LR: 9.862e-04  Data: 0.000 (0.011)
Train: 15 [ 100/390 ( 26%)]  Loss:  3.804023 (3.7900)  Time: 0.747s,  171.30/s  (0.755s,  169.61/s)  LR: 9.862e-04  Data: 0.000 (0.006)
Train: 15 [ 150/390 ( 39%)]  Loss:  3.674278 (3.7610)  Time: 0.747s,  171.24/s  (0.752s,  170.18/s)  LR: 9.862e-04  Data: 0.000 (0.004)
Train: 15 [ 200/390 ( 51%)]  Loss:  4.274524 (3.8637)  Time: 0.746s,  171.49/s  (0.751s,  170.49/s)  LR: 9.862e-04  Data: 0.000 (0.003)
Train: 15 [ 250/390 ( 64%)]  Loss:  3.811133 (3.8550)  Time: 0.747s,  171.43/s  (0.750s,  170.66/s)  LR: 9.862e-04  Data: 0.000 (0.002)
Train: 15 [ 300/390 ( 77%)]  Loss:  3.732751 (3.8375)  Time: 0.747s,  171.32/s  (0.749s,  170.78/s)  LR: 9.862e-04  Data: 0.000 (0.002)
Train: 15 [ 350/390 ( 90%)]  Loss:  3.682907 (3.8182)  Time: 0.749s,  170.87/s  (0.749s,  170.86/s)  LR: 9.862e-04  Data: 0.003 (0.002)
Train: 15 [ 389/390 (100%)]  Loss:  3.635899 (3.7979)  Time: 0.746s,  171.56/s  (0.749s,  170.92/s)  LR: 9.862e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.261 (1.261)  DataTime: 0.994 (0.994)  Loss:  3.2539 (3.2539)  Acc@1: 20.3125 (20.3125)  Acc@5: 53.1250 (53.1250)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.020)  Loss:  3.4395 (3.3588)  Acc@1: 17.1875 (17.8615)  Acc@5: 46.0938 (50.0153)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.013)  Loss:  3.3906 (3.3719)  Acc@1: 12.5000 (17.4300)  Acc@5: 37.5000 (49.7700)
tb
loss  0 : 4.28655 15
loss  1 : 3.257171875 15
loss  2 : 3.65416875 15
loss  3 : 3.276309375 15
{'loss_different_0': 4.28655, 'loss_different_1': 3.257171875, 'loss_different_2': 3.65416875, 'loss_different_3': 3.276309375}
Top1:  17.43
Train: 16 [   0/390 (  0%)]  Loss:  3.752838 (3.7528)  Time: 1.856s,   68.96/s  (1.856s,   68.96/s)  LR: 9.843e-04  Data: 1.107 (1.107)
Train: 16 [  50/390 ( 13%)]  Loss:  3.636691 (3.6948)  Time: 0.747s,  171.39/s  (0.769s,  166.53/s)  LR: 9.843e-04  Data: 0.000 (0.022)
Train: 16 [ 100/390 ( 26%)]  Loss:  3.726813 (3.7054)  Time: 0.746s,  171.51/s  (0.758s,  168.91/s)  LR: 9.843e-04  Data: 0.000 (0.011)
Train: 16 [ 150/390 ( 39%)]  Loss:  3.679482 (3.6990)  Time: 0.747s,  171.41/s  (0.754s,  169.73/s)  LR: 9.843e-04  Data: 0.000 (0.008)
Train: 16 [ 200/390 ( 51%)]  Loss:  3.717767 (3.7027)  Time: 0.747s,  171.40/s  (0.752s,  170.14/s)  LR: 9.843e-04  Data: 0.000 (0.006)
Train: 16 [ 250/390 ( 64%)]  Loss:  3.682626 (3.6994)  Time: 0.748s,  171.23/s  (0.752s,  170.14/s)  LR: 9.843e-04  Data: 0.000 (0.005)
Train: 16 [ 300/390 ( 77%)]  Loss:  3.700713 (3.6996)  Time: 0.747s,  171.35/s  (0.751s,  170.35/s)  LR: 9.843e-04  Data: 0.000 (0.004)
Train: 16 [ 350/390 ( 90%)]  Loss:  3.961873 (3.7324)  Time: 0.751s,  170.47/s  (0.751s,  170.49/s)  LR: 9.843e-04  Data: 0.003 (0.003)
Train: 16 [ 389/390 (100%)]  Loss:  3.606395 (3.7184)  Time: 0.747s,  171.36/s  (0.750s,  170.58/s)  LR: 9.843e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.222 (1.222)  DataTime: 0.867 (0.867)  Loss:  3.1719 (3.1719)  Acc@1: 19.5312 (19.5312)  Acc@5: 55.4688 (55.4688)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.017)  Loss:  3.3730 (3.2972)  Acc@1: 21.8750 (18.8266)  Acc@5: 47.6562 (51.3480)
Test: [  78/78]  Time: 0.035 (0.268)  DataTime: 0.000 (0.011)  Loss:  3.3340 (3.3092)  Acc@1: 12.5000 (18.4400)  Acc@5: 31.2500 (51.1100)
tb
loss  0 : 4.24239375 16
loss  1 : 3.188346875 16
loss  2 : 3.609528125 16
loss  3 : 3.223803125 16
{'loss_different_0': 4.24239375, 'loss_different_1': 3.188346875, 'loss_different_2': 3.609528125, 'loss_different_3': 3.223803125}
Top1:  18.44
Train: 17 [   0/390 (  0%)]  Loss:  3.549278 (3.5493)  Time: 1.356s,   94.39/s  (1.356s,   94.39/s)  LR: 9.823e-04  Data: 0.569 (0.569)
Train: 17 [  50/390 ( 13%)]  Loss:  3.856681 (3.7030)  Time: 0.747s,  171.24/s  (0.759s,  168.63/s)  LR: 9.823e-04  Data: 0.000 (0.011)
Train: 17 [ 100/390 ( 26%)]  Loss:  3.617809 (3.6746)  Time: 0.747s,  171.35/s  (0.753s,  169.97/s)  LR: 9.823e-04  Data: 0.000 (0.006)
Train: 17 [ 150/390 ( 39%)]  Loss:  4.320833 (3.8362)  Time: 0.747s,  171.42/s  (0.751s,  170.42/s)  LR: 9.823e-04  Data: 0.000 (0.004)
Train: 17 [ 200/390 ( 51%)]  Loss:  3.634087 (3.7957)  Time: 0.748s,  171.20/s  (0.750s,  170.64/s)  LR: 9.823e-04  Data: 0.000 (0.003)
Train: 17 [ 250/390 ( 64%)]  Loss:  3.580033 (3.7598)  Time: 0.747s,  171.36/s  (0.749s,  170.79/s)  LR: 9.823e-04  Data: 0.000 (0.003)
Train: 17 [ 300/390 ( 77%)]  Loss:  3.470383 (3.7184)  Time: 0.747s,  171.35/s  (0.749s,  170.88/s)  LR: 9.823e-04  Data: 0.000 (0.002)
Train: 17 [ 350/390 ( 90%)]  Loss:  3.519509 (3.6936)  Time: 0.749s,  170.94/s  (0.749s,  170.94/s)  LR: 9.823e-04  Data: 0.003 (0.002)
Train: 17 [ 389/390 (100%)]  Loss:  3.617420 (3.6851)  Time: 0.748s,  171.21/s  (0.749s,  170.99/s)  LR: 9.823e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.871 (0.871)  DataTime: 0.609 (0.609)  Loss:  3.1270 (3.1270)  Acc@1: 22.6562 (22.6562)  Acc@5: 53.9062 (53.9062)
Test: [  50/78]  Time: 0.259 (0.271)  DataTime: 0.000 (0.012)  Loss:  3.3125 (3.2395)  Acc@1: 21.0938 (19.0104)  Acc@5: 49.2188 (53.1556)
Test: [  78/78]  Time: 0.034 (0.264)  DataTime: 0.000 (0.008)  Loss:  3.2227 (3.2510)  Acc@1: 12.5000 (18.7300)  Acc@5: 56.2500 (52.8000)
tb
loss  0 : 4.1626125 17
loss  1 : 3.15130625 17
loss  2 : 3.553746875 17
loss  3 : 3.1877375 17
{'loss_different_0': 4.1626125, 'loss_different_1': 3.15130625, 'loss_different_2': 3.553746875, 'loss_different_3': 3.1877375}
Top1:  18.73
Train: 18 [   0/390 (  0%)]  Loss:  3.536120 (3.5361)  Time: 1.838s,   69.65/s  (1.838s,   69.65/s)  LR: 9.801e-04  Data: 1.089 (1.089)
Train: 18 [  50/390 ( 13%)]  Loss:  3.574480 (3.5553)  Time: 0.748s,  171.20/s  (0.768s,  166.56/s)  LR: 9.801e-04  Data: 0.000 (0.022)
Train: 18 [ 100/390 ( 26%)]  Loss:  3.672058 (3.5942)  Time: 0.746s,  171.50/s  (0.758s,  168.90/s)  LR: 9.801e-04  Data: 0.000 (0.011)
Train: 18 [ 150/390 ( 39%)]  Loss:  3.489815 (3.5681)  Time: 0.746s,  171.50/s  (0.756s,  169.41/s)  LR: 9.801e-04  Data: 0.000 (0.008)
Train: 18 [ 200/390 ( 51%)]  Loss:  4.202812 (3.6951)  Time: 0.747s,  171.43/s  (0.753s,  169.89/s)  LR: 9.801e-04  Data: 0.000 (0.006)
Train: 18 [ 250/390 ( 64%)]  Loss:  3.418038 (3.6489)  Time: 0.747s,  171.43/s  (0.752s,  170.19/s)  LR: 9.801e-04  Data: 0.000 (0.005)
Train: 18 [ 300/390 ( 77%)]  Loss:  3.478920 (3.6246)  Time: 0.746s,  171.51/s  (0.751s,  170.38/s)  LR: 9.801e-04  Data: 0.000 (0.004)
Train: 18 [ 350/390 ( 90%)]  Loss:  3.540964 (3.6142)  Time: 0.749s,  170.83/s  (0.751s,  170.53/s)  LR: 9.801e-04  Data: 0.003 (0.003)
Train: 18 [ 389/390 (100%)]  Loss:  3.452332 (3.5962)  Time: 0.746s,  171.69/s  (0.750s,  170.61/s)  LR: 9.801e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.946 (0.946)  DataTime: 0.660 (0.660)  Loss:  3.0293 (3.0293)  Acc@1: 25.0000 (25.0000)  Acc@5: 58.5938 (58.5938)
Test: [  50/78]  Time: 0.258 (0.272)  DataTime: 0.000 (0.013)  Loss:  3.2480 (3.1824)  Acc@1: 23.4375 (21.0325)  Acc@5: 48.4375 (54.4424)
Test: [  78/78]  Time: 0.034 (0.264)  DataTime: 0.000 (0.009)  Loss:  3.1660 (3.1927)  Acc@1: 12.5000 (20.3500)  Acc@5: 56.2500 (54.3000)
tb
loss  0 : 4.121615625 18
loss  1 : 3.083784375 18
loss  2 : 3.517971875 18
loss  3 : 3.131853125 18
{'loss_different_0': 4.121615625, 'loss_different_1': 3.083784375, 'loss_different_2': 3.517971875, 'loss_different_3': 3.131853125}
Top1:  20.35
Train: 19 [   0/390 (  0%)]  Loss:  3.459507 (3.4595)  Time: 1.856s,   68.95/s  (1.856s,   68.95/s)  LR: 9.779e-04  Data: 1.108 (1.108)
Train: 19 [  50/390 ( 13%)]  Loss:  3.403510 (3.4315)  Time: 0.747s,  171.33/s  (0.769s,  166.53/s)  LR: 9.779e-04  Data: 0.000 (0.022)
Train: 19 [ 100/390 ( 26%)]  Loss:  3.383232 (3.4154)  Time: 0.747s,  171.31/s  (0.758s,  168.86/s)  LR: 9.779e-04  Data: 0.000 (0.011)
Train: 19 [ 150/390 ( 39%)]  Loss:  3.494225 (3.4351)  Time: 0.746s,  171.48/s  (0.754s,  169.67/s)  LR: 9.779e-04  Data: 0.000 (0.008)
Train: 19 [ 200/390 ( 51%)]  Loss:  3.605103 (3.4691)  Time: 0.748s,  171.08/s  (0.753s,  170.07/s)  LR: 9.779e-04  Data: 0.000 (0.006)
Train: 19 [ 250/390 ( 64%)]  Loss:  3.515175 (3.4768)  Time: 0.748s,  171.22/s  (0.752s,  170.32/s)  LR: 9.779e-04  Data: 0.000 (0.005)
Train: 19 [ 300/390 ( 77%)]  Loss:  3.858499 (3.5313)  Time: 0.746s,  171.55/s  (0.752s,  170.26/s)  LR: 9.779e-04  Data: 0.000 (0.004)
Train: 19 [ 350/390 ( 90%)]  Loss:  3.356454 (3.5095)  Time: 0.750s,  170.76/s  (0.751s,  170.42/s)  LR: 9.779e-04  Data: 0.003 (0.004)
Train: 19 [ 389/390 (100%)]  Loss:  3.520422 (3.5107)  Time: 0.747s,  171.25/s  (0.751s,  170.52/s)  LR: 9.779e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.054 (1.054)  DataTime: 0.739 (0.739)  Loss:  2.9863 (2.9863)  Acc@1: 24.2188 (24.2188)  Acc@5: 60.1562 (60.1562)
Test: [  50/78]  Time: 0.258 (0.274)  DataTime: 0.000 (0.015)  Loss:  3.1875 (3.1129)  Acc@1: 24.2188 (21.7525)  Acc@5: 54.6875 (56.0509)
Test: [  78/78]  Time: 0.034 (0.266)  DataTime: 0.000 (0.010)  Loss:  3.1113 (3.1259)  Acc@1: 12.5000 (20.9600)  Acc@5: 50.0000 (55.8500)
tb
loss  0 : 4.0515 19
loss  1 : 3.051628125 19
loss  2 : 3.434228125 19
loss  3 : 3.105571875 19
{'loss_different_0': 4.0515, 'loss_different_1': 3.051628125, 'loss_different_2': 3.434228125, 'loss_different_3': 3.105571875}
Top1:  20.96
Train: 20 [   0/390 (  0%)]  Loss:  3.511368 (3.5114)  Time: 1.268s,  100.95/s  (1.268s,  100.95/s)  LR: 9.755e-04  Data: 0.451 (0.451)
Train: 20 [  50/390 ( 13%)]  Loss:  3.820198 (3.6658)  Time: 0.746s,  171.50/s  (0.757s,  169.06/s)  LR: 9.755e-04  Data: 0.000 (0.009)
Train: 20 [ 100/390 ( 26%)]  Loss:  4.277967 (3.8698)  Time: 0.747s,  171.38/s  (0.752s,  170.20/s)  LR: 9.755e-04  Data: 0.000 (0.005)
Train: 20 [ 150/390 ( 39%)]  Loss:  3.559582 (3.7923)  Time: 0.746s,  171.57/s  (0.750s,  170.59/s)  LR: 9.755e-04  Data: 0.000 (0.003)
Train: 20 [ 200/390 ( 51%)]  Loss:  4.319352 (3.8977)  Time: 0.748s,  171.22/s  (0.750s,  170.78/s)  LR: 9.755e-04  Data: 0.000 (0.003)
Train: 20 [ 250/390 ( 64%)]  Loss:  3.627409 (3.8526)  Time: 0.747s,  171.36/s  (0.749s,  170.90/s)  LR: 9.755e-04  Data: 0.000 (0.002)
Train: 20 [ 300/390 ( 77%)]  Loss:  3.729442 (3.8350)  Time: 0.748s,  171.14/s  (0.749s,  170.98/s)  LR: 9.755e-04  Data: 0.000 (0.002)
Train: 20 [ 350/390 ( 90%)]  Loss:  3.513166 (3.7948)  Time: 0.749s,  170.87/s  (0.748s,  171.03/s)  LR: 9.755e-04  Data: 0.003 (0.002)
Train: 20 [ 389/390 (100%)]  Loss:  3.483384 (3.7602)  Time: 0.747s,  171.36/s  (0.748s,  171.07/s)  LR: 9.755e-04  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.049 (1.049)  DataTime: 0.749 (0.749)  Loss:  2.9219 (2.9219)  Acc@1: 26.5625 (26.5625)  Acc@5: 60.9375 (60.9375)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.015)  Loss:  3.1348 (3.0638)  Acc@1: 25.7812 (22.7022)  Acc@5: 56.2500 (57.6134)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.010)  Loss:  3.0469 (3.0734)  Acc@1: 12.5000 (21.9600)  Acc@5: 62.5000 (57.2200)
tb
loss  0 : 3.971725 20
loss  1 : 3.004434375 20
loss  2 : 3.388203125 20
loss  3 : 3.065696875 20
{'loss_different_0': 3.971725, 'loss_different_1': 3.004434375, 'loss_different_2': 3.388203125, 'loss_different_3': 3.065696875}
Top1:  21.96
Saving model to ./output/train/20250828-134932-pretrain_cifar_100_using_resuming_cifar_10_lr_1-3/pretrain_cifar_100_using_resuming_cifar_10_lr_1-3_20.pt
Train: 21 [   0/390 (  0%)]  Loss:  3.519360 (3.5194)  Time: 1.380s,   92.75/s  (1.380s,   92.75/s)  LR: 9.730e-04  Data: 0.594 (0.594)
Train: 21 [  50/390 ( 13%)]  Loss:  3.858493 (3.6889)  Time: 0.748s,  171.22/s  (0.759s,  168.54/s)  LR: 9.730e-04  Data: 0.000 (0.012)
Train: 21 [ 100/390 ( 26%)]  Loss:  3.540380 (3.6394)  Time: 0.748s,  171.23/s  (0.753s,  169.90/s)  LR: 9.730e-04  Data: 0.000 (0.006)
Train: 21 [ 150/390 ( 39%)]  Loss:  3.393935 (3.5780)  Time: 0.747s,  171.31/s  (0.751s,  170.37/s)  LR: 9.730e-04  Data: 0.000 (0.004)
Train: 21 [ 200/390 ( 51%)]  Loss:  3.605316 (3.5835)  Time: 0.748s,  171.22/s  (0.750s,  170.61/s)  LR: 9.730e-04  Data: 0.000 (0.003)
Train: 21 [ 250/390 ( 64%)]  Loss:  3.665457 (3.5972)  Time: 0.746s,  171.50/s  (0.750s,  170.60/s)  LR: 9.730e-04  Data: 0.000 (0.003)
Train: 21 [ 300/390 ( 77%)]  Loss:  3.316485 (3.5571)  Time: 0.746s,  171.48/s  (0.750s,  170.73/s)  LR: 9.730e-04  Data: 0.000 (0.002)
Train: 21 [ 350/390 ( 90%)]  Loss:  3.380817 (3.5350)  Time: 0.749s,  170.84/s  (0.749s,  170.82/s)  LR: 9.730e-04  Data: 0.003 (0.002)
Train: 21 [ 389/390 (100%)]  Loss:  3.360958 (3.5157)  Time: 0.746s,  171.47/s  (0.749s,  170.88/s)  LR: 9.730e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.913 (0.913)  DataTime: 0.638 (0.638)  Loss:  2.8906 (2.8906)  Acc@1: 25.7812 (25.7812)  Acc@5: 60.9375 (60.9375)
Test: [  50/78]  Time: 0.259 (0.272)  DataTime: 0.000 (0.013)  Loss:  3.0957 (3.0258)  Acc@1: 27.3438 (23.0239)  Acc@5: 57.0312 (58.3180)
Test: [  78/78]  Time: 0.034 (0.264)  DataTime: 0.000 (0.008)  Loss:  3.0293 (3.0341)  Acc@1: 18.7500 (22.6500)  Acc@5: 50.0000 (58.0600)
tb
loss  0 : 3.935859375 21
loss  1 : 2.97654375 21
loss  2 : 3.371340625 21
loss  3 : 3.0234 21
{'loss_different_0': 3.935859375, 'loss_different_1': 2.97654375, 'loss_different_2': 3.371340625, 'loss_different_3': 3.0234}
Top1:  22.65
Train: 22 [   0/390 (  0%)]  Loss:  3.410096 (3.4101)  Time: 0.887s,  144.38/s  (0.887s,  144.38/s)  LR: 9.704e-04  Data: 0.118 (0.118)
Train: 22 [  50/390 ( 13%)]  Loss:  3.673523 (3.5418)  Time: 0.747s,  171.29/s  (0.750s,  170.67/s)  LR: 9.704e-04  Data: 0.000 (0.003)
Train: 22 [ 100/390 ( 26%)]  Loss:  3.435911 (3.5065)  Time: 0.747s,  171.41/s  (0.748s,  171.02/s)  LR: 9.704e-04  Data: 0.000 (0.002)
Train: 22 [ 150/390 ( 39%)]  Loss:  3.502361 (3.5055)  Time: 0.747s,  171.29/s  (0.748s,  171.15/s)  LR: 9.704e-04  Data: 0.000 (0.001)
Train: 22 [ 200/390 ( 51%)]  Loss:  3.390912 (3.4826)  Time: 0.747s,  171.27/s  (0.748s,  171.20/s)  LR: 9.704e-04  Data: 0.000 (0.001)
Train: 22 [ 250/390 ( 64%)]  Loss:  3.309293 (3.4537)  Time: 0.747s,  171.35/s  (0.748s,  171.24/s)  LR: 9.704e-04  Data: 0.000 (0.001)
Train: 22 [ 300/390 ( 77%)]  Loss:  3.503436 (3.4608)  Time: 0.746s,  171.59/s  (0.747s,  171.26/s)  LR: 9.704e-04  Data: 0.000 (0.001)
Train: 22 [ 350/390 ( 90%)]  Loss:  4.233865 (3.5574)  Time: 1.027s,  124.66/s  (0.748s,  171.09/s)  LR: 9.704e-04  Data: 0.003 (0.001)
Train: 22 [ 389/390 (100%)]  Loss:  3.381484 (3.5379)  Time: 0.746s,  171.55/s  (0.748s,  171.13/s)  LR: 9.704e-04  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.205 (1.205)  DataTime: 0.938 (0.938)  Loss:  2.8477 (2.8477)  Acc@1: 24.2188 (24.2188)  Acc@5: 64.0625 (64.0625)
Test: [  50/78]  Time: 0.258 (0.277)  DataTime: 0.000 (0.019)  Loss:  3.0566 (2.9766)  Acc@1: 24.2188 (23.7898)  Acc@5: 57.8125 (59.6354)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.012)  Loss:  2.9160 (2.9863)  Acc@1: 18.7500 (23.1600)  Acc@5: 68.7500 (59.2400)
tb
loss  0 : 3.829871875 22
loss  1 : 2.941128125 22
loss  2 : 3.3185 22
loss  3 : 3.0126125 22
{'loss_different_0': 3.829871875, 'loss_different_1': 2.941128125, 'loss_different_2': 3.3185, 'loss_different_3': 3.0126125}
Top1:  23.16
Train: 23 [   0/390 (  0%)]  Loss:  4.288458 (4.2885)  Time: 1.154s,  110.88/s  (1.154s,  110.88/s)  LR: 9.677e-04  Data: 0.329 (0.329)
Train: 23 [  50/390 ( 13%)]  Loss:  3.285816 (3.7871)  Time: 0.747s,  171.39/s  (0.755s,  169.58/s)  LR: 9.677e-04  Data: 0.000 (0.007)
Train: 23 [ 100/390 ( 26%)]  Loss:  3.289059 (3.6211)  Time: 0.747s,  171.36/s  (0.751s,  170.46/s)  LR: 9.677e-04  Data: 0.000 (0.004)
Train: 23 [ 150/390 ( 39%)]  Loss:  3.390931 (3.5636)  Time: 0.748s,  171.23/s  (0.750s,  170.76/s)  LR: 9.677e-04  Data: 0.000 (0.003)
Train: 23 [ 200/390 ( 51%)]  Loss:  3.311577 (3.5132)  Time: 0.747s,  171.39/s  (0.749s,  170.91/s)  LR: 9.677e-04  Data: 0.000 (0.002)
Train: 23 [ 250/390 ( 64%)]  Loss:  3.283920 (3.4750)  Time: 0.747s,  171.44/s  (0.749s,  171.01/s)  LR: 9.677e-04  Data: 0.000 (0.002)
Train: 23 [ 300/390 ( 77%)]  Loss:  3.396223 (3.4637)  Time: 0.747s,  171.44/s  (0.748s,  171.07/s)  LR: 9.677e-04  Data: 0.000 (0.001)
Train: 23 [ 350/390 ( 90%)]  Loss:  3.381685 (3.4535)  Time: 0.749s,  170.88/s  (0.748s,  171.11/s)  LR: 9.677e-04  Data: 0.003 (0.001)
Train: 23 [ 389/390 (100%)]  Loss:  3.452738 (3.4534)  Time: 0.747s,  171.37/s  (0.748s,  171.15/s)  LR: 9.677e-04  Data: 0.000 (0.001)
Test: [   0/78]  Time: 1.241 (1.241)  DataTime: 0.946 (0.946)  Loss:  2.7656 (2.7656)  Acc@1: 28.9062 (28.9062)  Acc@5: 64.8438 (64.8438)
Test: [  50/78]  Time: 0.258 (0.278)  DataTime: 0.000 (0.019)  Loss:  3.0098 (2.9296)  Acc@1: 26.5625 (24.4638)  Acc@5: 59.3750 (60.2022)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.012)  Loss:  2.9180 (2.9386)  Acc@1: 25.0000 (23.9600)  Acc@5: 68.7500 (60.1900)
tb
loss  0 : 3.76068125 23
loss  1 : 2.899621875 23
loss  2 : 3.291275 23
loss  3 : 2.963153125 23
{'loss_different_0': 3.76068125, 'loss_different_1': 2.899621875, 'loss_different_2': 3.291275, 'loss_different_3': 2.963153125}
Top1:  23.96
Train: 24 [   0/390 (  0%)]  Loss:  3.411285 (3.4113)  Time: 1.821s,   70.29/s  (1.821s,   70.29/s)  LR: 9.649e-04  Data: 1.070 (1.070)
Train: 24 [  50/390 ( 13%)]  Loss:  3.291958 (3.3516)  Time: 0.747s,  171.43/s  (0.768s,  166.68/s)  LR: 9.649e-04  Data: 0.000 (0.021)
Train: 24 [ 100/390 ( 26%)]  Loss:  3.895017 (3.5328)  Time: 0.747s,  171.37/s  (0.758s,  168.97/s)  LR: 9.649e-04  Data: 0.000 (0.011)
Train: 24 [ 150/390 ( 39%)]  Loss:  3.163292 (3.4404)  Time: 0.747s,  171.43/s  (0.756s,  169.36/s)  LR: 9.649e-04  Data: 0.000 (0.007)
Train: 24 [ 200/390 ( 51%)]  Loss:  3.284791 (3.4093)  Time: 0.747s,  171.32/s  (0.754s,  169.85/s)  LR: 9.649e-04  Data: 0.000 (0.006)
Train: 24 [ 250/390 ( 64%)]  Loss:  3.959382 (3.5010)  Time: 0.747s,  171.36/s  (0.752s,  170.16/s)  LR: 9.649e-04  Data: 0.000 (0.005)
Train: 24 [ 300/390 ( 77%)]  Loss:  3.436408 (3.4917)  Time: 0.747s,  171.40/s  (0.751s,  170.35/s)  LR: 9.649e-04  Data: 0.000 (0.004)
Train: 24 [ 350/390 ( 90%)]  Loss:  3.748421 (3.5238)  Time: 0.750s,  170.74/s  (0.751s,  170.49/s)  LR: 9.649e-04  Data: 0.003 (0.003)
Train: 24 [ 389/390 (100%)]  Loss:  3.341686 (3.5036)  Time: 0.747s,  171.44/s  (0.750s,  170.59/s)  LR: 9.649e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.340 (1.340)  DataTime: 1.077 (1.077)  Loss:  2.7676 (2.7676)  Acc@1: 27.3438 (27.3438)  Acc@5: 63.2812 (63.2812)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.021)  Loss:  2.9805 (2.8893)  Acc@1: 26.5625 (24.9694)  Acc@5: 57.0312 (61.4277)
Test: [  78/78]  Time: 0.034 (0.269)  DataTime: 0.000 (0.014)  Loss:  2.8906 (2.8991)  Acc@1: 25.0000 (24.5700)  Acc@5: 56.2500 (61.0900)
tb
loss  0 : 3.690934375 24
loss  1 : 2.8788125 24
loss  2 : 3.245315625 24
loss  3 : 2.9454625 24
{'loss_different_0': 3.690934375, 'loss_different_1': 2.8788125, 'loss_different_2': 3.245315625, 'loss_different_3': 2.9454625}
Top1:  24.57
Train: 25 [   0/390 (  0%)]  Loss:  3.297959 (3.2980)  Time: 1.398s,   91.56/s  (1.398s,   91.56/s)  LR: 9.619e-04  Data: 0.563 (0.563)
Train: 25 [  50/390 ( 13%)]  Loss:  3.375903 (3.3369)  Time: 0.747s,  171.44/s  (0.760s,  168.45/s)  LR: 9.619e-04  Data: 0.000 (0.011)
Train: 25 [ 100/390 ( 26%)]  Loss:  3.214158 (3.2960)  Time: 0.748s,  171.05/s  (0.754s,  169.85/s)  LR: 9.619e-04  Data: 0.000 (0.006)
Train: 25 [ 150/390 ( 39%)]  Loss:  4.432400 (3.5801)  Time: 0.748s,  171.18/s  (0.751s,  170.33/s)  LR: 9.619e-04  Data: 0.000 (0.004)
Train: 25 [ 200/390 ( 51%)]  Loss:  3.250107 (3.5141)  Time: 0.747s,  171.46/s  (0.750s,  170.59/s)  LR: 9.619e-04  Data: 0.000 (0.003)
Train: 25 [ 250/390 ( 64%)]  Loss:  3.442358 (3.5021)  Time: 0.747s,  171.39/s  (0.750s,  170.75/s)  LR: 9.619e-04  Data: 0.000 (0.003)
Train: 25 [ 300/390 ( 77%)]  Loss:  3.832902 (3.5494)  Time: 0.748s,  171.20/s  (0.749s,  170.85/s)  LR: 9.619e-04  Data: 0.000 (0.002)
Train: 25 [ 350/390 ( 90%)]  Loss:  3.392834 (3.5298)  Time: 0.749s,  171.00/s  (0.749s,  170.92/s)  LR: 9.619e-04  Data: 0.003 (0.002)
Train: 25 [ 389/390 (100%)]  Loss:  3.223666 (3.4958)  Time: 0.747s,  171.36/s  (0.749s,  170.97/s)  LR: 9.619e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.381 (1.381)  DataTime: 1.097 (1.097)  Loss:  2.7207 (2.7207)  Acc@1: 25.7812 (25.7812)  Acc@5: 63.2812 (63.2812)
Test: [  50/78]  Time: 0.258 (0.281)  DataTime: 0.000 (0.022)  Loss:  2.9180 (2.8511)  Acc@1: 28.1250 (26.0263)  Acc@5: 60.9375 (62.2243)
Test: [  78/78]  Time: 0.035 (0.270)  DataTime: 0.000 (0.014)  Loss:  2.8555 (2.8583)  Acc@1: 18.7500 (25.6100)  Acc@5: 68.7500 (62.2700)
tb
loss  0 : 3.63625625 25
loss  1 : 2.837846875 25
loss  2 : 3.22489375 25
loss  3 : 2.90499375 25
{'loss_different_0': 3.63625625, 'loss_different_1': 2.837846875, 'loss_different_2': 3.22489375, 'loss_different_3': 2.90499375}
Top1:  25.61
Train: 26 [   0/390 (  0%)]  Loss:  4.110920 (4.1109)  Time: 1.806s,   70.87/s  (1.806s,   70.87/s)  LR: 9.589e-04  Data: 1.058 (1.058)
Train: 26 [  50/390 ( 13%)]  Loss:  3.368706 (3.7398)  Time: 0.748s,  171.22/s  (0.772s,  165.88/s)  LR: 9.589e-04  Data: 0.000 (0.021)
Train: 26 [ 100/390 ( 26%)]  Loss:  3.402635 (3.6274)  Time: 0.747s,  171.32/s  (0.760s,  168.52/s)  LR: 9.589e-04  Data: 0.000 (0.011)
Train: 26 [ 150/390 ( 39%)]  Loss:  3.430683 (3.5782)  Time: 0.747s,  171.33/s  (0.755s,  169.43/s)  LR: 9.589e-04  Data: 0.000 (0.007)
Train: 26 [ 200/390 ( 51%)]  Loss:  3.320722 (3.5267)  Time: 0.748s,  171.23/s  (0.753s,  169.90/s)  LR: 9.589e-04  Data: 0.000 (0.006)
Train: 26 [ 250/390 ( 64%)]  Loss:  4.121307 (3.6258)  Time: 0.747s,  171.34/s  (0.752s,  170.20/s)  LR: 9.589e-04  Data: 0.000 (0.005)
Train: 26 [ 300/390 ( 77%)]  Loss:  3.325798 (3.5830)  Time: 0.747s,  171.45/s  (0.751s,  170.39/s)  LR: 9.589e-04  Data: 0.000 (0.004)
Train: 26 [ 350/390 ( 90%)]  Loss:  3.307011 (3.5485)  Time: 0.749s,  170.87/s  (0.751s,  170.53/s)  LR: 9.589e-04  Data: 0.003 (0.003)
Train: 26 [ 389/390 (100%)]  Loss:  3.023991 (3.4902)  Time: 0.746s,  171.49/s  (0.750s,  170.62/s)  LR: 9.589e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.959 (0.959)  DataTime: 0.676 (0.676)  Loss:  2.6543 (2.6543)  Acc@1: 28.9062 (28.9062)  Acc@5: 64.8438 (64.8438)
Test: [  50/78]  Time: 0.258 (0.275)  DataTime: 0.000 (0.016)  Loss:  2.8848 (2.8073)  Acc@1: 28.1250 (26.6391)  Acc@5: 61.7188 (62.8830)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.011)  Loss:  2.8438 (2.8142)  Acc@1: 18.7500 (26.4700)  Acc@5: 56.2500 (63.0200)
tb
loss  0 : 3.54398125 26
loss  1 : 2.79235625 26
loss  2 : 3.178759375 26
loss  3 : 2.872915625 26
{'loss_different_0': 3.54398125, 'loss_different_1': 2.79235625, 'loss_different_2': 3.178759375, 'loss_different_3': 2.872915625}
Top1:  26.47
Train: 27 [   0/390 (  0%)]  Loss:  3.206383 (3.2064)  Time: 1.451s,   88.22/s  (1.451s,   88.22/s)  LR: 9.557e-04  Data: 0.629 (0.629)
Train: 27 [  50/390 ( 13%)]  Loss:  3.073610 (3.1400)  Time: 0.747s,  171.45/s  (0.761s,  168.27/s)  LR: 9.557e-04  Data: 0.000 (0.013)
Train: 27 [ 100/390 ( 26%)]  Loss:  4.013242 (3.4311)  Time: 0.746s,  171.51/s  (0.754s,  169.79/s)  LR: 9.557e-04  Data: 0.000 (0.007)
Train: 27 [ 150/390 ( 39%)]  Loss:  3.232083 (3.3813)  Time: 0.748s,  171.21/s  (0.752s,  170.31/s)  LR: 9.557e-04  Data: 0.000 (0.004)
Train: 27 [ 200/390 ( 51%)]  Loss:  3.162177 (3.3375)  Time: 0.747s,  171.32/s  (0.752s,  170.27/s)  LR: 9.557e-04  Data: 0.000 (0.003)
Train: 27 [ 250/390 ( 64%)]  Loss:  4.320545 (3.5013)  Time: 0.748s,  171.11/s  (0.751s,  170.49/s)  LR: 9.557e-04  Data: 0.000 (0.003)
Train: 27 [ 300/390 ( 77%)]  Loss:  3.075485 (3.4405)  Time: 0.747s,  171.31/s  (0.750s,  170.64/s)  LR: 9.557e-04  Data: 0.000 (0.002)
Train: 27 [ 350/390 ( 90%)]  Loss:  3.057887 (3.3927)  Time: 0.749s,  170.89/s  (0.750s,  170.74/s)  LR: 9.557e-04  Data: 0.003 (0.002)
Train: 27 [ 389/390 (100%)]  Loss:  3.257324 (3.3776)  Time: 0.746s,  171.53/s  (0.749s,  170.81/s)  LR: 9.557e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.063 (1.063)  DataTime: 0.772 (0.772)  Loss:  2.6270 (2.6270)  Acc@1: 32.0312 (32.0312)  Acc@5: 62.5000 (62.5000)
Test: [  50/78]  Time: 0.258 (0.274)  DataTime: 0.000 (0.015)  Loss:  2.8770 (2.7636)  Acc@1: 28.1250 (27.3131)  Acc@5: 57.8125 (64.0165)
Test: [  78/78]  Time: 0.034 (0.266)  DataTime: 0.000 (0.010)  Loss:  2.7441 (2.7706)  Acc@1: 25.0000 (26.9600)  Acc@5: 68.7500 (64.0100)
tb
loss  0 : 3.47353125 27
loss  1 : 2.7713 27
loss  2 : 3.125009375 27
loss  3 : 2.85218125 27
{'loss_different_0': 3.47353125, 'loss_different_1': 2.7713, 'loss_different_2': 3.125009375, 'loss_different_3': 2.85218125}
Top1:  26.96
Train: 28 [   0/390 (  0%)]  Loss:  3.191543 (3.1915)  Time: 1.371s,   93.36/s  (1.371s,   93.36/s)  LR: 9.524e-04  Data: 0.504 (0.504)
Train: 28 [  50/390 ( 13%)]  Loss:  3.082058 (3.1368)  Time: 0.747s,  171.40/s  (0.759s,  168.62/s)  LR: 9.524e-04  Data: 0.000 (0.010)
Train: 28 [ 100/390 ( 26%)]  Loss:  3.173742 (3.1491)  Time: 0.747s,  171.43/s  (0.753s,  169.97/s)  LR: 9.524e-04  Data: 0.000 (0.005)
Train: 28 [ 150/390 ( 39%)]  Loss:  3.963883 (3.3528)  Time: 0.747s,  171.34/s  (0.751s,  170.43/s)  LR: 9.524e-04  Data: 0.000 (0.004)
Train: 28 [ 200/390 ( 51%)]  Loss:  3.122506 (3.3067)  Time: 0.748s,  171.21/s  (0.750s,  170.66/s)  LR: 9.524e-04  Data: 0.000 (0.003)
Train: 28 [ 250/390 ( 64%)]  Loss:  3.084305 (3.2697)  Time: 0.747s,  171.40/s  (0.749s,  170.79/s)  LR: 9.524e-04  Data: 0.000 (0.002)
Train: 28 [ 300/390 ( 77%)]  Loss:  3.235337 (3.2648)  Time: 0.748s,  171.23/s  (0.749s,  170.89/s)  LR: 9.524e-04  Data: 0.000 (0.002)
Train: 28 [ 350/390 ( 90%)]  Loss:  4.056705 (3.3638)  Time: 0.749s,  170.85/s  (0.749s,  170.95/s)  LR: 9.524e-04  Data: 0.003 (0.002)
Train: 28 [ 389/390 (100%)]  Loss:  3.173733 (3.3426)  Time: 0.747s,  171.33/s  (0.749s,  171.00/s)  LR: 9.524e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.846 (0.846)  DataTime: 0.576 (0.576)  Loss:  2.6191 (2.6191)  Acc@1: 33.5938 (33.5938)  Acc@5: 64.0625 (64.0625)
Test: [  50/78]  Time: 0.258 (0.271)  DataTime: 0.000 (0.012)  Loss:  2.8340 (2.7311)  Acc@1: 28.9062 (28.6152)  Acc@5: 62.5000 (65.1042)
Test: [  78/78]  Time: 0.034 (0.264)  DataTime: 0.000 (0.008)  Loss:  2.7031 (2.7397)  Acc@1: 31.2500 (28.0600)  Acc@5: 62.5000 (65.0100)
tb
loss  0 : 3.45613125 28
loss  1 : 2.72411875 28
loss  2 : 3.1112375 28
loss  3 : 2.82506875 28
{'loss_different_0': 3.45613125, 'loss_different_1': 2.72411875, 'loss_different_2': 3.1112375, 'loss_different_3': 2.82506875}
Top1:  28.06
Train: 29 [   0/390 (  0%)]  Loss:  3.230321 (3.2303)  Time: 1.831s,   69.90/s  (1.831s,   69.90/s)  LR: 9.490e-04  Data: 1.082 (1.082)
Train: 29 [  50/390 ( 13%)]  Loss:  3.066614 (3.1485)  Time: 0.747s,  171.26/s  (0.768s,  166.63/s)  LR: 9.490e-04  Data: 0.000 (0.022)
Train: 29 [ 100/390 ( 26%)]  Loss:  3.252525 (3.1832)  Time: 0.747s,  171.42/s  (0.759s,  168.54/s)  LR: 9.490e-04  Data: 0.000 (0.011)
Train: 29 [ 150/390 ( 39%)]  Loss:  3.613984 (3.2909)  Time: 0.747s,  171.45/s  (0.755s,  169.46/s)  LR: 9.490e-04  Data: 0.000 (0.007)
Train: 29 [ 200/390 ( 51%)]  Loss:  3.084358 (3.2496)  Time: 0.747s,  171.24/s  (0.753s,  169.92/s)  LR: 9.490e-04  Data: 0.000 (0.006)
Train: 29 [ 250/390 ( 64%)]  Loss:  2.973845 (3.2036)  Time: 0.747s,  171.29/s  (0.752s,  170.20/s)  LR: 9.490e-04  Data: 0.000 (0.005)
Train: 29 [ 300/390 ( 77%)]  Loss:  3.219477 (3.2059)  Time: 0.747s,  171.29/s  (0.751s,  170.39/s)  LR: 9.490e-04  Data: 0.000 (0.004)
Train: 29 [ 350/390 ( 90%)]  Loss:  2.909773 (3.1689)  Time: 0.749s,  170.88/s  (0.751s,  170.53/s)  LR: 9.490e-04  Data: 0.003 (0.003)
Train: 29 [ 389/390 (100%)]  Loss:  3.106449 (3.1619)  Time: 0.747s,  171.29/s  (0.750s,  170.61/s)  LR: 9.490e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.162 (1.162)  DataTime: 0.851 (0.851)  Loss:  2.5605 (2.5605)  Acc@1: 33.5938 (33.5938)  Acc@5: 64.0625 (64.0625)
Test: [  50/78]  Time: 0.258 (0.276)  DataTime: 0.000 (0.017)  Loss:  2.7852 (2.6928)  Acc@1: 32.8125 (29.5037)  Acc@5: 65.6250 (66.0539)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.011)  Loss:  2.6270 (2.7003)  Acc@1: 43.7500 (29.0100)  Acc@5: 62.5000 (66.2900)
tb
loss  0 : 3.386253125 29
loss  1 : 2.6917875 29
loss  2 : 3.069559375 29
loss  3 : 2.782565625 29
{'loss_different_0': 3.386253125, 'loss_different_1': 2.6917875, 'loss_different_2': 3.069559375, 'loss_different_3': 2.782565625}
Top1:  29.01
Train: 30 [   0/390 (  0%)]  Loss:  2.897619 (2.8976)  Time: 1.850s,   69.19/s  (1.850s,   69.19/s)  LR: 9.455e-04  Data: 1.101 (1.101)
Train: 30 [  50/390 ( 13%)]  Loss:  3.214921 (3.0563)  Time: 0.746s,  171.52/s  (0.769s,  166.50/s)  LR: 9.455e-04  Data: 0.000 (0.022)
Train: 30 [ 100/390 ( 26%)]  Loss:  2.888580 (3.0004)  Time: 0.747s,  171.29/s  (0.758s,  168.84/s)  LR: 9.455e-04  Data: 0.000 (0.011)
Train: 30 [ 150/390 ( 39%)]  Loss:  2.957476 (2.9896)  Time: 0.747s,  171.32/s  (0.755s,  169.63/s)  LR: 9.455e-04  Data: 0.000 (0.008)
Train: 30 [ 200/390 ( 51%)]  Loss:  4.236976 (3.2391)  Time: 0.747s,  171.42/s  (0.753s,  170.03/s)  LR: 9.455e-04  Data: 0.000 (0.006)
Train: 30 [ 250/390 ( 64%)]  Loss:  3.137165 (3.2221)  Time: 0.747s,  171.39/s  (0.753s,  170.02/s)  LR: 9.455e-04  Data: 0.000 (0.005)
Train: 30 [ 300/390 ( 77%)]  Loss:  3.092497 (3.2036)  Time: 0.747s,  171.29/s  (0.752s,  170.23/s)  LR: 9.455e-04  Data: 0.000 (0.004)
Train: 30 [ 350/390 ( 90%)]  Loss:  3.867415 (3.2866)  Time: 0.750s,  170.59/s  (0.751s,  170.38/s)  LR: 9.455e-04  Data: 0.003 (0.003)
Train: 30 [ 389/390 (100%)]  Loss:  3.206022 (3.2776)  Time: 0.747s,  171.31/s  (0.751s,  170.47/s)  LR: 9.455e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.130 (1.130)  DataTime: 0.858 (0.858)  Loss:  2.5449 (2.5449)  Acc@1: 32.0312 (32.0312)  Acc@5: 65.6250 (65.6250)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.017)  Loss:  2.7461 (2.6625)  Acc@1: 34.3750 (29.9326)  Acc@5: 66.4062 (66.6973)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.011)  Loss:  2.6367 (2.6704)  Acc@1: 43.7500 (29.6500)  Acc@5: 62.5000 (66.6600)
tb
loss  0 : 3.3389 30
loss  1 : 2.666253125 30
loss  2 : 3.049609375 30
loss  3 : 2.760421875 30
{'loss_different_0': 3.3389, 'loss_different_1': 2.666253125, 'loss_different_2': 3.049609375, 'loss_different_3': 2.760421875}
Top1:  29.65
Saving model to ./output/train/20250828-134932-pretrain_cifar_100_using_resuming_cifar_10_lr_1-3/pretrain_cifar_100_using_resuming_cifar_10_lr_1-3_30.pt
Train: 31 [   0/390 (  0%)]  Loss:  4.079995 (4.0800)  Time: 1.461s,   87.59/s  (1.461s,   87.59/s)  LR: 9.419e-04  Data: 0.680 (0.680)
Train: 31 [  50/390 ( 13%)]  Loss:  3.142464 (3.6112)  Time: 0.747s,  171.34/s  (0.761s,  168.09/s)  LR: 9.419e-04  Data: 0.000 (0.014)
Train: 31 [ 100/390 ( 26%)]  Loss:  4.067171 (3.7632)  Time: 0.748s,  171.12/s  (0.755s,  169.64/s)  LR: 9.419e-04  Data: 0.000 (0.007)
Train: 31 [ 150/390 ( 39%)]  Loss:  3.110249 (3.6000)  Time: 0.747s,  171.30/s  (0.752s,  170.18/s)  LR: 9.419e-04  Data: 0.000 (0.005)
Train: 31 [ 200/390 ( 51%)]  Loss:  3.468707 (3.5737)  Time: 0.747s,  171.36/s  (0.751s,  170.46/s)  LR: 9.419e-04  Data: 0.000 (0.004)
Train: 31 [ 250/390 ( 64%)]  Loss:  2.969877 (3.4731)  Time: 0.747s,  171.39/s  (0.750s,  170.64/s)  LR: 9.419e-04  Data: 0.000 (0.003)
Train: 31 [ 300/390 ( 77%)]  Loss:  2.920796 (3.3942)  Time: 0.747s,  171.26/s  (0.750s,  170.75/s)  LR: 9.419e-04  Data: 0.000 (0.003)
Train: 31 [ 350/390 ( 90%)]  Loss:  3.573959 (3.4167)  Time: 0.750s,  170.77/s  (0.749s,  170.83/s)  LR: 9.419e-04  Data: 0.003 (0.002)
Train: 31 [ 389/390 (100%)]  Loss:  3.629296 (3.4403)  Time: 0.746s,  171.52/s  (0.749s,  170.89/s)  LR: 9.419e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.975 (0.975)  DataTime: 0.646 (0.646)  Loss:  2.4941 (2.4941)  Acc@1: 34.3750 (34.3750)  Acc@5: 66.4062 (66.4062)
Test: [  50/78]  Time: 0.258 (0.275)  DataTime: 0.000 (0.016)  Loss:  2.7227 (2.6228)  Acc@1: 31.2500 (30.8977)  Acc@5: 66.4062 (67.5245)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.010)  Loss:  2.7090 (2.6315)  Acc@1: 18.7500 (30.2900)  Acc@5: 75.0000 (67.6200)
tb
loss  0 : 3.25023125 31
loss  1 : 2.646240625 31
loss  2 : 2.991715625 31
loss  3 : 2.74675625 31
{'loss_different_0': 3.25023125, 'loss_different_1': 2.646240625, 'loss_different_2': 2.991715625, 'loss_different_3': 2.74675625}
Top1:  30.29
Train: 32 [   0/390 (  0%)]  Loss:  3.676478 (3.6765)  Time: 1.802s,   71.04/s  (1.802s,   71.04/s)  LR: 9.382e-04  Data: 1.054 (1.054)
Train: 32 [  50/390 ( 13%)]  Loss:  2.925830 (3.3012)  Time: 0.747s,  171.27/s  (0.768s,  166.70/s)  LR: 9.382e-04  Data: 0.000 (0.021)
Train: 32 [ 100/390 ( 26%)]  Loss:  3.324008 (3.3088)  Time: 0.748s,  171.21/s  (0.758s,  168.96/s)  LR: 9.382e-04  Data: 0.000 (0.011)
Train: 32 [ 150/390 ( 39%)]  Loss:  3.079899 (3.2516)  Time: 0.748s,  171.21/s  (0.754s,  169.74/s)  LR: 9.382e-04  Data: 0.000 (0.007)
Train: 32 [ 200/390 ( 51%)]  Loss:  3.144758 (3.2302)  Time: 0.747s,  171.37/s  (0.753s,  169.91/s)  LR: 9.382e-04  Data: 0.000 (0.006)
Train: 32 [ 250/390 ( 64%)]  Loss:  3.140424 (3.2152)  Time: 0.748s,  171.21/s  (0.752s,  170.19/s)  LR: 9.382e-04  Data: 0.001 (0.005)
Train: 32 [ 300/390 ( 77%)]  Loss:  2.978168 (3.1814)  Time: 0.748s,  171.20/s  (0.751s,  170.38/s)  LR: 9.382e-04  Data: 0.000 (0.004)
Train: 32 [ 350/390 ( 90%)]  Loss:  2.986470 (3.1570)  Time: 0.749s,  170.87/s  (0.751s,  170.52/s)  LR: 9.382e-04  Data: 0.003 (0.003)
Train: 32 [ 389/390 (100%)]  Loss:  3.218441 (3.1638)  Time: 0.746s,  171.57/s  (0.750s,  170.60/s)  LR: 9.382e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.002 (1.002)  DataTime: 0.664 (0.664)  Loss:  2.5078 (2.5078)  Acc@1: 36.7188 (36.7188)  Acc@5: 68.7500 (68.7500)
Test: [  50/78]  Time: 0.259 (0.273)  DataTime: 0.000 (0.013)  Loss:  2.7207 (2.5933)  Acc@1: 35.1562 (31.4951)  Acc@5: 64.8438 (68.1985)
Test: [  78/78]  Time: 0.034 (0.265)  DataTime: 0.000 (0.009)  Loss:  2.6777 (2.6014)  Acc@1: 25.0000 (31.1400)  Acc@5: 68.7500 (68.1300)
tb
loss  0 : 3.22631875 32
loss  1 : 2.614503125 32
loss  2 : 2.982565625 32
loss  3 : 2.704525 32
{'loss_different_0': 3.22631875, 'loss_different_1': 2.614503125, 'loss_different_2': 2.982565625, 'loss_different_3': 2.704525}
Top1:  31.14
Train: 33 [   0/390 (  0%)]  Loss:  3.184010 (3.1840)  Time: 1.381s,   92.69/s  (1.381s,   92.69/s)  LR: 9.343e-04  Data: 0.547 (0.547)
Train: 33 [  50/390 ( 13%)]  Loss:  4.298905 (3.7415)  Time: 0.747s,  171.34/s  (0.760s,  168.53/s)  LR: 9.343e-04  Data: 0.000 (0.011)
Train: 33 [ 100/390 ( 26%)]  Loss:  2.954809 (3.4792)  Time: 0.747s,  171.44/s  (0.753s,  169.94/s)  LR: 9.343e-04  Data: 0.000 (0.006)
Train: 33 [ 150/390 ( 39%)]  Loss:  2.958205 (3.3490)  Time: 0.747s,  171.45/s  (0.751s,  170.40/s)  LR: 9.343e-04  Data: 0.000 (0.004)
Train: 33 [ 200/390 ( 51%)]  Loss:  3.163343 (3.3119)  Time: 0.747s,  171.40/s  (0.750s,  170.63/s)  LR: 9.343e-04  Data: 0.000 (0.003)
Train: 33 [ 250/390 ( 64%)]  Loss:  3.135277 (3.2824)  Time: 0.748s,  171.20/s  (0.750s,  170.77/s)  LR: 9.343e-04  Data: 0.000 (0.003)
Train: 33 [ 300/390 ( 77%)]  Loss:  2.873089 (3.2239)  Time: 0.747s,  171.37/s  (0.749s,  170.87/s)  LR: 9.343e-04  Data: 0.000 (0.002)
Train: 33 [ 350/390 ( 90%)]  Loss:  3.097536 (3.2081)  Time: 0.749s,  170.95/s  (0.750s,  170.75/s)  LR: 9.343e-04  Data: 0.003 (0.002)
Train: 33 [ 389/390 (100%)]  Loss:  2.819658 (3.1650)  Time: 0.748s,  171.24/s  (0.749s,  170.81/s)  LR: 9.343e-04  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.950 (0.950)  DataTime: 0.663 (0.663)  Loss:  2.4121 (2.4121)  Acc@1: 33.5938 (33.5938)  Acc@5: 68.7500 (68.7500)
Test: [  50/78]  Time: 0.258 (0.272)  DataTime: 0.000 (0.013)  Loss:  2.6582 (2.5663)  Acc@1: 34.3750 (31.7862)  Acc@5: 66.4062 (68.7347)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.009)  Loss:  2.5801 (2.5743)  Acc@1: 37.5000 (31.2800)  Acc@5: 68.7500 (68.8300)
tb
loss  0 : 3.22246875 33
loss  1 : 2.5819 33
loss  2 : 2.954921875 33
loss  3 : 2.676446875 33
{'loss_different_0': 3.22246875, 'loss_different_1': 2.5819, 'loss_different_2': 2.954921875, 'loss_different_3': 2.676446875}
Top1:  31.28
Train: 34 [   0/390 (  0%)]  Loss:  3.898212 (3.8982)  Time: 1.823s,   70.21/s  (1.823s,   70.21/s)  LR: 9.304e-04  Data: 1.075 (1.075)
Train: 34 [  50/390 ( 13%)]  Loss:  3.145397 (3.5218)  Time: 0.747s,  171.40/s  (0.768s,  166.64/s)  LR: 9.304e-04  Data: 0.000 (0.021)
Train: 34 [ 100/390 ( 26%)]  Loss:  4.175261 (3.7396)  Time: 0.746s,  171.48/s  (0.758s,  168.93/s)  LR: 9.304e-04  Data: 0.000 (0.011)
Train: 34 [ 150/390 ( 39%)]  Loss:  2.875522 (3.5236)  Time: 0.747s,  171.29/s  (0.754s,  169.72/s)  LR: 9.304e-04  Data: 0.000 (0.007)
Train: 34 [ 200/390 ( 51%)]  Loss:  2.843440 (3.3876)  Time: 0.747s,  171.31/s  (0.752s,  170.12/s)  LR: 9.304e-04  Data: 0.000 (0.006)
Train: 34 [ 250/390 ( 64%)]  Loss:  2.997906 (3.3226)  Time: 0.748s,  171.21/s  (0.751s,  170.36/s)  LR: 9.304e-04  Data: 0.000 (0.005)
Train: 34 [ 300/390 ( 77%)]  Loss:  2.933714 (3.2671)  Time: 0.747s,  171.35/s  (0.751s,  170.52/s)  LR: 9.304e-04  Data: 0.001 (0.004)
Train: 34 [ 350/390 ( 90%)]  Loss:  2.855946 (3.2157)  Time: 0.749s,  170.87/s  (0.750s,  170.64/s)  LR: 9.304e-04  Data: 0.003 (0.003)
Train: 34 [ 389/390 (100%)]  Loss:  3.014505 (3.1933)  Time: 0.747s,  171.44/s  (0.750s,  170.71/s)  LR: 9.304e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.913 (0.913)  DataTime: 0.645 (0.645)  Loss:  2.4102 (2.4102)  Acc@1: 40.6250 (40.6250)  Acc@5: 71.0938 (71.0938)
Test: [  50/78]  Time: 0.258 (0.274)  DataTime: 0.000 (0.016)  Loss:  2.6055 (2.5333)  Acc@1: 37.5000 (32.7512)  Acc@5: 67.1875 (69.4393)
Test: [  78/78]  Time: 0.034 (0.266)  DataTime: 0.000 (0.010)  Loss:  2.4727 (2.5392)  Acc@1: 31.2500 (32.0700)  Acc@5: 68.7500 (69.4700)
tb
loss  0 : 3.149803125 34
loss  1 : 2.558359375 34
loss  2 : 2.92725 34
loss  3 : 2.668478125 34
{'loss_different_0': 3.149803125, 'loss_different_1': 2.558359375, 'loss_different_2': 2.92725, 'loss_different_3': 2.668478125}
Top1:  32.07
Train: 35 [   0/390 (  0%)]  Loss:  3.007199 (3.0072)  Time: 1.831s,   69.89/s  (1.831s,   69.89/s)  LR: 9.263e-04  Data: 1.083 (1.083)
Train: 35 [  50/390 ( 13%)]  Loss:  4.011467 (3.5093)  Time: 0.748s,  171.13/s  (0.768s,  166.57/s)  LR: 9.263e-04  Data: 0.000 (0.022)
Train: 35 [ 100/390 ( 26%)]  Loss:  2.908668 (3.3091)  Time: 0.747s,  171.37/s  (0.758s,  168.88/s)  LR: 9.263e-04  Data: 0.000 (0.011)
Train: 35 [ 150/390 ( 39%)]  Loss:  2.950807 (3.2195)  Time: 0.747s,  171.36/s  (0.754s,  169.68/s)  LR: 9.263e-04  Data: 0.000 (0.007)
Train: 35 [ 200/390 ( 51%)]  Loss:  2.984620 (3.1726)  Time: 0.747s,  171.30/s  (0.753s,  170.09/s)  LR: 9.263e-04  Data: 0.000 (0.006)
Train: 35 [ 250/390 ( 64%)]  Loss:  3.559838 (3.2371)  Time: 0.748s,  171.21/s  (0.752s,  170.18/s)  LR: 9.263e-04  Data: 0.000 (0.005)
Train: 35 [ 300/390 ( 77%)]  Loss:  3.824538 (3.3210)  Time: 0.747s,  171.32/s  (0.751s,  170.36/s)  LR: 9.263e-04  Data: 0.000 (0.004)
Train: 35 [ 350/390 ( 90%)]  Loss:  2.848622 (3.2620)  Time: 0.750s,  170.73/s  (0.751s,  170.50/s)  LR: 9.263e-04  Data: 0.003 (0.003)
Train: 35 [ 389/390 (100%)]  Loss:  3.711223 (3.3119)  Time: 0.747s,  171.44/s  (0.750s,  170.59/s)  LR: 9.263e-04  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.939 (0.939)  DataTime: 0.667 (0.667)  Loss:  2.3594 (2.3594)  Acc@1: 37.5000 (37.5000)  Acc@5: 71.0938 (71.0938)
Test: [  50/78]  Time: 0.258 (0.272)  DataTime: 0.000 (0.013)  Loss:  2.5703 (2.4914)  Acc@1: 38.2812 (34.3903)  Acc@5: 69.5312 (70.2665)
Test: [  78/78]  Time: 0.034 (0.264)  DataTime: 0.000 (0.009)  Loss:  2.5742 (2.4997)  Acc@1: 25.0000 (33.9400)  Acc@5: 68.7500 (70.3000)
tb
loss  0 : 3.08701875 35
loss  1 : 2.52435625 35
loss  2 : 2.863565625 35
loss  3 : 2.63871875 35
{'loss_different_0': 3.08701875, 'loss_different_1': 2.52435625, 'loss_different_2': 2.863565625, 'loss_different_3': 2.63871875}
Top1:  33.94
Train: 36 [   0/390 (  0%)]  Loss:  3.933731 (3.9337)  Time: 1.451s,   88.22/s  (1.451s,   88.22/s)  LR: 9.222e-04  Data: 0.665 (0.665)
^C