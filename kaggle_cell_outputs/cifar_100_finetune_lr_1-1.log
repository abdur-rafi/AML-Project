2025-08-28 11:40:28.040491: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1756381228.063679     285 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1756381228.071184     285 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Creating model..
, number of params: 27599012
SGD!!!
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Train: 0 [   0/390 (  0%)]  Loss:  4.622955 (4.6230)  Time: 3.049s,   41.99/s  (3.049s,   41.99/s)  LR: 1.000e-01  Data: 0.920 (0.920)
Train: 0 [  50/390 ( 13%)]  Loss:  4.284022 (4.4535)  Time: 0.748s,  171.22/s  (0.793s,  161.42/s)  LR: 1.000e-01  Data: 0.000 (0.018)
Train: 0 [ 100/390 ( 26%)]  Loss:  3.621881 (4.1763)  Time: 0.748s,  171.07/s  (0.771s,  166.12/s)  LR: 1.000e-01  Data: 0.000 (0.009)
Train: 0 [ 150/390 ( 39%)]  Loss:  3.453497 (3.9956)  Time: 0.750s,  170.71/s  (0.763s,  167.75/s)  LR: 1.000e-01  Data: 0.000 (0.006)
Train: 0 [ 200/390 ( 51%)]  Loss:  3.894326 (3.9753)  Time: 0.748s,  171.23/s  (0.759s,  168.60/s)  LR: 1.000e-01  Data: 0.000 (0.005)
Train: 0 [ 250/390 ( 64%)]  Loss:  3.352514 (3.8715)  Time: 0.748s,  171.05/s  (0.757s,  169.11/s)  LR: 1.000e-01  Data: 0.000 (0.004)
Train: 0 [ 300/390 ( 77%)]  Loss:  3.540166 (3.8242)  Time: 0.747s,  171.24/s  (0.755s,  169.45/s)  LR: 1.000e-01  Data: 0.000 (0.003)
Train: 0 [ 350/390 ( 90%)]  Loss:  3.323137 (3.7616)  Time: 0.750s,  170.56/s  (0.754s,  169.69/s)  LR: 1.000e-01  Data: 0.003 (0.003)
Train: 0 [ 389/390 (100%)]  Loss:  3.152355 (3.6939)  Time: 0.748s,  171.20/s  (0.754s,  169.85/s)  LR: 1.000e-01  Data: 0.000 (0.003)
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Test: [   0/78]  Time: 1.315 (1.315)  DataTime: 0.906 (0.906)  Loss:  2.5605 (2.5605)  Acc@1: 34.3750 (34.3750)  Acc@5: 66.4062 (66.4062)
Test: [  50/78]  Time: 0.259 (0.280)  DataTime: 0.000 (0.018)  Loss:  2.7715 (2.7688)  Acc@1: 24.2188 (26.3021)  Acc@5: 60.1562 (60.9681)
Test: [  78/78]  Time: 0.043 (0.270)  DataTime: 0.000 (0.012)  Loss:  2.8809 (2.7790)  Acc@1: 25.0000 (26.5500)  Acc@5: 56.2500 (60.8200)
tb
loss  0 : 3.867153125 0
loss  1 : 2.85515625 0
loss  2 : 3.4519 0
loss  3 : 3.015428125 0
{'loss_different_0': 3.867153125, 'loss_different_1': 2.85515625, 'loss_different_2': 3.4519, 'loss_different_3': 3.015428125}
Top1:  26.55
Saving model to ./output/train/20250828-114037-pretrain_cifar_100_using_resuming_cifar_10/pretrain_cifar_100_using_resuming_cifar_10_0.pt
Train: 1 [   0/390 (  0%)]  Loss:  3.310162 (3.3102)  Time: 1.908s,   67.09/s  (1.908s,   67.09/s)  LR: 9.999e-02  Data: 1.160 (1.160)
Train: 1 [  50/390 ( 13%)]  Loss:  2.929724 (3.1199)  Time: 0.747s,  171.36/s  (0.778s,  164.61/s)  LR: 9.999e-02  Data: 0.000 (0.023)
Train: 1 [ 100/390 ( 26%)]  Loss:  2.968809 (3.0696)  Time: 0.747s,  171.30/s  (0.763s,  167.83/s)  LR: 9.999e-02  Data: 0.000 (0.012)
Train: 1 [ 150/390 ( 39%)]  Loss:  3.173056 (3.0954)  Time: 0.747s,  171.27/s  (0.758s,  168.95/s)  LR: 9.999e-02  Data: 0.000 (0.008)
Train: 1 [ 200/390 ( 51%)]  Loss:  3.113653 (3.0991)  Time: 0.747s,  171.36/s  (0.755s,  169.51/s)  LR: 9.999e-02  Data: 0.000 (0.006)
Train: 1 [ 250/390 ( 64%)]  Loss:  2.767208 (3.0438)  Time: 0.747s,  171.40/s  (0.754s,  169.86/s)  LR: 9.999e-02  Data: 0.000 (0.005)
Train: 1 [ 300/390 ( 77%)]  Loss:  3.645672 (3.1298)  Time: 0.748s,  171.07/s  (0.753s,  170.09/s)  LR: 9.999e-02  Data: 0.000 (0.004)
Train: 1 [ 350/390 ( 90%)]  Loss:  2.705380 (3.0767)  Time: 0.749s,  170.93/s  (0.752s,  170.25/s)  LR: 9.999e-02  Data: 0.003 (0.004)
Train: 1 [ 389/390 (100%)]  Loss:  3.207402 (3.0912)  Time: 0.747s,  171.34/s  (0.751s,  170.36/s)  LR: 9.999e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.404 (1.404)  DataTime: 1.076 (1.076)  Loss:  2.3926 (2.3926)  Acc@1: 32.8125 (32.8125)  Acc@5: 70.3125 (70.3125)
Test: [  50/78]  Time: 0.259 (0.281)  DataTime: 0.000 (0.021)  Loss:  2.5254 (2.4138)  Acc@1: 35.1562 (34.6814)  Acc@5: 64.0625 (69.7763)
Test: [  78/78]  Time: 0.035 (0.271)  DataTime: 0.000 (0.014)  Loss:  2.4062 (2.4227)  Acc@1: 37.5000 (34.5900)  Acc@5: 62.5000 (69.3800)
tb
loss  0 : 3.673084375 1
loss  1 : 2.562390625 1
loss  2 : 3.0290875 1
loss  3 : 2.644684375 1
{'loss_different_0': 3.673084375, 'loss_different_1': 2.562390625, 'loss_different_2': 3.0290875, 'loss_different_3': 2.644684375}
Top1:  34.59
Train: 2 [   0/390 (  0%)]  Loss:  2.993225 (2.9932)  Time: 1.923s,   66.55/s  (1.923s,   66.55/s)  LR: 9.998e-02  Data: 1.175 (1.175)
Train: 2 [  50/390 ( 13%)]  Loss:  2.499804 (2.7465)  Time: 0.748s,  171.15/s  (0.771s,  166.12/s)  LR: 9.998e-02  Data: 0.000 (0.023)
Train: 2 [ 100/390 ( 26%)]  Loss:  3.028010 (2.8403)  Time: 0.747s,  171.28/s  (0.759s,  168.63/s)  LR: 9.998e-02  Data: 0.000 (0.012)
Train: 2 [ 150/390 ( 39%)]  Loss:  4.248846 (3.1925)  Time: 0.748s,  171.23/s  (0.755s,  169.49/s)  LR: 9.998e-02  Data: 0.000 (0.008)
Train: 2 [ 200/390 ( 51%)]  Loss:  2.731527 (3.1003)  Time: 0.748s,  171.15/s  (0.755s,  169.58/s)  LR: 9.998e-02  Data: 0.000 (0.006)
Train: 2 [ 250/390 ( 64%)]  Loss:  2.727022 (3.0381)  Time: 0.748s,  171.13/s  (0.753s,  169.91/s)  LR: 9.998e-02  Data: 0.000 (0.005)
Train: 2 [ 300/390 ( 77%)]  Loss:  2.596797 (2.9750)  Time: 0.747s,  171.28/s  (0.752s,  170.13/s)  LR: 9.998e-02  Data: 0.000 (0.004)
Train: 2 [ 350/390 ( 90%)]  Loss:  4.162446 (3.1235)  Time: 0.750s,  170.70/s  (0.752s,  170.29/s)  LR: 9.998e-02  Data: 0.003 (0.004)
Train: 2 [ 389/390 (100%)]  Loss:  3.942435 (3.2145)  Time: 0.747s,  171.35/s  (0.751s,  170.39/s)  LR: 9.998e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.132 (1.132)  DataTime: 0.851 (0.851)  Loss:  2.2676 (2.2676)  Acc@1: 39.8438 (39.8438)  Acc@5: 75.0000 (75.0000)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.017)  Loss:  2.3457 (2.3699)  Acc@1: 37.5000 (36.5196)  Acc@5: 70.3125 (71.0172)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  2.2188 (2.3810)  Acc@1: 31.2500 (36.1000)  Acc@5: 81.2500 (70.5800)
tb
loss  0 : 3.93493125 2
loss  1 : 2.4644125 2
loss  2 : 3.447521875 2
loss  3 : 2.52619375 2
{'loss_different_0': 3.93493125, 'loss_different_1': 2.4644125, 'loss_different_2': 3.447521875, 'loss_different_3': 2.52619375}
Top1:  36.1
Train: 3 [   0/390 (  0%)]  Loss:  2.771781 (2.7718)  Time: 1.305s,   98.08/s  (1.305s,   98.08/s)  LR: 9.994e-02  Data: 0.451 (0.451)
Train: 3 [  50/390 ( 13%)]  Loss:  2.558725 (2.6653)  Time: 0.748s,  171.06/s  (0.758s,  168.77/s)  LR: 9.994e-02  Data: 0.000 (0.009)
Train: 3 [ 100/390 ( 26%)]  Loss:  2.520972 (2.6172)  Time: 0.748s,  171.13/s  (0.753s,  169.99/s)  LR: 9.994e-02  Data: 0.000 (0.005)
Train: 3 [ 150/390 ( 39%)]  Loss:  2.631451 (2.6207)  Time: 0.747s,  171.33/s  (0.751s,  170.40/s)  LR: 9.994e-02  Data: 0.000 (0.003)
Train: 3 [ 200/390 ( 51%)]  Loss:  2.336753 (2.5639)  Time: 0.748s,  171.12/s  (0.750s,  170.61/s)  LR: 9.994e-02  Data: 0.000 (0.003)
Train: 3 [ 250/390 ( 64%)]  Loss:  2.496258 (2.5527)  Time: 0.746s,  171.47/s  (0.750s,  170.74/s)  LR: 9.994e-02  Data: 0.000 (0.002)
Train: 3 [ 300/390 ( 77%)]  Loss:  2.229088 (2.5064)  Time: 0.748s,  171.20/s  (0.749s,  170.83/s)  LR: 9.994e-02  Data: 0.000 (0.002)
Train: 3 [ 350/390 ( 90%)]  Loss:  4.279082 (2.7280)  Time: 0.750s,  170.72/s  (0.749s,  170.89/s)  LR: 9.994e-02  Data: 0.003 (0.002)
Train: 3 [ 389/390 (100%)]  Loss:  2.219684 (2.6715)  Time: 0.746s,  171.48/s  (0.749s,  170.93/s)  LR: 9.994e-02  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.281 (1.281)  DataTime: 0.946 (0.946)  Loss:  2.1328 (2.1328)  Acc@1: 40.6250 (40.6250)  Acc@5: 75.0000 (75.0000)
Test: [  50/78]  Time: 0.259 (0.279)  DataTime: 0.000 (0.019)  Loss:  2.1523 (2.1117)  Acc@1: 45.3125 (41.7892)  Acc@5: 75.0000 (76.9761)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.012)  Loss:  1.9141 (2.1126)  Acc@1: 43.7500 (42.1100)  Acc@5: 81.2500 (76.8000)
tb
loss  0 : 3.555903125 3
loss  1 : 2.21954375 3
loss  2 : 2.7123 3
loss  3 : 2.557078125 3
{'loss_different_0': 3.555903125, 'loss_different_1': 2.21954375, 'loss_different_2': 2.7123, 'loss_different_3': 2.557078125}
Top1:  42.11
Train: 4 [   0/390 (  0%)]  Loss:  3.120396 (3.1204)  Time: 1.944s,   65.85/s  (1.944s,   65.85/s)  LR: 9.990e-02  Data: 1.193 (1.193)
Train: 4 [  50/390 ( 13%)]  Loss:  2.447237 (2.7838)  Time: 0.747s,  171.41/s  (0.771s,  166.05/s)  LR: 9.990e-02  Data: 0.000 (0.024)
Train: 4 [ 100/390 ( 26%)]  Loss:  2.400861 (2.6562)  Time: 0.748s,  171.13/s  (0.759s,  168.60/s)  LR: 9.990e-02  Data: 0.000 (0.012)
Train: 4 [ 150/390 ( 39%)]  Loss:  2.188665 (2.5393)  Time: 0.747s,  171.36/s  (0.757s,  169.16/s)  LR: 9.990e-02  Data: 0.000 (0.008)
Train: 4 [ 200/390 ( 51%)]  Loss:  4.086048 (2.8486)  Time: 0.747s,  171.24/s  (0.754s,  169.71/s)  LR: 9.990e-02  Data: 0.001 (0.006)
Train: 4 [ 250/390 ( 64%)]  Loss:  2.459174 (2.7837)  Time: 0.747s,  171.28/s  (0.753s,  170.04/s)  LR: 9.990e-02  Data: 0.000 (0.005)
Train: 4 [ 300/390 ( 77%)]  Loss:  2.422961 (2.7322)  Time: 0.747s,  171.42/s  (0.752s,  170.26/s)  LR: 9.990e-02  Data: 0.000 (0.004)
Train: 4 [ 350/390 ( 90%)]  Loss:  2.594073 (2.7149)  Time: 0.749s,  170.96/s  (0.751s,  170.41/s)  LR: 9.990e-02  Data: 0.002 (0.004)
Train: 4 [ 389/390 (100%)]  Loss:  2.611161 (2.7034)  Time: 0.747s,  171.44/s  (0.751s,  170.51/s)  LR: 9.990e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.985 (0.985)  DataTime: 0.660 (0.660)  Loss:  2.0137 (2.0137)  Acc@1: 49.2188 (49.2188)  Acc@5: 78.1250 (78.1250)
Test: [  50/78]  Time: 0.258 (0.275)  DataTime: 0.000 (0.015)  Loss:  2.2051 (2.1383)  Acc@1: 41.4062 (42.6777)  Acc@5: 70.3125 (75.5668)
Test: [  78/78]  Time: 0.034 (0.266)  DataTime: 0.000 (0.010)  Loss:  2.4570 (2.1483)  Acc@1: 37.5000 (42.3400)  Acc@5: 68.7500 (75.1600)
tb
loss  0 : 3.804740625 4
loss  1 : 2.2623375 4
loss  2 : 2.941225 4
loss  3 : 2.527059375 4
{'loss_different_0': 3.804740625, 'loss_different_1': 2.2623375, 'loss_different_2': 2.941225, 'loss_different_3': 2.527059375}
Top1:  42.34
Train: 5 [   0/390 (  0%)]  Loss:  2.427297 (2.4273)  Time: 1.472s,   86.96/s  (1.472s,   86.96/s)  LR: 9.985e-02  Data: 0.668 (0.668)
Train: 5 [  50/390 ( 13%)]  Loss:  2.550688 (2.4890)  Time: 0.747s,  171.44/s  (0.761s,  168.17/s)  LR: 9.985e-02  Data: 0.000 (0.013)
Train: 5 [ 100/390 ( 26%)]  Loss:  2.256234 (2.4114)  Time: 0.747s,  171.36/s  (0.754s,  169.74/s)  LR: 9.985e-02  Data: 0.000 (0.007)
Train: 5 [ 150/390 ( 39%)]  Loss:  2.302350 (2.3841)  Time: 0.747s,  171.27/s  (0.752s,  170.27/s)  LR: 9.985e-02  Data: 0.000 (0.005)
Train: 5 [ 200/390 ( 51%)]  Loss:  2.232987 (2.3539)  Time: 0.747s,  171.40/s  (0.751s,  170.54/s)  LR: 9.985e-02  Data: 0.000 (0.004)
Train: 5 [ 250/390 ( 64%)]  Loss:  3.760930 (2.5884)  Time: 0.748s,  171.12/s  (0.750s,  170.70/s)  LR: 9.985e-02  Data: 0.000 (0.003)
Train: 5 [ 300/390 ( 77%)]  Loss:  2.163410 (2.5277)  Time: 0.747s,  171.30/s  (0.750s,  170.59/s)  LR: 9.985e-02  Data: 0.000 (0.003)
Train: 5 [ 350/390 ( 90%)]  Loss:  2.910736 (2.5756)  Time: 0.749s,  170.91/s  (0.750s,  170.70/s)  LR: 9.985e-02  Data: 0.002 (0.002)
Train: 5 [ 389/390 (100%)]  Loss:  2.365581 (2.5522)  Time: 0.746s,  171.48/s  (0.750s,  170.77/s)  LR: 9.985e-02  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.107 (1.107)  DataTime: 0.839 (0.839)  Loss:  1.9980 (1.9980)  Acc@1: 49.2188 (49.2188)  Acc@5: 78.9062 (78.9062)
Test: [  50/78]  Time: 0.258 (0.275)  DataTime: 0.000 (0.017)  Loss:  2.2520 (2.0870)  Acc@1: 39.0625 (44.2862)  Acc@5: 70.3125 (75.4289)
Test: [  78/78]  Time: 0.034 (0.266)  DataTime: 0.000 (0.011)  Loss:  2.4160 (2.1029)  Acc@1: 31.2500 (44.3000)  Acc@5: 75.0000 (75.1900)
tb
loss  0 : 3.480503125 5
loss  1 : 2.278928125 5
loss  2 : 2.87714375 5
loss  3 : 2.591015625 5
{'loss_different_0': 3.480503125, 'loss_different_1': 2.278928125, 'loss_different_2': 2.87714375, 'loss_different_3': 2.591015625}
Top1:  44.3
Train: 6 [   0/390 (  0%)]  Loss:  2.443829 (2.4438)  Time: 1.667s,   76.78/s  (1.667s,   76.78/s)  LR: 9.978e-02  Data: 0.884 (0.884)
Train: 6 [  50/390 ( 13%)]  Loss:  2.295218 (2.3695)  Time: 0.747s,  171.28/s  (0.765s,  167.30/s)  LR: 9.978e-02  Data: 0.000 (0.018)
Train: 6 [ 100/390 ( 26%)]  Loss:  3.471826 (2.7370)  Time: 0.747s,  171.27/s  (0.756s,  169.29/s)  LR: 9.978e-02  Data: 0.000 (0.009)
Train: 6 [ 150/390 ( 39%)]  Loss:  2.575370 (2.6966)  Time: 0.747s,  171.31/s  (0.753s,  169.96/s)  LR: 9.978e-02  Data: 0.000 (0.006)
Train: 6 [ 200/390 ( 51%)]  Loss:  2.421154 (2.6415)  Time: 0.747s,  171.46/s  (0.752s,  170.30/s)  LR: 9.978e-02  Data: 0.000 (0.005)
Train: 6 [ 250/390 ( 64%)]  Loss:  3.884937 (2.8487)  Time: 0.746s,  171.55/s  (0.751s,  170.52/s)  LR: 9.978e-02  Data: 0.000 (0.004)
Train: 6 [ 300/390 ( 77%)]  Loss:  2.376554 (2.7813)  Time: 0.747s,  171.30/s  (0.750s,  170.64/s)  LR: 9.978e-02  Data: 0.000 (0.003)
Train: 6 [ 350/390 ( 90%)]  Loss:  2.326995 (2.7245)  Time: 0.750s,  170.73/s  (0.750s,  170.74/s)  LR: 9.978e-02  Data: 0.003 (0.003)
Train: 6 [ 389/390 (100%)]  Loss:  2.874298 (2.7411)  Time: 0.747s,  171.43/s  (0.749s,  170.80/s)  LR: 9.978e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.864 (0.864)  DataTime: 0.568 (0.568)  Loss:  1.9688 (1.9688)  Acc@1: 48.4375 (48.4375)  Acc@5: 77.3438 (77.3438)
Test: [  50/78]  Time: 0.259 (0.272)  DataTime: 0.000 (0.011)  Loss:  2.1387 (2.0088)  Acc@1: 42.1875 (44.8376)  Acc@5: 71.0938 (77.7267)
Test: [  78/78]  Time: 0.035 (0.265)  DataTime: 0.000 (0.007)  Loss:  2.1230 (2.0188)  Acc@1: 37.5000 (44.7000)  Acc@5: 81.2500 (77.4900)
tb
loss  0 : 3.1640625 6
loss  1 : 2.198221875 6
loss  2 : 2.7722125 6
loss  3 : 2.5086625 6
{'loss_different_0': 3.1640625, 'loss_different_1': 2.198221875, 'loss_different_2': 2.7722125, 'loss_different_3': 2.5086625}
Top1:  44.7
Train: 7 [   0/390 (  0%)]  Loss:  2.428732 (2.4287)  Time: 1.552s,   82.50/s  (1.552s,   82.50/s)  LR: 9.970e-02  Data: 0.786 (0.786)
Train: 7 [  50/390 ( 13%)]  Loss:  2.236312 (2.3325)  Time: 0.747s,  171.24/s  (0.763s,  167.72/s)  LR: 9.970e-02  Data: 0.000 (0.016)
Train: 7 [ 100/390 ( 26%)]  Loss:  2.603823 (2.4230)  Time: 0.747s,  171.32/s  (0.755s,  169.47/s)  LR: 9.970e-02  Data: 0.000 (0.008)
Train: 7 [ 150/390 ( 39%)]  Loss:  2.356409 (2.4063)  Time: 0.747s,  171.33/s  (0.753s,  170.08/s)  LR: 9.970e-02  Data: 0.000 (0.006)
Train: 7 [ 200/390 ( 51%)]  Loss:  2.414062 (2.4079)  Time: 0.747s,  171.25/s  (0.752s,  170.11/s)  LR: 9.970e-02  Data: 0.000 (0.004)
Train: 7 [ 250/390 ( 64%)]  Loss:  2.515432 (2.4258)  Time: 0.747s,  171.41/s  (0.751s,  170.35/s)  LR: 9.970e-02  Data: 0.000 (0.003)
Train: 7 [ 300/390 ( 77%)]  Loss:  2.300023 (2.4078)  Time: 0.747s,  171.32/s  (0.751s,  170.50/s)  LR: 9.970e-02  Data: 0.000 (0.003)
Train: 7 [ 350/390 ( 90%)]  Loss:  2.914492 (2.4712)  Time: 0.750s,  170.72/s  (0.750s,  170.61/s)  LR: 9.970e-02  Data: 0.003 (0.003)
Train: 7 [ 389/390 (100%)]  Loss:  2.308160 (2.4530)  Time: 0.747s,  171.37/s  (0.750s,  170.69/s)  LR: 9.970e-02  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.286 (1.286)  DataTime: 1.020 (1.020)  Loss:  1.7480 (1.7480)  Acc@1: 50.7812 (50.7812)  Acc@5: 82.8125 (82.8125)
Test: [  50/78]  Time: 0.258 (0.279)  DataTime: 0.000 (0.020)  Loss:  2.1406 (1.8606)  Acc@1: 43.7500 (48.6060)  Acc@5: 74.2188 (80.1471)
Test: [  78/78]  Time: 0.034 (0.269)  DataTime: 0.000 (0.013)  Loss:  2.1504 (1.8633)  Acc@1: 37.5000 (48.6900)  Acc@5: 75.0000 (80.0700)
tb
loss  0 : 3.3280125 7
loss  1 : 2.00705 7
loss  2 : 2.587671875 7
loss  3 : 2.230084375 7
{'loss_different_0': 3.3280125, 'loss_different_1': 2.00705, 'loss_different_2': 2.587671875, 'loss_different_3': 2.230084375}
Top1:  48.69
Train: 8 [   0/390 (  0%)]  Loss:  2.477045 (2.4770)  Time: 1.779s,   71.95/s  (1.779s,   71.95/s)  LR: 9.961e-02  Data: 1.032 (1.032)
Train: 8 [  50/390 ( 13%)]  Loss:  2.389780 (2.4334)  Time: 0.748s,  171.20/s  (0.767s,  166.79/s)  LR: 9.961e-02  Data: 0.000 (0.021)
Train: 8 [ 100/390 ( 26%)]  Loss:  2.319496 (2.3954)  Time: 0.748s,  171.14/s  (0.757s,  168.99/s)  LR: 9.961e-02  Data: 0.000 (0.011)
Train: 8 [ 150/390 ( 39%)]  Loss:  4.007804 (2.7985)  Time: 0.747s,  171.34/s  (0.754s,  169.75/s)  LR: 9.961e-02  Data: 0.000 (0.007)
Train: 8 [ 200/390 ( 51%)]  Loss:  2.201659 (2.6792)  Time: 0.747s,  171.28/s  (0.752s,  170.13/s)  LR: 9.961e-02  Data: 0.000 (0.005)
Train: 8 [ 250/390 ( 64%)]  Loss:  2.106737 (2.5838)  Time: 0.746s,  171.47/s  (0.751s,  170.35/s)  LR: 9.961e-02  Data: 0.000 (0.004)
Train: 8 [ 300/390 ( 77%)]  Loss:  2.207901 (2.5301)  Time: 0.747s,  171.31/s  (0.751s,  170.52/s)  LR: 9.961e-02  Data: 0.000 (0.004)
Train: 8 [ 350/390 ( 90%)]  Loss:  2.213542 (2.4905)  Time: 0.749s,  170.99/s  (0.751s,  170.46/s)  LR: 9.961e-02  Data: 0.002 (0.003)
Train: 8 [ 389/390 (100%)]  Loss:  2.566420 (2.4989)  Time: 0.747s,  171.46/s  (0.750s,  170.56/s)  LR: 9.961e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.977 (0.977)  DataTime: 0.677 (0.677)  Loss:  1.7119 (1.7119)  Acc@1: 51.5625 (51.5625)  Acc@5: 81.2500 (81.2500)
Test: [  50/78]  Time: 0.258 (0.273)  DataTime: 0.000 (0.013)  Loss:  2.0352 (1.8616)  Acc@1: 45.3125 (47.8094)  Acc@5: 78.9062 (80.8058)
Test: [  78/78]  Time: 0.034 (0.265)  DataTime: 0.000 (0.009)  Loss:  2.0762 (1.8703)  Acc@1: 50.0000 (47.9100)  Acc@5: 75.0000 (80.4800)
tb
loss  0 : 3.542871875 8
loss  1 : 1.988546875 8
loss  2 : 2.594128125 8
loss  3 : 2.19358125 8
{'loss_different_0': 3.542871875, 'loss_different_1': 1.988546875, 'loss_different_2': 2.594128125, 'loss_different_3': 2.19358125}
Top1:  47.91
Train: 9 [   0/390 (  0%)]  Loss:  2.148049 (2.1480)  Time: 1.823s,   70.20/s  (1.823s,   70.20/s)  LR: 9.950e-02  Data: 1.073 (1.073)
Train: 9 [  50/390 ( 13%)]  Loss:  3.401981 (2.7750)  Time: 0.747s,  171.40/s  (0.768s,  166.68/s)  LR: 9.950e-02  Data: 0.000 (0.021)
Train: 9 [ 100/390 ( 26%)]  Loss:  2.133232 (2.5611)  Time: 0.747s,  171.46/s  (0.758s,  168.97/s)  LR: 9.950e-02  Data: 0.000 (0.011)
Train: 9 [ 150/390 ( 39%)]  Loss:  2.254086 (2.4843)  Time: 0.747s,  171.32/s  (0.754s,  169.75/s)  LR: 9.950e-02  Data: 0.000 (0.007)
Train: 9 [ 200/390 ( 51%)]  Loss:  3.898413 (2.7672)  Time: 0.747s,  171.25/s  (0.752s,  170.13/s)  LR: 9.950e-02  Data: 0.000 (0.006)
Train: 9 [ 250/390 ( 64%)]  Loss:  2.045369 (2.6469)  Time: 0.747s,  171.40/s  (0.751s,  170.37/s)  LR: 9.950e-02  Data: 0.000 (0.005)
Train: 9 [ 300/390 ( 77%)]  Loss:  2.028532 (2.5585)  Time: 0.747s,  171.38/s  (0.751s,  170.53/s)  LR: 9.950e-02  Data: 0.000 (0.004)
Train: 9 [ 350/390 ( 90%)]  Loss:  2.371830 (2.5352)  Time: 0.749s,  170.92/s  (0.750s,  170.65/s)  LR: 9.950e-02  Data: 0.003 (0.003)
Train: 9 [ 389/390 (100%)]  Loss:  2.042619 (2.4805)  Time: 0.748s,  171.21/s  (0.750s,  170.72/s)  LR: 9.950e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.284 (1.284)  DataTime: 1.024 (1.024)  Loss:  1.8877 (1.8877)  Acc@1: 53.9062 (53.9062)  Acc@5: 82.0312 (82.0312)
Test: [  50/78]  Time: 0.258 (0.279)  DataTime: 0.000 (0.020)  Loss:  1.8867 (1.8464)  Acc@1: 52.3438 (49.2953)  Acc@5: 78.1250 (80.9589)
Test: [  78/78]  Time: 0.035 (0.269)  DataTime: 0.000 (0.013)  Loss:  1.5059 (1.8499)  Acc@1: 56.2500 (49.8000)  Acc@5: 81.2500 (80.5100)
tb
loss  0 : 3.027340625 9
loss  1 : 1.965765625 9
loss  2 : 2.5023109375 9
loss  3 : 2.32083125 9
{'loss_different_0': 3.027340625, 'loss_different_1': 1.965765625, 'loss_different_2': 2.5023109375, 'loss_different_3': 2.32083125}
Top1:  49.8
Train: 10 [   0/390 (  0%)]  Loss:  1.884560 (1.8846)  Time: 1.407s,   90.97/s  (1.407s,   90.97/s)  LR: 9.938e-02  Data: 0.615 (0.615)
Train: 10 [  50/390 ( 13%)]  Loss:  3.128143 (2.5064)  Time: 0.748s,  171.23/s  (0.760s,  168.41/s)  LR: 9.938e-02  Data: 0.000 (0.012)
Train: 10 [ 100/390 ( 26%)]  Loss:  2.749511 (2.5874)  Time: 0.747s,  171.37/s  (0.754s,  169.83/s)  LR: 9.938e-02  Data: 0.000 (0.006)
Train: 10 [ 150/390 ( 39%)]  Loss:  2.144580 (2.4767)  Time: 0.747s,  171.42/s  (0.752s,  170.32/s)  LR: 9.938e-02  Data: 0.000 (0.004)
Train: 10 [ 200/390 ( 51%)]  Loss:  3.175765 (2.6165)  Time: 0.747s,  171.45/s  (0.750s,  170.57/s)  LR: 9.938e-02  Data: 0.000 (0.003)
Train: 10 [ 250/390 ( 64%)]  Loss:  2.087860 (2.5284)  Time: 0.748s,  171.13/s  (0.751s,  170.51/s)  LR: 9.938e-02  Data: 0.000 (0.003)
Train: 10 [ 300/390 ( 77%)]  Loss:  2.081704 (2.4646)  Time: 0.747s,  171.41/s  (0.750s,  170.65/s)  LR: 9.938e-02  Data: 0.000 (0.002)
Train: 10 [ 350/390 ( 90%)]  Loss:  2.517907 (2.4713)  Time: 0.749s,  170.84/s  (0.750s,  170.75/s)  LR: 9.938e-02  Data: 0.003 (0.002)
Train: 10 [ 389/390 (100%)]  Loss:  2.112646 (2.4314)  Time: 0.747s,  171.42/s  (0.749s,  170.81/s)  LR: 9.938e-02  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.120 (1.120)  DataTime: 0.819 (0.819)  Loss:  1.7969 (1.7969)  Acc@1: 50.0000 (50.0000)  Acc@5: 83.5938 (83.5938)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.016)  Loss:  1.9990 (1.7748)  Acc@1: 43.7500 (51.2255)  Acc@5: 77.3438 (81.9087)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.9648 (1.7750)  Acc@1: 43.7500 (50.9600)  Acc@5: 75.0000 (81.7300)
tb
loss  0 : 3.052409375 10
loss  1 : 1.89946875 10
loss  2 : 2.410265625 10
loss  3 : 2.061296875 10
{'loss_different_0': 3.052409375, 'loss_different_1': 1.89946875, 'loss_different_2': 2.410265625, 'loss_different_3': 2.061296875}
Top1:  50.96
Saving model to ./output/train/20250828-114037-pretrain_cifar_100_using_resuming_cifar_10/pretrain_cifar_100_using_resuming_cifar_10_10.pt
Train: 11 [   0/390 (  0%)]  Loss:  2.357448 (2.3574)  Time: 1.349s,   94.89/s  (1.349s,   94.89/s)  LR: 9.926e-02  Data: 0.577 (0.577)
Train: 11 [  50/390 ( 13%)]  Loss:  2.288714 (2.3231)  Time: 0.747s,  171.32/s  (0.759s,  168.63/s)  LR: 9.926e-02  Data: 0.000 (0.012)
Train: 11 [ 100/390 ( 26%)]  Loss:  2.206580 (2.2842)  Time: 0.747s,  171.33/s  (0.753s,  169.95/s)  LR: 9.926e-02  Data: 0.000 (0.006)
Train: 11 [ 150/390 ( 39%)]  Loss:  2.177165 (2.2575)  Time: 0.747s,  171.30/s  (0.751s,  170.39/s)  LR: 9.926e-02  Data: 0.000 (0.004)
Train: 11 [ 200/390 ( 51%)]  Loss:  3.326543 (2.4713)  Time: 0.747s,  171.24/s  (0.750s,  170.61/s)  LR: 9.926e-02  Data: 0.000 (0.003)
Train: 11 [ 250/390 ( 64%)]  Loss:  3.287470 (2.6073)  Time: 0.747s,  171.28/s  (0.750s,  170.75/s)  LR: 9.926e-02  Data: 0.000 (0.003)
Train: 11 [ 300/390 ( 77%)]  Loss:  2.017752 (2.5231)  Time: 0.747s,  171.27/s  (0.749s,  170.84/s)  LR: 9.926e-02  Data: 0.000 (0.002)
Train: 11 [ 350/390 ( 90%)]  Loss:  2.233614 (2.4869)  Time: 0.750s,  170.57/s  (0.749s,  170.91/s)  LR: 9.926e-02  Data: 0.003 (0.002)
Train: 11 [ 389/390 (100%)]  Loss:  3.706454 (2.6224)  Time: 0.747s,  171.36/s  (0.749s,  170.95/s)  LR: 9.926e-02  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.100 (1.100)  DataTime: 0.791 (0.791)  Loss:  1.7510 (1.7510)  Acc@1: 54.6875 (54.6875)  Acc@5: 79.6875 (79.6875)
Test: [  50/78]  Time: 0.259 (0.275)  DataTime: 0.000 (0.016)  Loss:  2.1074 (1.9128)  Acc@1: 47.6562 (48.8817)  Acc@5: 79.6875 (79.4424)
Test: [  78/78]  Time: 0.035 (0.267)  DataTime: 0.000 (0.010)  Loss:  1.8018 (1.9112)  Acc@1: 56.2500 (48.8000)  Acc@5: 81.2500 (79.5200)
tb
loss  0 : 3.351571875 11
loss  1 : 2.0732359375 11
loss  2 : 2.6254125 11
loss  3 : 2.343628125 11
{'loss_different_0': 3.351571875, 'loss_different_1': 2.0732359375, 'loss_different_2': 2.6254125, 'loss_different_3': 2.343628125}
Top1:  48.8
Train: 12 [   0/390 (  0%)]  Loss:  2.079322 (2.0793)  Time: 1.811s,   70.70/s  (1.811s,   70.70/s)  LR: 9.911e-02  Data: 1.061 (1.061)
Train: 12 [  50/390 ( 13%)]  Loss:  2.523686 (2.3015)  Time: 0.748s,  171.19/s  (0.773s,  165.49/s)  LR: 9.911e-02  Data: 0.000 (0.021)
Train: 12 [ 100/390 ( 26%)]  Loss:  2.349094 (2.3174)  Time: 0.748s,  171.19/s  (0.760s,  168.32/s)  LR: 9.911e-02  Data: 0.000 (0.011)
Train: 12 [ 150/390 ( 39%)]  Loss:  3.434036 (2.5965)  Time: 0.747s,  171.37/s  (0.756s,  169.31/s)  LR: 9.911e-02  Data: 0.000 (0.007)
Train: 12 [ 200/390 ( 51%)]  Loss:  2.268144 (2.5309)  Time: 0.748s,  171.22/s  (0.754s,  169.80/s)  LR: 9.911e-02  Data: 0.000 (0.006)
Train: 12 [ 250/390 ( 64%)]  Loss:  2.196765 (2.4752)  Time: 0.747s,  171.32/s  (0.752s,  170.10/s)  LR: 9.911e-02  Data: 0.000 (0.005)
Train: 12 [ 300/390 ( 77%)]  Loss:  2.148254 (2.4285)  Time: 0.747s,  171.24/s  (0.752s,  170.31/s)  LR: 9.911e-02  Data: 0.000 (0.004)
Train: 12 [ 350/390 ( 90%)]  Loss:  2.214421 (2.4017)  Time: 0.749s,  170.91/s  (0.751s,  170.45/s)  LR: 9.911e-02  Data: 0.003 (0.003)
Train: 12 [ 389/390 (100%)]  Loss:  2.261802 (2.3862)  Time: 0.747s,  171.25/s  (0.751s,  170.54/s)  LR: 9.911e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.079 (1.079)  DataTime: 0.803 (0.803)  Loss:  1.6729 (1.6729)  Acc@1: 57.8125 (57.8125)  Acc@5: 83.5938 (83.5938)
Test: [  50/78]  Time: 0.258 (0.275)  DataTime: 0.000 (0.016)  Loss:  1.8662 (1.8166)  Acc@1: 50.7812 (49.9694)  Acc@5: 80.4688 (81.0509)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.010)  Loss:  2.0957 (1.8235)  Acc@1: 37.5000 (50.1300)  Acc@5: 81.2500 (80.9000)
tb
loss  0 : 3.32509375 12
loss  1 : 1.91235625 12
loss  2 : 2.44551875 12
loss  3 : 2.18475625 12
{'loss_different_0': 3.32509375, 'loss_different_1': 1.91235625, 'loss_different_2': 2.44551875, 'loss_different_3': 2.18475625}
Top1:  50.13
Train: 13 [   0/390 (  0%)]  Loss:  2.361404 (2.3614)  Time: 1.405s,   91.11/s  (1.405s,   91.11/s)  LR: 9.896e-02  Data: 0.578 (0.578)
Train: 13 [  50/390 ( 13%)]  Loss:  2.166287 (2.2638)  Time: 0.747s,  171.33/s  (0.760s,  168.42/s)  LR: 9.896e-02  Data: 0.000 (0.012)
Train: 13 [ 100/390 ( 26%)]  Loss:  2.279697 (2.2691)  Time: 0.747s,  171.38/s  (0.754s,  169.86/s)  LR: 9.896e-02  Data: 0.000 (0.006)
Train: 13 [ 150/390 ( 39%)]  Loss:  2.256298 (2.2659)  Time: 0.747s,  171.38/s  (0.751s,  170.35/s)  LR: 9.896e-02  Data: 0.000 (0.004)
Train: 13 [ 200/390 ( 51%)]  Loss:  3.378053 (2.4883)  Time: 0.747s,  171.31/s  (0.750s,  170.60/s)  LR: 9.896e-02  Data: 0.000 (0.003)
Train: 13 [ 250/390 ( 64%)]  Loss:  2.369501 (2.4685)  Time: 0.747s,  171.32/s  (0.750s,  170.75/s)  LR: 9.896e-02  Data: 0.000 (0.003)
Train: 13 [ 300/390 ( 77%)]  Loss:  2.070402 (2.4117)  Time: 0.747s,  171.35/s  (0.749s,  170.85/s)  LR: 9.896e-02  Data: 0.000 (0.002)
Train: 13 [ 350/390 ( 90%)]  Loss:  2.091549 (2.3716)  Time: 0.750s,  170.72/s  (0.749s,  170.80/s)  LR: 9.896e-02  Data: 0.003 (0.002)
Train: 13 [ 389/390 (100%)]  Loss:  2.219489 (2.3547)  Time: 0.747s,  171.33/s  (0.749s,  170.86/s)  LR: 9.896e-02  Data: 0.000 (0.002)
Test: [   0/78]  Time: 1.158 (1.158)  DataTime: 0.888 (0.888)  Loss:  1.5283 (1.5283)  Acc@1: 54.6875 (54.6875)  Acc@5: 85.9375 (85.9375)
Test: [  50/78]  Time: 0.259 (0.276)  DataTime: 0.000 (0.018)  Loss:  1.9492 (1.7247)  Acc@1: 48.4375 (52.1599)  Acc@5: 73.4375 (82.1998)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.011)  Loss:  2.0684 (1.7281)  Acc@1: 43.7500 (52.3200)  Acc@5: 68.7500 (81.9900)
tb
loss  0 : 3.02593125 13
loss  1 : 1.8474859375 13
loss  2 : 2.43308125 13
loss  3 : 2.080696875 13
{'loss_different_0': 3.02593125, 'loss_different_1': 1.8474859375, 'loss_different_2': 2.43308125, 'loss_different_3': 2.080696875}
Top1:  52.32
Train: 14 [   0/390 (  0%)]  Loss:  1.938761 (1.9388)  Time: 1.297s,   98.67/s  (1.297s,   98.67/s)  LR: 9.880e-02  Data: 0.519 (0.519)
Train: 14 [  50/390 ( 13%)]  Loss:  2.165446 (2.0521)  Time: 0.747s,  171.37/s  (0.758s,  168.88/s)  LR: 9.880e-02  Data: 0.000 (0.010)
Train: 14 [ 100/390 ( 26%)]  Loss:  1.872479 (1.9922)  Time: 0.748s,  171.19/s  (0.753s,  170.08/s)  LR: 9.880e-02  Data: 0.000 (0.005)
Train: 14 [ 150/390 ( 39%)]  Loss:  1.946594 (1.9808)  Time: 0.747s,  171.29/s  (0.751s,  170.49/s)  LR: 9.880e-02  Data: 0.000 (0.004)
Train: 14 [ 200/390 ( 51%)]  Loss:  2.406515 (2.0660)  Time: 0.747s,  171.40/s  (0.750s,  170.70/s)  LR: 9.880e-02  Data: 0.000 (0.003)
Train: 14 [ 250/390 ( 64%)]  Loss:  4.140375 (2.4117)  Time: 0.748s,  171.20/s  (0.749s,  170.83/s)  LR: 9.880e-02  Data: 0.000 (0.002)
Train: 14 [ 300/390 ( 77%)]  Loss:  1.991248 (2.3516)  Time: 0.746s,  171.67/s  (0.749s,  170.93/s)  LR: 9.880e-02  Data: 0.000 (0.002)
Train: 14 [ 350/390 ( 90%)]  Loss:  1.899678 (2.2951)  Time: 0.749s,  170.85/s  (0.749s,  171.00/s)  LR: 9.880e-02  Data: 0.003 (0.002)
Train: 14 [ 389/390 (100%)]  Loss:  2.138455 (2.2777)  Time: 0.746s,  171.56/s  (0.748s,  171.05/s)  LR: 9.880e-02  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.822 (0.822)  DataTime: 0.550 (0.550)  Loss:  1.5186 (1.5186)  Acc@1: 60.9375 (60.9375)  Acc@5: 82.8125 (82.8125)
Test: [  50/78]  Time: 0.258 (0.270)  DataTime: 0.000 (0.011)  Loss:  1.8428 (1.7019)  Acc@1: 52.3438 (52.8186)  Acc@5: 77.3438 (83.1801)
Test: [  78/78]  Time: 0.034 (0.263)  DataTime: 0.000 (0.007)  Loss:  1.8652 (1.7102)  Acc@1: 43.7500 (52.4800)  Acc@5: 81.2500 (83.2900)
tb
loss  0 : 3.08968125 14
loss  1 : 1.8277734375 14
loss  2 : 2.393621875 14
loss  3 : 2.115871875 14
{'loss_different_0': 3.08968125, 'loss_different_1': 1.8277734375, 'loss_different_2': 2.393621875, 'loss_different_3': 2.115871875}
Top1:  52.48
Train: 15 [   0/390 (  0%)]  Loss:  2.093335 (2.0933)  Time: 0.884s,  144.85/s  (0.884s,  144.85/s)  LR: 9.862e-02  Data: 0.113 (0.113)
Train: 15 [  50/390 ( 13%)]  Loss:  2.664215 (2.3788)  Time: 0.747s,  171.43/s  (0.750s,  170.73/s)  LR: 9.862e-02  Data: 0.000 (0.002)
Train: 15 [ 100/390 ( 26%)]  Loss:  2.612113 (2.4566)  Time: 0.746s,  171.51/s  (0.751s,  170.43/s)  LR: 9.862e-02  Data: 0.000 (0.001)
Train: 15 [ 150/390 ( 39%)]  Loss:  2.129603 (2.3748)  Time: 0.747s,  171.36/s  (0.750s,  170.75/s)  LR: 9.862e-02  Data: 0.000 (0.001)
Train: 15 [ 200/390 ( 51%)]  Loss:  4.009686 (2.7018)  Time: 0.746s,  171.50/s  (0.749s,  170.92/s)  LR: 9.862e-02  Data: 0.000 (0.001)
Train: 15 [ 250/390 ( 64%)]  Loss:  2.290224 (2.6332)  Time: 0.746s,  171.58/s  (0.748s,  171.03/s)  LR: 9.862e-02  Data: 0.000 (0.001)
Train: 15 [ 300/390 ( 77%)]  Loss:  2.180358 (2.5685)  Time: 0.746s,  171.62/s  (0.748s,  171.09/s)  LR: 9.862e-02  Data: 0.000 (0.001)
Train: 15 [ 350/390 ( 90%)]  Loss:  2.094612 (2.5093)  Time: 0.749s,  170.97/s  (0.748s,  171.14/s)  LR: 9.862e-02  Data: 0.003 (0.001)
Train: 15 [ 389/390 (100%)]  Loss:  2.093310 (2.4631)  Time: 0.746s,  171.48/s  (0.748s,  171.17/s)  LR: 9.862e-02  Data: 0.000 (0.001)
Test: [   0/78]  Time: 0.957 (0.957)  DataTime: 0.679 (0.679)  Loss:  1.6553 (1.6553)  Acc@1: 55.4688 (55.4688)  Acc@5: 81.2500 (81.2500)
Test: [  50/78]  Time: 0.258 (0.272)  DataTime: 0.000 (0.014)  Loss:  1.8252 (1.6932)  Acc@1: 53.1250 (53.7071)  Acc@5: 79.6875 (84.1299)
Test: [  78/78]  Time: 0.034 (0.265)  DataTime: 0.000 (0.009)  Loss:  1.2393 (1.6907)  Acc@1: 75.0000 (53.9300)  Acc@5: 87.5000 (83.9900)
tb
loss  0 : 3.27375625 15
loss  1 : 1.744153125 15
loss  2 : 2.3275609375 15
loss  3 : 1.985809375 15
{'loss_different_0': 3.27375625, 'loss_different_1': 1.744153125, 'loss_different_2': 2.3275609375, 'loss_different_3': 1.985809375}
Top1:  53.93
Train: 16 [   0/390 (  0%)]  Loss:  2.273861 (2.2739)  Time: 1.766s,   72.49/s  (1.766s,   72.49/s)  LR: 9.843e-02  Data: 1.017 (1.017)
Train: 16 [  50/390 ( 13%)]  Loss:  1.974179 (2.1240)  Time: 0.747s,  171.41/s  (0.767s,  166.96/s)  LR: 9.843e-02  Data: 0.000 (0.020)
Train: 16 [ 100/390 ( 26%)]  Loss:  2.278588 (2.1755)  Time: 0.747s,  171.30/s  (0.757s,  169.15/s)  LR: 9.843e-02  Data: 0.000 (0.010)
Train: 16 [ 150/390 ( 39%)]  Loss:  2.122286 (2.1622)  Time: 0.747s,  171.38/s  (0.753s,  169.90/s)  LR: 9.843e-02  Data: 0.000 (0.007)
Train: 16 [ 200/390 ( 51%)]  Loss:  1.981887 (2.1262)  Time: 0.747s,  171.25/s  (0.752s,  170.27/s)  LR: 9.843e-02  Data: 0.000 (0.005)
Train: 16 [ 250/390 ( 64%)]  Loss:  1.979017 (2.1016)  Time: 0.747s,  171.45/s  (0.751s,  170.50/s)  LR: 9.843e-02  Data: 0.000 (0.004)
Train: 16 [ 300/390 ( 77%)]  Loss:  2.237203 (2.1210)  Time: 0.747s,  171.36/s  (0.750s,  170.65/s)  LR: 9.843e-02  Data: 0.000 (0.004)
Train: 16 [ 350/390 ( 90%)]  Loss:  3.139335 (2.2483)  Time: 0.749s,  170.93/s  (0.750s,  170.76/s)  LR: 9.843e-02  Data: 0.003 (0.003)
Train: 16 [ 389/390 (100%)]  Loss:  1.941556 (2.2142)  Time: 0.746s,  171.53/s  (0.750s,  170.72/s)  LR: 9.843e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.155 (1.155)  DataTime: 0.884 (0.884)  Loss:  1.7148 (1.7148)  Acc@1: 57.8125 (57.8125)  Acc@5: 82.8125 (82.8125)
Test: [  50/78]  Time: 0.258 (0.276)  DataTime: 0.000 (0.018)  Loss:  1.9785 (1.7935)  Acc@1: 48.4375 (50.8732)  Acc@5: 72.6562 (82.0006)
Test: [  78/78]  Time: 0.034 (0.267)  DataTime: 0.000 (0.011)  Loss:  1.9844 (1.7994)  Acc@1: 50.0000 (50.9600)  Acc@5: 81.2500 (81.6300)
tb
loss  0 : 3.013353125 16
loss  1 : 1.97448125 16
loss  2 : 2.534996875 16
loss  3 : 2.18111875 16
{'loss_different_0': 3.013353125, 'loss_different_1': 1.97448125, 'loss_different_2': 2.534996875, 'loss_different_3': 2.18111875}
Top1:  50.96
Train: 17 [   0/390 (  0%)]  Loss:  1.907834 (1.9078)  Time: 1.760s,   72.74/s  (1.760s,   72.74/s)  LR: 9.823e-02  Data: 1.012 (1.012)
Train: 17 [  50/390 ( 13%)]  Loss:  2.866143 (2.3870)  Time: 0.747s,  171.43/s  (0.767s,  166.98/s)  LR: 9.823e-02  Data: 0.000 (0.020)
Train: 17 [ 100/390 ( 26%)]  Loss:  2.233917 (2.3360)  Time: 0.747s,  171.44/s  (0.757s,  169.16/s)  LR: 9.823e-02  Data: 0.000 (0.010)
Train: 17 [ 150/390 ( 39%)]  Loss:  3.996915 (2.7512)  Time: 0.747s,  171.24/s  (0.753s,  169.91/s)  LR: 9.823e-02  Data: 0.000 (0.007)
Train: 17 [ 200/390 ( 51%)]  Loss:  1.999381 (2.6008)  Time: 0.747s,  171.37/s  (0.752s,  170.26/s)  LR: 9.823e-02  Data: 0.000 (0.005)
Train: 17 [ 250/390 ( 64%)]  Loss:  2.125571 (2.5216)  Time: 0.747s,  171.44/s  (0.751s,  170.48/s)  LR: 9.823e-02  Data: 0.000 (0.004)
Train: 17 [ 300/390 ( 77%)]  Loss:  1.912418 (2.4346)  Time: 0.747s,  171.34/s  (0.750s,  170.63/s)  LR: 9.823e-02  Data: 0.000 (0.004)
Train: 17 [ 350/390 ( 90%)]  Loss:  2.186476 (2.4036)  Time: 0.750s,  170.78/s  (0.750s,  170.73/s)  LR: 9.823e-02  Data: 0.003 (0.003)
Train: 17 [ 389/390 (100%)]  Loss:  2.104820 (2.3704)  Time: 0.747s,  171.41/s  (0.749s,  170.79/s)  LR: 9.823e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.090 (1.090)  DataTime: 0.828 (0.828)  Loss:  1.5869 (1.5869)  Acc@1: 57.8125 (57.8125)  Acc@5: 84.3750 (84.3750)
Test: [  50/78]  Time: 0.259 (0.275)  DataTime: 0.000 (0.016)  Loss:  1.7500 (1.7210)  Acc@1: 50.7812 (52.1599)  Acc@5: 84.3750 (82.4602)
Test: [  78/78]  Time: 0.035 (0.266)  DataTime: 0.000 (0.011)  Loss:  1.5215 (1.7168)  Acc@1: 62.5000 (52.8100)  Acc@5: 81.2500 (82.4900)
tb
loss  0 : 3.265865625 17
loss  1 : 1.8359 17
loss  2 : 2.4581015625 17
loss  3 : 2.01644375 17
{'loss_different_0': 3.265865625, 'loss_different_1': 1.8359, 'loss_different_2': 2.4581015625, 'loss_different_3': 2.01644375}
Top1:  52.81
Train: 18 [   0/390 (  0%)]  Loss:  2.074427 (2.0744)  Time: 1.592s,   80.42/s  (1.592s,   80.42/s)  LR: 9.801e-02  Data: 0.835 (0.835)
Train: 18 [  50/390 ( 13%)]  Loss:  2.086139 (2.0803)  Time: 0.747s,  171.37/s  (0.764s,  167.61/s)  LR: 9.801e-02  Data: 0.000 (0.017)
Train: 18 [ 100/390 ( 26%)]  Loss:  2.397216 (2.1859)  Time: 0.747s,  171.45/s  (0.755s,  169.43/s)  LR: 9.801e-02  Data: 0.000 (0.009)
Train: 18 [ 150/390 ( 39%)]  Loss:  1.739806 (2.0744)  Time: 0.747s,  171.33/s  (0.753s,  170.06/s)  LR: 9.801e-02  Data: 0.000 (0.006)
Train: 18 [ 200/390 ( 51%)]  Loss:  3.951612 (2.4498)  Time: 0.747s,  171.31/s  (0.751s,  170.37/s)  LR: 9.801e-02  Data: 0.000 (0.004)
Train: 18 [ 250/390 ( 64%)]  Loss:  2.022815 (2.3787)  Time: 0.748s,  171.19/s  (0.750s,  170.57/s)  LR: 9.801e-02  Data: 0.000 (0.004)
Train: 18 [ 300/390 ( 77%)]  Loss:  1.747326 (2.2885)  Time: 0.747s,  171.26/s  (0.751s,  170.55/s)  LR: 9.801e-02  Data: 0.000 (0.003)
Train: 18 [ 350/390 ( 90%)]  Loss:  1.957165 (2.2471)  Time: 0.749s,  170.90/s  (0.750s,  170.66/s)  LR: 9.801e-02  Data: 0.003 (0.003)
Train: 18 [ 389/390 (100%)]  Loss:  2.031671 (2.2231)  Time: 0.747s,  171.41/s  (0.750s,  170.74/s)  LR: 9.801e-02  Data: 0.000 (0.002)
Test: [   0/78]  Time: 0.895 (0.895)  DataTime: 0.624 (0.624)  Loss:  1.6836 (1.6836)  Acc@1: 56.2500 (56.2500)  Acc@5: 84.3750 (84.3750)
Test: [  50/78]  Time: 0.258 (0.271)  DataTime: 0.000 (0.013)  Loss:  1.7832 (1.7333)  Acc@1: 43.7500 (52.2518)  Acc@5: 83.5938 (82.1691)
Test: [  78/78]  Time: 0.034 (0.264)  DataTime: 0.000 (0.008)  Loss:  1.4775 (1.7417)  Acc@1: 56.2500 (52.2400)  Acc@5: 93.7500 (81.9000)
tb
loss  0 : 3.169215625 18
loss  1 : 1.851378125 18
loss  2 : 2.4123265625 18
loss  3 : 2.094721875 18
{'loss_different_0': 3.169215625, 'loss_different_1': 1.851378125, 'loss_different_2': 2.4123265625, 'loss_different_3': 2.094721875}
Top1:  52.24
Train: 19 [   0/390 (  0%)]  Loss:  2.076026 (2.0760)  Time: 1.721s,   74.39/s  (1.721s,   74.39/s)  LR: 9.779e-02  Data: 0.971 (0.971)
Train: 19 [  50/390 ( 13%)]  Loss:  1.738786 (1.9074)  Time: 0.748s,  171.17/s  (0.766s,  167.07/s)  LR: 9.779e-02  Data: 0.000 (0.019)
Train: 19 [ 100/390 ( 26%)]  Loss:  2.062051 (1.9590)  Time: 0.748s,  171.05/s  (0.757s,  169.15/s)  LR: 9.779e-02  Data: 0.000 (0.010)
Train: 19 [ 150/390 ( 39%)]  Loss:  1.987147 (1.9660)  Time: 0.748s,  171.09/s  (0.754s,  169.87/s)  LR: 9.779e-02  Data: 0.000 (0.007)
Train: 19 [ 200/390 ( 51%)]  Loss:  2.066181 (1.9860)  Time: 0.747s,  171.40/s  (0.752s,  170.23/s)  LR: 9.779e-02  Data: 0.000 (0.005)
Train: 19 [ 250/390 ( 64%)]  Loss:  2.031121 (1.9936)  Time: 0.747s,  171.34/s  (0.751s,  170.45/s)  LR: 9.779e-02  Data: 0.000 (0.004)
Train: 19 [ 300/390 ( 77%)]  Loss:  3.156310 (2.1597)  Time: 0.747s,  171.35/s  (0.750s,  170.60/s)  LR: 9.779e-02  Data: 0.000 (0.004)
Train: 19 [ 350/390 ( 90%)]  Loss:  2.046842 (2.1456)  Time: 0.749s,  170.80/s  (0.750s,  170.70/s)  LR: 9.779e-02  Data: 0.003 (0.003)
Train: 19 [ 389/390 (100%)]  Loss:  2.372141 (2.1707)  Time: 0.747s,  171.44/s  (0.750s,  170.77/s)  LR: 9.779e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 0.881 (0.881)  DataTime: 0.610 (0.610)  Loss:  1.5967 (1.5967)  Acc@1: 57.0312 (57.0312)  Acc@5: 83.5938 (83.5938)
Test: [  50/78]  Time: 0.259 (0.272)  DataTime: 0.000 (0.013)  Loss:  1.8223 (1.7299)  Acc@1: 50.0000 (52.2978)  Acc@5: 79.6875 (82.5061)
Test: [  78/78]  Time: 0.035 (0.264)  DataTime: 0.000 (0.008)  Loss:  1.3896 (1.7300)  Acc@1: 75.0000 (52.8000)  Acc@5: 87.5000 (82.2900)
tb
loss  0 : 3.06830625 19
loss  1 : 1.86433125 19
loss  2 : 2.5221203125 19
loss  3 : 2.048509375 19
{'loss_different_0': 3.06830625, 'loss_different_1': 1.86433125, 'loss_different_2': 2.5221203125, 'loss_different_3': 2.048509375}
Top1:  52.8
Train: 20 [   0/390 (  0%)]  Loss:  2.205927 (2.2059)  Time: 1.774s,   72.16/s  (1.774s,   72.16/s)  LR: 9.755e-02  Data: 1.026 (1.026)
Train: 20 [  50/390 ( 13%)]  Loss:  2.851604 (2.5288)  Time: 0.747s,  171.27/s  (0.773s,  165.65/s)  LR: 9.755e-02  Data: 0.000 (0.020)
Train: 20 [ 100/390 ( 26%)]  Loss:  3.863858 (2.9738)  Time: 0.747s,  171.30/s  (0.760s,  168.40/s)  LR: 9.755e-02  Data: 0.000 (0.010)
Train: 20 [ 150/390 ( 39%)]  Loss:  2.050982 (2.7431)  Time: 0.747s,  171.25/s  (0.756s,  169.36/s)  LR: 9.755e-02  Data: 0.000 (0.007)
Train: 20 [ 200/390 ( 51%)]  Loss:  4.092368 (3.0129)  Time: 0.748s,  171.18/s  (0.754s,  169.84/s)  LR: 9.755e-02  Data: 0.000 (0.005)
Train: 20 [ 250/390 ( 64%)]  Loss:  2.393597 (2.9097)  Time: 0.747s,  171.37/s  (0.752s,  170.14/s)  LR: 9.755e-02  Data: 0.000 (0.004)
Train: 20 [ 300/390 ( 77%)]  Loss:  2.772867 (2.8902)  Time: 0.747s,  171.45/s  (0.751s,  170.34/s)  LR: 9.755e-02  Data: 0.000 (0.004)
Train: 20 [ 350/390 ( 90%)]  Loss:  2.140664 (2.7965)  Time: 0.750s,  170.72/s  (0.751s,  170.47/s)  LR: 9.755e-02  Data: 0.003 (0.003)
Train: 20 [ 389/390 (100%)]  Loss:  2.162503 (2.7260)  Time: 0.747s,  171.26/s  (0.750s,  170.57/s)  LR: 9.755e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.194 (1.194)  DataTime: 0.862 (0.862)  Loss:  1.5781 (1.5781)  Acc@1: 56.2500 (56.2500)  Acc@5: 85.1562 (85.1562)
Test: [  50/78]  Time: 0.258 (0.277)  DataTime: 0.000 (0.017)  Loss:  1.8848 (1.7470)  Acc@1: 53.1250 (52.8339)  Acc@5: 75.7812 (82.2457)
Test: [  78/78]  Time: 0.034 (0.268)  DataTime: 0.000 (0.011)  Loss:  2.3145 (1.7468)  Acc@1: 37.5000 (52.7900)  Acc@5: 68.7500 (82.1500)
tb
loss  0 : 3.393634375 20
loss  1 : 1.87496875 20
loss  2 : 2.3782625 20
loss  3 : 2.02679375 20
{'loss_different_0': 3.393634375, 'loss_different_1': 1.87496875, 'loss_different_2': 2.3782625, 'loss_different_3': 2.02679375}
Top1:  52.79
Saving model to ./output/train/20250828-114037-pretrain_cifar_100_using_resuming_cifar_10/pretrain_cifar_100_using_resuming_cifar_10_20.pt
Train: 21 [   0/390 (  0%)]  Loss:  2.082157 (2.0822)  Time: 1.719s,   74.46/s  (1.719s,   74.46/s)  LR: 9.730e-02  Data: 0.964 (0.964)
Train: 21 [  50/390 ( 13%)]  Loss:  2.941185 (2.5117)  Time: 0.747s,  171.24/s  (0.766s,  167.05/s)  LR: 9.730e-02  Data: 0.000 (0.019)
Train: 21 [ 100/390 ( 26%)]  Loss:  2.129113 (2.3842)  Time: 0.747s,  171.38/s  (0.757s,  169.12/s)  LR: 9.730e-02  Data: 0.000 (0.010)
Train: 21 [ 150/390 ( 39%)]  Loss:  1.970924 (2.2808)  Time: 0.747s,  171.43/s  (0.754s,  169.84/s)  LR: 9.730e-02  Data: 0.000 (0.007)
Train: 21 [ 200/390 ( 51%)]  Loss:  2.288208 (2.2823)  Time: 0.748s,  171.14/s  (0.752s,  170.20/s)  LR: 9.730e-02  Data: 0.000 (0.005)
Train: 21 [ 250/390 ( 64%)]  Loss:  2.581687 (2.3322)  Time: 0.747s,  171.30/s  (0.751s,  170.42/s)  LR: 9.730e-02  Data: 0.000 (0.004)
Train: 21 [ 300/390 ( 77%)]  Loss:  1.781250 (2.2535)  Time: 0.747s,  171.31/s  (0.750s,  170.57/s)  LR: 9.730e-02  Data: 0.000 (0.004)
Train: 21 [ 350/390 ( 90%)]  Loss:  2.062471 (2.2296)  Time: 0.749s,  170.87/s  (0.751s,  170.55/s)  LR: 9.730e-02  Data: 0.003 (0.003)
Train: 21 [ 389/390 (100%)]  Loss:  2.014181 (2.2057)  Time: 0.747s,  171.36/s  (0.750s,  170.64/s)  LR: 9.730e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.288 (1.288)  DataTime: 1.006 (1.006)  Loss:  1.5859 (1.5859)  Acc@1: 55.4688 (55.4688)  Acc@5: 80.4688 (80.4688)
Test: [  50/78]  Time: 0.258 (0.279)  DataTime: 0.000 (0.020)  Loss:  1.9170 (1.7223)  Acc@1: 45.3125 (51.6697)  Acc@5: 82.8125 (82.9810)
Test: [  78/78]  Time: 0.034 (0.269)  DataTime: 0.000 (0.013)  Loss:  1.6992 (1.7311)  Acc@1: 50.0000 (51.8900)  Acc@5: 87.5000 (83.0100)
tb
loss  0 : 3.0968 21
loss  1 : 1.9766171875 21
loss  2 : 2.42056875 21
loss  3 : 2.060875 21
{'loss_different_0': 3.0968, 'loss_different_1': 1.9766171875, 'loss_different_2': 2.42056875, 'loss_different_3': 2.060875}
Top1:  51.89
Train: 22 [   0/390 (  0%)]  Loss:  1.875501 (1.8755)  Time: 1.777s,   72.05/s  (1.777s,   72.05/s)  LR: 9.704e-02  Data: 1.029 (1.029)
Train: 22 [  50/390 ( 13%)]  Loss:  2.653150 (2.2643)  Time: 0.748s,  171.19/s  (0.767s,  166.79/s)  LR: 9.704e-02  Data: 0.000 (0.020)
Train: 22 [ 100/390 ( 26%)]  Loss:  2.148363 (2.2257)  Time: 0.747s,  171.34/s  (0.758s,  168.96/s)  LR: 9.704e-02  Data: 0.000 (0.011)
Train: 22 [ 150/390 ( 39%)]  Loss:  2.343971 (2.2552)  Time: 0.747s,  171.42/s  (0.754s,  169.71/s)  LR: 9.704e-02  Data: 0.000 (0.007)
Train: 22 [ 200/390 ( 51%)]  Loss:  2.078815 (2.2200)  Time: 0.747s,  171.34/s  (0.753s,  170.09/s)  LR: 9.704e-02  Data: 0.000 (0.005)
Train: 22 [ 250/390 ( 64%)]  Loss:  2.138810 (2.2064)  Time: 0.748s,  171.22/s  (0.751s,  170.33/s)  LR: 9.704e-02  Data: 0.000 (0.004)
Train: 22 [ 300/390 ( 77%)]  Loss:  2.129184 (2.1954)  Time: 0.747s,  171.31/s  (0.751s,  170.48/s)  LR: 9.704e-02  Data: 0.000 (0.004)
Train: 22 [ 350/390 ( 90%)]  Loss:  3.894981 (2.4078)  Time: 0.749s,  170.79/s  (0.750s,  170.59/s)  LR: 9.704e-02  Data: 0.003 (0.003)
Train: 22 [ 389/390 (100%)]  Loss:  2.034032 (2.3663)  Time: 0.748s,  171.17/s  (0.750s,  170.66/s)  LR: 9.704e-02  Data: 0.000 (0.003)
Test: [   0/78]  Time: 1.025 (1.025)  DataTime: 0.690 (0.690)  Loss:  1.5879 (1.5879)  Acc@1: 53.1250 (53.1250)  Acc@5: 82.8125 (82.8125)
Test: [  50/78]  Time: 0.259 (0.274)  DataTime: 0.000 (0.014)  Loss:  1.7490 (1.6629)  Acc@1: 52.3438 (54.4424)  Acc@5: 82.0312 (83.2414)
Test: [  78/78]  Time: 0.034 (0.266)  DataTime: 0.000 (0.009)  Loss:  1.8330 (1.6771)  Acc@1: 31.2500 (53.7000)  Acc@5: 87.5000 (83.2200)
tb
loss  0 : 3.0433125 22
loss  1 : 1.79235625 22
loss  2 : 2.398884375 22
loss  3 : 2.10804375 22
{'loss_different_0': 3.0433125, 'loss_different_1': 1.79235625, 'loss_different_2': 2.398884375, 'loss_different_3': 2.10804375}
Top1:  53.7
Train: 23 [   0/390 (  0%)]  Loss:  3.842565 (3.8426)  Time: 1.703s,   75.17/s  (1.703s,   75.17/s)  LR: 9.677e-02  Data: 0.905 (0.905)
Train: 23 [  50/390 ( 13%)]  Loss:  2.201553 (3.0221)  Time: 0.747s,  171.43/s  (0.766s,  167.09/s)  LR: 9.677e-02  Data: 0.000 (0.018)
Train: 23 [ 100/390 ( 26%)]  Loss:  2.021089 (2.6884)  Time: 0.747s,  171.25/s  (0.757s,  169.13/s)  LR: 9.677e-02  Data: 0.000 (0.009)
Train: 23 [ 150/390 ( 39%)]  Loss:  2.209280 (2.5686)  Time: 0.748s,  171.17/s  (0.754s,  169.83/s)  LR: 9.677e-02  Data: 0.000 (0.006)
Train: 23 [ 200/390 ( 51%)]  Loss:  2.056431 (2.4662)  Time: 0.748s,  171.09/s  (0.752s,  170.18/s)  LR: 9.677e-02  Data: 0.001 (0.005)
Train: 23 [ 250/390 ( 64%)]  Loss:  2.023006 (2.3923)  Time: 0.747s,  171.26/s  (0.751s,  170.39/s)  LR: 9.677e-02  Data: 0.000 (0.004)
Train: 23 [ 300/390 ( 77%)]  Loss:  1.982157 (2.3337)  Time: 0.747s,  171.36/s  (0.751s,  170.38/s)  LR: 9.677e-02  Data: 0.000 (0.003)
Train: 23 [ 350/390 ( 90%)]  Loss:  2.012420 (2.2936)  Time: 0.750s,  170.66/s  (0.751s,  170.50/s)  LR: 9.677e-02  Data: 0.003 (0.003)
^C
Traceback (most recent call last):
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 233, in <module>
    main()
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 69, in main
    train_metrics = train_epoch(
                    ^^^^^^^^^^^^
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 127, in train_epoch
    loss_scaler(loss, optimizer)
  File "/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py", line 95, in __call__
    self._scaler.scale(loss).backward(create_graph=create_graph)
  File "/usr/local/lib/python3.11/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt