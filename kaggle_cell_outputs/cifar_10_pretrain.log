2025-08-28 06:43:04.649620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1756363384.673716     155 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1756363384.681035     155 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
create dataset:  cifar10
Creating model..
, number of params: 27599012
SGD!!!
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Train: 0 [   0/391 (  0%)]  Loss:  4.612244 (4.6122)  Time: 1.152s,  111.09/s  (1.152s,  111.09/s)  LR: 1.000e-01  Data: 0.054 (0.054)
Train: 0 [  50/391 ( 13%)]  Loss:  2.028873 (3.3206)  Time: 0.792s,  161.68/s  (0.799s,  160.11/s)  LR: 1.000e-01  Data: 0.046 (0.046)
Train: 0 [ 100/391 ( 26%)]  Loss:  1.953081 (2.8647)  Time: 0.793s,  161.35/s  (0.796s,  160.87/s)  LR: 1.000e-01  Data: 0.047 (0.046)
Train: 0 [ 150/391 ( 38%)]  Loss:  1.870285 (2.6161)  Time: 0.791s,  161.78/s  (0.795s,  161.10/s)  LR: 1.000e-01  Data: 0.044 (0.046)
Train: 0 [ 200/391 ( 51%)]  Loss:  1.835501 (2.4600)  Time: 0.790s,  162.12/s  (0.794s,  161.26/s)  LR: 1.000e-01  Data: 0.043 (0.045)
Train: 0 [ 250/391 ( 64%)]  Loss:  1.856594 (2.3594)  Time: 0.791s,  161.72/s  (0.793s,  161.34/s)  LR: 1.000e-01  Data: 0.046 (0.045)
Train: 0 [ 300/391 ( 77%)]  Loss:  1.766047 (2.2747)  Time: 0.791s,  161.85/s  (0.793s,  161.40/s)  LR: 1.000e-01  Data: 0.045 (0.045)
Train: 0 [ 350/391 ( 90%)]  Loss:  1.618753 (2.1927)  Time: 0.791s,  161.72/s  (0.793s,  161.47/s)  LR: 1.000e-01  Data: 0.044 (0.045)
Train: 0 [ 390/391 (100%)]  Loss:  1.638062 (2.1525)  Time: 0.513s,  155.88/s  (0.793s,  100.94/s)  LR: 1.000e-01  Data: 0.032 (0.045)
/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast():
Test: [   0/99]  Time: 0.279 (0.279)  DataTime: 0.022 (0.022)  Loss:  1.5195 (1.5195)  Acc@1: 38.0000 (38.0000)  Acc@5: 93.0000 (93.0000)
Test: [  50/99]  Time: 0.234 (0.235)  DataTime: 0.022 (0.022)  Loss:  1.5127 (1.5567)  Acc@1: 45.0000 (40.5294)  Acc@5: 92.0000 (90.9804)
Test: [  99/99]  Time: 0.234 (0.235)  DataTime: 0.022 (0.022)  Loss:  1.5879 (1.5657)  Acc@1: 38.0000 (40.7800)  Acc@5: 93.0000 (91.0300)
tb
loss  0 : 2.33068359375 0
loss  1 : 1.650400390625 0
loss  2 : 1.642119140625 0
loss  3 : 1.627587890625 0
{'loss_different_0': 2.33068359375, 'loss_different_1': 1.650400390625, 'loss_different_2': 1.642119140625, 'loss_different_3': 1.627587890625}
Top1:  40.78
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250828-064313-pretrain_cifar_10/pretrain_cifar_10_0.pt
Train: 1 [   0/391 (  0%)]  Loss:  1.486691 (1.4867)  Time: 0.793s,  161.35/s  (0.793s,  161.35/s)  LR: 9.999e-02  Data: 0.047 (0.047)
Train: 1 [  50/391 ( 13%)]  Loss:  1.439904 (1.4633)  Time: 0.792s,  161.55/s  (0.792s,  161.56/s)  LR: 9.999e-02  Data: 0.046 (0.046)
Train: 1 [ 100/391 ( 26%)]  Loss:  1.565354 (1.4973)  Time: 0.791s,  161.85/s  (0.792s,  161.66/s)  LR: 9.999e-02  Data: 0.045 (0.045)
Train: 1 [ 150/391 ( 38%)]  Loss:  1.434601 (1.4816)  Time: 0.792s,  161.54/s  (0.792s,  161.67/s)  LR: 9.999e-02  Data: 0.045 (0.045)
Train: 1 [ 200/391 ( 51%)]  Loss:  1.223873 (1.4301)  Time: 0.791s,  161.91/s  (0.792s,  161.70/s)  LR: 9.999e-02  Data: 0.045 (0.045)
Train: 1 [ 250/391 ( 64%)]  Loss:  1.273142 (1.4039)  Time: 0.788s,  162.34/s  (0.791s,  161.73/s)  LR: 9.999e-02  Data: 0.042 (0.045)
Train: 1 [ 300/391 ( 77%)]  Loss:  1.167180 (1.3701)  Time: 0.792s,  161.63/s  (0.791s,  161.74/s)  LR: 9.999e-02  Data: 0.046 (0.045)
Train: 1 [ 350/391 ( 90%)]  Loss:  1.286735 (1.3597)  Time: 0.790s,  161.96/s  (0.791s,  161.73/s)  LR: 9.999e-02  Data: 0.044 (0.045)
Train: 1 [ 390/391 (100%)]  Loss:  1.252439 (1.3519)  Time: 0.511s,  156.53/s  (0.791s,  101.14/s)  LR: 9.999e-02  Data: 0.032 (0.045)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.2803 (1.2803)  Acc@1: 52.0000 (52.0000)  Acc@5: 96.0000 (96.0000)
Test: [  50/99]  Time: 0.237 (0.236)  DataTime: 0.025 (0.023)  Loss:  1.2266 (1.4445)  Acc@1: 54.0000 (49.0392)  Acc@5: 94.0000 (92.6078)
Test: [  99/99]  Time: 0.234 (0.235)  DataTime: 0.022 (0.023)  Loss:  1.3623 (1.4583)  Acc@1: 49.0000 (48.6000)  Acc@5: 92.0000 (92.5300)
tb
loss  0 : 2.6737890625 1
loss  1 : 1.764150390625 1
loss  2 : 1.72111328125 1
loss  3 : 1.66078125 1
{'loss_different_0': 2.6737890625, 'loss_different_1': 1.764150390625, 'loss_different_2': 1.72111328125, 'loss_different_3': 1.66078125}
Top1:  48.6
Train: 2 [   0/391 (  0%)]  Loss:  1.191217 (1.1912)  Time: 0.796s,  160.86/s  (0.796s,  160.86/s)  LR: 9.998e-02  Data: 0.050 (0.050)
Train: 2 [  50/391 ( 13%)]  Loss:  1.161929 (1.1766)  Time: 0.792s,  161.67/s  (0.793s,  161.43/s)  LR: 9.998e-02  Data: 0.045 (0.047)
Train: 2 [ 100/391 ( 26%)]  Loss:  1.037699 (1.1303)  Time: 0.792s,  161.63/s  (0.793s,  161.43/s)  LR: 9.998e-02  Data: 0.045 (0.047)
Train: 2 [ 150/391 ( 38%)]  Loss:  0.969195 (1.0900)  Time: 0.793s,  161.47/s  (0.793s,  161.45/s)  LR: 9.998e-02  Data: 0.047 (0.047)
Train: 2 [ 200/391 ( 51%)]  Loss:  1.098541 (1.0917)  Time: 0.790s,  162.09/s  (0.792s,  161.55/s)  LR: 9.998e-02  Data: 0.044 (0.046)
Train: 2 [ 250/391 ( 64%)]  Loss:  1.053620 (1.0854)  Time: 0.791s,  161.91/s  (0.792s,  161.61/s)  LR: 9.998e-02  Data: 0.044 (0.046)
Train: 2 [ 300/391 ( 77%)]  Loss:  1.086486 (1.0855)  Time: 0.790s,  162.05/s  (0.792s,  161.55/s)  LR: 9.998e-02  Data: 0.044 (0.046)
Train: 2 [ 350/391 ( 90%)]  Loss:  0.856087 (1.0568)  Time: 0.790s,  162.05/s  (0.792s,  161.58/s)  LR: 9.998e-02  Data: 0.044 (0.046)
Train: 2 [ 390/391 (100%)]  Loss:  1.321532 (1.0760)  Time: 0.509s,  157.22/s  (0.791s,  101.10/s)  LR: 9.998e-02  Data: 0.029 (0.045)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  1.2578 (1.2578)  Acc@1: 59.0000 (59.0000)  Acc@5: 95.0000 (95.0000)
Test: [  50/99]  Time: 0.235 (0.235)  DataTime: 0.022 (0.022)  Loss:  1.1904 (1.1776)  Acc@1: 54.0000 (59.2941)  Acc@5: 98.0000 (96.0588)
Test: [  99/99]  Time: 0.234 (0.235)  DataTime: 0.022 (0.022)  Loss:  1.2227 (1.1856)  Acc@1: 57.0000 (58.5800)  Acc@5: 93.0000 (95.9500)
tb
loss  0 : 3.3472265625 2
loss  1 : 1.3856787109375 2
loss  2 : 1.577421875 2
loss  3 : 1.6968359375 2
{'loss_different_0': 3.3472265625, 'loss_different_1': 1.3856787109375, 'loss_different_2': 1.577421875, 'loss_different_3': 1.6968359375}
Top1:  58.58
Train: 3 [   0/391 (  0%)]  Loss:  1.083728 (1.0837)  Time: 0.792s,  161.64/s  (0.792s,  161.64/s)  LR: 9.994e-02  Data: 0.046 (0.046)
Train: 3 [  50/391 ( 13%)]  Loss:  0.994310 (1.0390)  Time: 0.789s,  162.29/s  (0.791s,  161.76/s)  LR: 9.994e-02  Data: 0.043 (0.045)
Train: 3 [ 100/391 ( 26%)]  Loss:  0.898228 (0.9921)  Time: 0.792s,  161.69/s  (0.792s,  161.71/s)  LR: 9.994e-02  Data: 0.046 (0.045)
Train: 3 [ 150/391 ( 38%)]  Loss:  0.922416 (0.9747)  Time: 0.792s,  161.70/s  (0.791s,  161.73/s)  LR: 9.994e-02  Data: 0.045 (0.045)
Train: 3 [ 200/391 ( 51%)]  Loss:  0.883201 (0.9564)  Time: 0.792s,  161.68/s  (0.791s,  161.75/s)  LR: 9.994e-02  Data: 0.046 (0.045)
Train: 3 [ 250/391 ( 64%)]  Loss:  0.995647 (0.9629)  Time: 0.790s,  161.92/s  (0.791s,  161.75/s)  LR: 9.994e-02  Data: 0.044 (0.045)
Train: 3 [ 300/391 ( 77%)]  Loss:  0.987514 (0.9664)  Time: 0.792s,  161.64/s  (0.791s,  161.75/s)  LR: 9.994e-02  Data: 0.046 (0.045)
Train: 3 [ 350/391 ( 90%)]  Loss:  0.789227 (0.9443)  Time: 0.792s,  161.71/s  (0.791s,  161.74/s)  LR: 9.994e-02  Data: 0.045 (0.045)
Train: 3 [ 390/391 (100%)]  Loss:  0.960655 (0.9455)  Time: 0.510s,  156.80/s  (0.791s,  101.18/s)  LR: 9.994e-02  Data: 0.031 (0.045)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.022 (0.022)  Loss:  1.4941 (1.4941)  Acc@1: 57.0000 (57.0000)  Acc@5: 95.0000 (95.0000)
Test: [  50/99]  Time: 0.234 (0.235)  DataTime: 0.022 (0.022)  Loss:  1.2324 (1.4068)  Acc@1: 55.0000 (56.0784)  Acc@5: 94.0000 (94.6078)
Test: [  99/99]  Time: 0.236 (0.235)  DataTime: 0.023 (0.022)  Loss:  1.4150 (1.4066)  Acc@1: 54.0000 (55.5700)  Acc@5: 94.0000 (94.7900)
tb
loss  0 : 3.387421875 3
loss  1 : 1.714873046875 3
loss  2 : 1.868017578125 3
loss  3 : 2.466669921875 3
{'loss_different_0': 3.387421875, 'loss_different_1': 1.714873046875, 'loss_different_2': 1.868017578125, 'loss_different_3': 2.466669921875}
Top1:  55.57
Train: 4 [   0/391 (  0%)]  Loss:  0.918340 (0.9183)  Time: 0.792s,  161.64/s  (0.792s,  161.64/s)  LR: 9.990e-02  Data: 0.046 (0.046)
Train: 4 [  50/391 ( 13%)]  Loss:  0.992160 (0.9552)  Time: 0.791s,  161.76/s  (0.792s,  161.64/s)  LR: 9.990e-02  Data: 0.046 (0.046)
Train: 4 [ 100/391 ( 26%)]  Loss:  0.840966 (0.9172)  Time: 0.793s,  161.32/s  (0.794s,  161.21/s)  LR: 9.990e-02  Data: 0.047 (0.045)
Train: 4 [ 150/391 ( 38%)]  Loss:  1.108418 (0.9650)  Time: 0.790s,  161.96/s  (0.793s,  161.42/s)  LR: 9.990e-02  Data: 0.045 (0.045)
Train: 4 [ 200/391 ( 51%)]  Loss:  0.809142 (0.9338)  Time: 0.792s,  161.68/s  (0.793s,  161.47/s)  LR: 9.990e-02  Data: 0.045 (0.045)
Train: 4 [ 250/391 ( 64%)]  Loss:  0.823227 (0.9154)  Time: 0.791s,  161.89/s  (0.792s,  161.53/s)  LR: 9.990e-02  Data: 0.045 (0.045)
Train: 4 [ 300/391 ( 77%)]  Loss:  0.884219 (0.9109)  Time: 0.791s,  161.72/s  (0.792s,  161.57/s)  LR: 9.990e-02  Data: 0.045 (0.045)
Train: 4 [ 350/391 ( 90%)]  Loss:  0.743417 (0.8900)  Time: 0.791s,  161.91/s  (0.792s,  161.58/s)  LR: 9.990e-02  Data: 0.044 (0.045)
Train: 4 [ 390/391 (100%)]  Loss:  0.739339 (0.8791)  Time: 0.510s,  156.71/s  (0.791s,  101.09/s)  LR: 9.990e-02  Data: 0.031 (0.045)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.023 (0.023)  Loss:  0.9375 (0.9375)  Acc@1: 66.0000 (66.0000)  Acc@5: 96.0000 (96.0000)
Test: [  50/99]  Time: 0.233 (0.235)  DataTime: 0.021 (0.023)  Loss:  0.8267 (0.9272)  Acc@1: 73.0000 (68.1373)  Acc@5: 97.0000 (97.1765)
Test: [  99/99]  Time: 0.232 (0.235)  DataTime: 0.020 (0.023)  Loss:  0.9756 (0.9363)  Acc@1: 65.0000 (68.0300)  Acc@5: 96.0000 (97.3600)
tb
loss  0 : 3.39576171875 4
loss  1 : 1.157041015625 4
loss  2 : 1.44525390625 4
loss  3 : 1.5024609375 4
{'loss_different_0': 3.39576171875, 'loss_different_1': 1.157041015625, 'loss_different_2': 1.44525390625, 'loss_different_3': 1.5024609375}
Top1:  68.03
Train: 5 [   0/391 (  0%)]  Loss:  0.732656 (0.7327)  Time: 0.794s,  161.28/s  (0.794s,  161.28/s)  LR: 9.985e-02  Data: 0.048 (0.048)
Train: 5 [  50/391 ( 13%)]  Loss:  0.709426 (0.7210)  Time: 0.793s,  161.46/s  (0.792s,  161.52/s)  LR: 9.985e-02  Data: 0.047 (0.046)
Train: 5 [ 100/391 ( 26%)]  Loss:  0.777041 (0.7397)  Time: 0.791s,  161.91/s  (0.792s,  161.65/s)  LR: 9.985e-02  Data: 0.044 (0.046)
Train: 5 [ 150/391 ( 38%)]  Loss:  0.650994 (0.7175)  Time: 0.790s,  161.99/s  (0.791s,  161.72/s)  LR: 9.985e-02  Data: 0.045 (0.045)
Train: 5 [ 200/391 ( 51%)]  Loss:  0.726971 (0.7194)  Time: 0.790s,  162.03/s  (0.791s,  161.76/s)  LR: 9.985e-02  Data: 0.045 (0.045)
Train: 5 [ 250/391 ( 64%)]  Loss:  0.631993 (0.7048)  Time: 0.794s,  161.17/s  (0.791s,  161.79/s)  LR: 9.985e-02  Data: 0.049 (0.045)
Train: 5 [ 300/391 ( 77%)]  Loss:  0.748503 (0.7111)  Time: 0.790s,  162.03/s  (0.791s,  161.82/s)  LR: 9.985e-02  Data: 0.044 (0.045)
Train: 5 [ 350/391 ( 90%)]  Loss:  0.936766 (0.7393)  Time: 0.789s,  162.13/s  (0.791s,  161.84/s)  LR: 9.985e-02  Data: 0.043 (0.045)
Train: 5 [ 390/391 (100%)]  Loss:  0.891144 (0.7503)  Time: 0.511s,  156.45/s  (0.790s,  101.25/s)  LR: 9.985e-02  Data: 0.032 (0.045)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.9253 (0.9253)  Acc@1: 67.0000 (67.0000)  Acc@5: 97.0000 (97.0000)
Test: [  50/99]  Time: 0.235 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.8892 (0.9737)  Acc@1: 66.0000 (68.8627)  Acc@5: 97.0000 (96.9412)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  1.0723 (0.9662)  Acc@1: 66.0000 (68.6500)  Acc@5: 97.0000 (97.0800)
tb
loss  0 : 3.75630859375 5
loss  1 : 1.1718505859375 5
loss  2 : 1.788759765625 5
loss  3 : 1.5772314453125 5
{'loss_different_0': 3.75630859375, 'loss_different_1': 1.1718505859375, 'loss_different_2': 1.788759765625, 'loss_different_3': 1.5772314453125}
Top1:  68.65
Train: 6 [   0/391 (  0%)]  Loss:  0.618278 (0.6183)  Time: 0.798s,  160.47/s  (0.798s,  160.47/s)  LR: 9.978e-02  Data: 0.052 (0.052)
Train: 6 [  50/391 ( 13%)]  Loss:  0.702644 (0.6605)  Time: 0.790s,  162.08/s  (0.794s,  161.24/s)  LR: 9.978e-02  Data: 0.043 (0.045)
Train: 6 [ 100/391 ( 26%)]  Loss:  0.706350 (0.6758)  Time: 0.789s,  162.23/s  (0.792s,  161.61/s)  LR: 9.978e-02  Data: 0.043 (0.044)
Train: 6 [ 150/391 ( 38%)]  Loss:  0.807431 (0.7087)  Time: 0.791s,  161.78/s  (0.791s,  161.75/s)  LR: 9.978e-02  Data: 0.045 (0.044)
Train: 6 [ 200/391 ( 51%)]  Loss:  0.690313 (0.7050)  Time: 0.788s,  162.44/s  (0.791s,  161.79/s)  LR: 9.978e-02  Data: 0.042 (0.044)
Train: 6 [ 250/391 ( 64%)]  Loss:  0.627052 (0.6920)  Time: 0.789s,  162.17/s  (0.791s,  161.81/s)  LR: 9.978e-02  Data: 0.043 (0.044)
Train: 6 [ 300/391 ( 77%)]  Loss:  0.584816 (0.6767)  Time: 0.791s,  161.87/s  (0.791s,  161.84/s)  LR: 9.978e-02  Data: 0.044 (0.044)
Train: 6 [ 350/391 ( 90%)]  Loss:  0.677202 (0.6768)  Time: 0.789s,  162.25/s  (0.791s,  161.88/s)  LR: 9.978e-02  Data: 0.044 (0.044)
Train: 6 [ 390/391 (100%)]  Loss:  0.770496 (0.6836)  Time: 0.509s,  157.13/s  (0.790s,  101.27/s)  LR: 9.978e-02  Data: 0.030 (0.044)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.7539 (0.7539)  Acc@1: 76.0000 (76.0000)  Acc@5: 98.0000 (98.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.8491 (0.8620)  Acc@1: 67.0000 (71.4118)  Acc@5: 97.0000 (97.2745)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.7964 (0.8609)  Acc@1: 75.0000 (71.3100)  Acc@5: 99.0000 (97.4400)
tb
loss  0 : 3.57607421875 6
loss  1 : 1.0628955078125 6
loss  2 : 1.66220703125 6
loss  3 : 1.37419921875 6
{'loss_different_0': 3.57607421875, 'loss_different_1': 1.0628955078125, 'loss_different_2': 1.66220703125, 'loss_different_3': 1.37419921875}
Top1:  71.31
Train: 7 [   0/391 (  0%)]  Loss:  0.711476 (0.7115)  Time: 0.793s,  161.47/s  (0.793s,  161.47/s)  LR: 9.970e-02  Data: 0.047 (0.047)
Train: 7 [  50/391 ( 13%)]  Loss:  0.731847 (0.7217)  Time: 0.789s,  162.17/s  (0.790s,  161.94/s)  LR: 9.970e-02  Data: 0.043 (0.044)
Train: 7 [ 100/391 ( 26%)]  Loss:  0.687221 (0.7102)  Time: 0.792s,  161.71/s  (0.790s,  161.99/s)  LR: 9.970e-02  Data: 0.045 (0.044)
Train: 7 [ 150/391 ( 38%)]  Loss:  0.703241 (0.7084)  Time: 0.791s,  161.76/s  (0.790s,  162.00/s)  LR: 9.970e-02  Data: 0.046 (0.044)
Train: 7 [ 200/391 ( 51%)]  Loss:  0.679043 (0.7026)  Time: 0.789s,  162.30/s  (0.791s,  161.77/s)  LR: 9.970e-02  Data: 0.043 (0.044)
Train: 7 [ 250/391 ( 64%)]  Loss:  0.695667 (0.7014)  Time: 0.789s,  162.25/s  (0.791s,  161.81/s)  LR: 9.970e-02  Data: 0.043 (0.044)
Train: 7 [ 300/391 ( 77%)]  Loss:  0.790463 (0.7141)  Time: 0.788s,  162.34/s  (0.791s,  161.84/s)  LR: 9.970e-02  Data: 0.042 (0.044)
Train: 7 [ 350/391 ( 90%)]  Loss:  0.612880 (0.7015)  Time: 0.789s,  162.18/s  (0.791s,  161.86/s)  LR: 9.970e-02  Data: 0.043 (0.044)
Train: 7 [ 390/391 (100%)]  Loss:  0.604333 (0.6944)  Time: 0.509s,  157.09/s  (0.790s,  101.26/s)  LR: 9.970e-02  Data: 0.030 (0.044)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.021 (0.021)  Loss:  0.7817 (0.7817)  Acc@1: 68.0000 (68.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.7104 (0.9551)  Acc@1: 74.0000 (69.2157)  Acc@5: 99.0000 (97.4118)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.9380 (0.9587)  Acc@1: 73.0000 (69.3500)  Acc@5: 96.0000 (97.3800)
tb
loss  0 : 3.66974609375 7
loss  1 : 1.2684228515625 7
loss  2 : 1.723388671875 7
loss  3 : 1.6305078125 7
{'loss_different_0': 3.66974609375, 'loss_different_1': 1.2684228515625, 'loss_different_2': 1.723388671875, 'loss_different_3': 1.6305078125}
Top1:  69.35
Train: 8 [   0/391 (  0%)]  Loss:  0.715932 (0.7159)  Time: 0.793s,  161.46/s  (0.793s,  161.46/s)  LR: 9.961e-02  Data: 0.047 (0.047)
Train: 8 [  50/391 ( 13%)]  Loss:  0.622106 (0.6690)  Time: 0.790s,  162.09/s  (0.790s,  162.05/s)  LR: 9.961e-02  Data: 0.044 (0.044)
Train: 8 [ 100/391 ( 26%)]  Loss:  0.602420 (0.6468)  Time: 0.793s,  161.47/s  (0.790s,  161.98/s)  LR: 9.961e-02  Data: 0.047 (0.044)
Train: 8 [ 150/391 ( 38%)]  Loss:  0.602078 (0.6356)  Time: 0.791s,  161.82/s  (0.790s,  161.97/s)  LR: 9.961e-02  Data: 0.045 (0.044)
Train: 8 [ 200/391 ( 51%)]  Loss:  0.508065 (0.6101)  Time: 0.790s,  162.03/s  (0.790s,  161.95/s)  LR: 9.961e-02  Data: 0.044 (0.044)
Train: 8 [ 250/391 ( 64%)]  Loss:  0.526608 (0.5962)  Time: 0.790s,  162.02/s  (0.790s,  161.96/s)  LR: 9.961e-02  Data: 0.044 (0.044)
Train: 8 [ 300/391 ( 77%)]  Loss:  0.650790 (0.6040)  Time: 0.789s,  162.32/s  (0.790s,  161.96/s)  LR: 9.961e-02  Data: 0.043 (0.044)
Train: 8 [ 350/391 ( 90%)]  Loss:  0.688568 (0.6146)  Time: 0.789s,  162.31/s  (0.790s,  161.98/s)  LR: 9.961e-02  Data: 0.043 (0.044)
Train: 8 [ 390/391 (100%)]  Loss:  0.693612 (0.6203)  Time: 0.510s,  156.94/s  (0.789s,  101.33/s)  LR: 9.961e-02  Data: 0.031 (0.044)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.022 (0.022)  Loss:  0.8608 (0.8608)  Acc@1: 73.0000 (73.0000)  Acc@5: 98.0000 (98.0000)
Test: [  50/99]  Time: 0.235 (0.234)  DataTime: 0.023 (0.022)  Loss:  0.9531 (1.0087)  Acc@1: 67.0000 (67.8431)  Acc@5: 97.0000 (97.2745)
Test: [  99/99]  Time: 0.236 (0.234)  DataTime: 0.024 (0.022)  Loss:  0.8032 (0.9934)  Acc@1: 73.0000 (68.2400)  Acc@5: 97.0000 (97.1900)
tb
loss  0 : 3.61541015625 8
loss  1 : 1.2121728515625 8
loss  2 : 1.829658203125 8
loss  3 : 1.772138671875 8
{'loss_different_0': 3.61541015625, 'loss_different_1': 1.2121728515625, 'loss_different_2': 1.829658203125, 'loss_different_3': 1.772138671875}
Top1:  68.24
Train: 9 [   0/391 (  0%)]  Loss:  0.712912 (0.7129)  Time: 0.793s,  161.50/s  (0.793s,  161.50/s)  LR: 9.950e-02  Data: 0.046 (0.046)
Train: 9 [  50/391 ( 13%)]  Loss:  0.594692 (0.6538)  Time: 0.790s,  162.07/s  (0.790s,  162.02/s)  LR: 9.950e-02  Data: 0.044 (0.044)
Train: 9 [ 100/391 ( 26%)]  Loss:  0.483600 (0.5971)  Time: 0.791s,  161.90/s  (0.790s,  162.02/s)  LR: 9.950e-02  Data: 0.044 (0.044)
Train: 9 [ 150/391 ( 38%)]  Loss:  0.905499 (0.6742)  Time: 0.790s,  161.96/s  (0.791s,  161.79/s)  LR: 9.950e-02  Data: 0.044 (0.044)
Train: 9 [ 200/391 ( 51%)]  Loss:  0.745861 (0.6885)  Time: 0.789s,  162.32/s  (0.791s,  161.82/s)  LR: 9.950e-02  Data: 0.043 (0.044)
Train: 9 [ 250/391 ( 64%)]  Loss:  0.697331 (0.6900)  Time: 0.791s,  161.84/s  (0.791s,  161.85/s)  LR: 9.950e-02  Data: 0.045 (0.044)
Train: 9 [ 300/391 ( 77%)]  Loss:  0.796444 (0.7052)  Time: 0.791s,  161.82/s  (0.791s,  161.88/s)  LR: 9.950e-02  Data: 0.044 (0.044)
Train: 9 [ 350/391 ( 90%)]  Loss:  0.687800 (0.7030)  Time: 0.788s,  162.34/s  (0.791s,  161.89/s)  LR: 9.950e-02  Data: 0.042 (0.044)
Train: 9 [ 390/391 (100%)]  Loss:  0.624694 (0.6973)  Time: 0.513s,  155.83/s  (0.790s,  101.27/s)  LR: 9.950e-02  Data: 0.034 (0.044)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5620 (0.5620)  Acc@1: 85.0000 (85.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.6587 (0.6658)  Acc@1: 77.0000 (78.1961)  Acc@5: 99.0000 (98.8431)
Test: [  99/99]  Time: 0.235 (0.234)  DataTime: 0.023 (0.022)  Loss:  0.7075 (0.6586)  Acc@1: 76.0000 (78.2700)  Acc@5: 99.0000 (98.8400)
tb
loss  0 : 3.692421875 9
loss  1 : 0.849482421875 9
loss  2 : 1.411064453125 9
loss  3 : 1.13431640625 9
{'loss_different_0': 3.692421875, 'loss_different_1': 0.849482421875, 'loss_different_2': 1.411064453125, 'loss_different_3': 1.13431640625}
Top1:  78.27
Train: 10 [   0/391 (  0%)]  Loss:  0.730369 (0.7304)  Time: 0.792s,  161.51/s  (0.792s,  161.51/s)  LR: 9.938e-02  Data: 0.047 (0.047)
Train: 10 [  50/391 ( 13%)]  Loss:  0.614478 (0.6724)  Time: 0.788s,  162.37/s  (0.791s,  161.85/s)  LR: 9.938e-02  Data: 0.043 (0.045)
Train: 10 [ 100/391 ( 26%)]  Loss:  0.567402 (0.6374)  Time: 0.789s,  162.26/s  (0.791s,  161.86/s)  LR: 9.938e-02  Data: 0.044 (0.045)
Train: 10 [ 150/391 ( 38%)]  Loss:  0.630632 (0.6357)  Time: 0.788s,  162.37/s  (0.791s,  161.88/s)  LR: 9.938e-02  Data: 0.043 (0.045)
Train: 10 [ 200/391 ( 51%)]  Loss:  0.464408 (0.6015)  Time: 0.791s,  161.80/s  (0.791s,  161.88/s)  LR: 9.938e-02  Data: 0.045 (0.045)
Train: 10 [ 250/391 ( 64%)]  Loss:  0.708769 (0.6193)  Time: 0.790s,  161.98/s  (0.791s,  161.86/s)  LR: 9.938e-02  Data: 0.044 (0.045)
Train: 10 [ 300/391 ( 77%)]  Loss:  0.591143 (0.6153)  Time: 0.792s,  161.52/s  (0.792s,  161.67/s)  LR: 9.938e-02  Data: 0.046 (0.045)
Train: 10 [ 350/391 ( 90%)]  Loss:  0.479120 (0.5983)  Time: 0.796s,  160.79/s  (0.792s,  161.70/s)  LR: 9.938e-02  Data: 0.050 (0.045)
Train: 10 [ 390/391 (100%)]  Loss:  0.648391 (0.6019)  Time: 0.510s,  156.86/s  (0.791s,  101.16/s)  LR: 9.938e-02  Data: 0.031 (0.045)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5771 (0.5771)  Acc@1: 83.0000 (83.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.236 (0.235)  DataTime: 0.024 (0.022)  Loss:  0.5962 (0.7257)  Acc@1: 81.0000 (76.2745)  Acc@5: 99.0000 (98.4118)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.8188 (0.7280)  Acc@1: 72.0000 (75.8200)  Acc@5: 99.0000 (98.5600)
tb
loss  0 : 3.59931640625 10
loss  1 : 0.9118603515625 10
loss  2 : 1.5372021484375 10
loss  3 : 1.2554345703125 10
{'loss_different_0': 3.59931640625, 'loss_different_1': 0.9118603515625, 'loss_different_2': 1.5372021484375, 'loss_different_3': 1.2554345703125}
Top1:  75.82
Train: 11 [   0/391 (  0%)]  Loss:  0.394085 (0.3941)  Time: 0.792s,  161.66/s  (0.792s,  161.66/s)  LR: 9.926e-02  Data: 0.046 (0.046)
Train: 11 [  50/391 ( 13%)]  Loss:  0.536331 (0.4652)  Time: 0.790s,  162.05/s  (0.791s,  161.89/s)  LR: 9.926e-02  Data: 0.043 (0.045)
Train: 11 [ 100/391 ( 26%)]  Loss:  0.525873 (0.4854)  Time: 0.792s,  161.71/s  (0.791s,  161.82/s)  LR: 9.926e-02  Data: 0.046 (0.045)
Train: 11 [ 150/391 ( 38%)]  Loss:  0.580768 (0.5093)  Time: 0.791s,  161.87/s  (0.791s,  161.88/s)  LR: 9.926e-02  Data: 0.044 (0.045)
Train: 11 [ 200/391 ( 51%)]  Loss:  0.458497 (0.4991)  Time: 0.790s,  161.99/s  (0.791s,  161.89/s)  LR: 9.926e-02  Data: 0.044 (0.045)
Train: 11 [ 250/391 ( 64%)]  Loss:  0.696658 (0.5320)  Time: 0.793s,  161.41/s  (0.791s,  161.88/s)  LR: 9.926e-02  Data: 0.047 (0.045)
Train: 11 [ 300/391 ( 77%)]  Loss:  0.509138 (0.5288)  Time: 0.789s,  162.27/s  (0.791s,  161.88/s)  LR: 9.926e-02  Data: 0.043 (0.045)
Train: 11 [ 350/391 ( 90%)]  Loss:  0.444443 (0.5182)  Time: 0.791s,  161.73/s  (0.791s,  161.88/s)  LR: 9.926e-02  Data: 0.046 (0.045)
Train: 11 [ 390/391 (100%)]  Loss:  0.447721 (0.5131)  Time: 0.512s,  156.29/s  (0.790s,  101.27/s)  LR: 9.926e-02  Data: 0.033 (0.045)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.8569 (0.8569)  Acc@1: 72.0000 (72.0000)  Acc@5: 98.0000 (98.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.7778 (0.9004)  Acc@1: 71.0000 (71.9412)  Acc@5: 98.0000 (97.9412)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.8862 (0.8946)  Acc@1: 70.0000 (72.0900)  Acc@5: 98.0000 (98.1200)
tb
loss  0 : 3.63173828125 11
loss  1 : 1.077255859375 11
loss  2 : 1.921171875 11
loss  3 : 1.558916015625 11
{'loss_different_0': 3.63173828125, 'loss_different_1': 1.077255859375, 'loss_different_2': 1.921171875, 'loss_different_3': 1.558916015625}
Top1:  72.09
Train: 12 [   0/391 (  0%)]  Loss:  0.603732 (0.6037)  Time: 0.793s,  161.38/s  (0.793s,  161.38/s)  LR: 9.911e-02  Data: 0.047 (0.047)
Train: 12 [  50/391 ( 13%)]  Loss:  0.531820 (0.5678)  Time: 0.788s,  162.36/s  (0.789s,  162.14/s)  LR: 9.911e-02  Data: 0.042 (0.043)
Train: 12 [ 100/391 ( 26%)]  Loss:  0.453340 (0.5296)  Time: 0.791s,  161.87/s  (0.789s,  162.15/s)  LR: 9.911e-02  Data: 0.044 (0.043)
Train: 12 [ 150/391 ( 38%)]  Loss:  0.648412 (0.5593)  Time: 0.788s,  162.53/s  (0.789s,  162.13/s)  LR: 9.911e-02  Data: 0.042 (0.043)
Train: 12 [ 200/391 ( 51%)]  Loss:  0.371714 (0.5218)  Time: 0.789s,  162.19/s  (0.790s,  162.10/s)  LR: 9.911e-02  Data: 0.043 (0.044)
Train: 12 [ 250/391 ( 64%)]  Loss:  0.554736 (0.5273)  Time: 0.788s,  162.35/s  (0.790s,  161.97/s)  LR: 9.911e-02  Data: 0.043 (0.044)
Train: 12 [ 300/391 ( 77%)]  Loss:  0.307452 (0.4959)  Time: 0.791s,  161.86/s  (0.790s,  161.98/s)  LR: 9.911e-02  Data: 0.045 (0.044)
Train: 12 [ 350/391 ( 90%)]  Loss:  0.558036 (0.5037)  Time: 0.791s,  161.87/s  (0.790s,  162.00/s)  LR: 9.911e-02  Data: 0.045 (0.044)
Train: 12 [ 390/391 (100%)]  Loss:  0.618516 (0.5120)  Time: 0.511s,  156.68/s  (0.789s,  101.35/s)  LR: 9.911e-02  Data: 0.031 (0.044)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.020 (0.020)  Loss:  0.4775 (0.4775)  Acc@1: 82.0000 (82.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.021 (0.021)  Loss:  0.4600 (0.5959)  Acc@1: 86.0000 (79.8431)  Acc@5: 100.0000 (99.0784)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.021)  Loss:  0.6270 (0.6002)  Acc@1: 81.0000 (79.6800)  Acc@5: 100.0000 (99.1500)
tb
loss  0 : 3.678359375 12
loss  1 : 0.70134765625 12
loss  2 : 1.425791015625 12
loss  3 : 1.1189990234375 12
{'loss_different_0': 3.678359375, 'loss_different_1': 0.70134765625, 'loss_different_2': 1.425791015625, 'loss_different_3': 1.1189990234375}
Top1:  79.68
Train: 13 [   0/391 (  0%)]  Loss:  0.495009 (0.4950)  Time: 0.793s,  161.49/s  (0.793s,  161.49/s)  LR: 9.896e-02  Data: 0.046 (0.046)
Train: 13 [  50/391 ( 13%)]  Loss:  0.455630 (0.4753)  Time: 0.789s,  162.25/s  (0.789s,  162.13/s)  LR: 9.896e-02  Data: 0.043 (0.043)
Train: 13 [ 100/391 ( 26%)]  Loss:  0.531341 (0.4940)  Time: 0.789s,  162.24/s  (0.790s,  162.07/s)  LR: 9.896e-02  Data: 0.043 (0.044)
Train: 13 [ 150/391 ( 38%)]  Loss:  0.436525 (0.4796)  Time: 0.790s,  162.01/s  (0.790s,  162.04/s)  LR: 9.896e-02  Data: 0.045 (0.044)
Train: 13 [ 200/391 ( 51%)]  Loss:  0.537274 (0.4912)  Time: 0.791s,  161.85/s  (0.790s,  162.03/s)  LR: 9.896e-02  Data: 0.045 (0.044)
Train: 13 [ 250/391 ( 64%)]  Loss:  0.458180 (0.4857)  Time: 0.790s,  162.10/s  (0.790s,  161.99/s)  LR: 9.896e-02  Data: 0.044 (0.044)
Train: 13 [ 300/391 ( 77%)]  Loss:  0.505802 (0.4885)  Time: 0.791s,  161.85/s  (0.790s,  161.97/s)  LR: 9.896e-02  Data: 0.045 (0.044)
Train: 13 [ 350/391 ( 90%)]  Loss:  0.650710 (0.5088)  Time: 0.789s,  162.25/s  (0.790s,  161.96/s)  LR: 9.896e-02  Data: 0.043 (0.044)
Train: 13 [ 390/391 (100%)]  Loss:  0.497982 (0.5080)  Time: 0.795s,  100.57/s  (0.790s,  101.23/s)  LR: 9.896e-02  Data: 0.033 (0.044)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.5786 (0.5786)  Acc@1: 80.0000 (80.0000)  Acc@5: 98.0000 (98.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.6187 (0.6277)  Acc@1: 77.0000 (79.3529)  Acc@5: 99.0000 (98.7059)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.5635 (0.6110)  Acc@1: 81.0000 (79.7600)  Acc@5: 98.0000 (98.7600)
tb
loss  0 : 3.71158203125 13
loss  1 : 0.70653564453125 13
loss  2 : 1.365361328125 13
loss  3 : 1.3099658203125 13
{'loss_different_0': 3.71158203125, 'loss_different_1': 0.70653564453125, 'loss_different_2': 1.365361328125, 'loss_different_3': 1.3099658203125}
Top1:  79.76
Train: 14 [   0/391 (  0%)]  Loss:  0.499752 (0.4998)  Time: 0.792s,  161.71/s  (0.792s,  161.71/s)  LR: 9.880e-02  Data: 0.046 (0.046)
Train: 14 [  50/391 ( 13%)]  Loss:  0.482914 (0.4913)  Time: 0.792s,  161.63/s  (0.791s,  161.89/s)  LR: 9.880e-02  Data: 0.045 (0.045)
Train: 14 [ 100/391 ( 26%)]  Loss:  0.384391 (0.4557)  Time: 0.790s,  161.98/s  (0.791s,  161.90/s)  LR: 9.880e-02  Data: 0.044 (0.045)
Train: 14 [ 150/391 ( 38%)]  Loss:  0.468503 (0.4589)  Time: 0.790s,  162.10/s  (0.791s,  161.90/s)  LR: 9.880e-02  Data: 0.043 (0.045)
Train: 14 [ 200/391 ( 51%)]  Loss:  0.524890 (0.4721)  Time: 0.789s,  162.31/s  (0.791s,  161.91/s)  LR: 9.880e-02  Data: 0.042 (0.044)
Train: 14 [ 250/391 ( 64%)]  Loss:  0.445911 (0.4677)  Time: 0.790s,  162.04/s  (0.791s,  161.91/s)  LR: 9.880e-02  Data: 0.043 (0.044)
Train: 14 [ 300/391 ( 77%)]  Loss:  0.576243 (0.4832)  Time: 0.788s,  162.47/s  (0.790s,  161.93/s)  LR: 9.880e-02  Data: 0.042 (0.044)
Train: 14 [ 350/391 ( 90%)]  Loss:  0.530426 (0.4891)  Time: 0.791s,  161.85/s  (0.790s,  161.96/s)  LR: 9.880e-02  Data: 0.044 (0.044)
Train: 14 [ 390/391 (100%)]  Loss:  0.484846 (0.4888)  Time: 0.508s,  157.63/s  (0.790s,  101.33/s)  LR: 9.880e-02  Data: 0.028 (0.044)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.023 (0.023)  Loss:  0.5352 (0.5352)  Acc@1: 84.0000 (84.0000)  Acc@5: 98.0000 (98.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.021)  Loss:  0.6226 (0.6342)  Acc@1: 80.0000 (79.0784)  Acc@5: 97.0000 (98.4510)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.021)  Loss:  0.6338 (0.6318)  Acc@1: 82.0000 (79.1600)  Acc@5: 98.0000 (98.5600)
tb
loss  0 : 3.5751171875 14
loss  1 : 0.72434326171875 14
loss  2 : 1.411884765625 14
loss  3 : 1.268681640625 14
{'loss_different_0': 3.5751171875, 'loss_different_1': 0.72434326171875, 'loss_different_2': 1.411884765625, 'loss_different_3': 1.268681640625}
Top1:  79.16
Train: 15 [   0/391 (  0%)]  Loss:  0.327289 (0.3273)  Time: 0.794s,  161.20/s  (0.794s,  161.20/s)  LR: 9.862e-02  Data: 0.048 (0.048)
Train: 15 [  50/391 ( 13%)]  Loss:  0.591208 (0.4592)  Time: 0.788s,  162.49/s  (0.790s,  161.95/s)  LR: 9.862e-02  Data: 0.041 (0.044)
Train: 15 [ 100/391 ( 26%)]  Loss:  0.579579 (0.4994)  Time: 0.789s,  162.19/s  (0.790s,  162.00/s)  LR: 9.862e-02  Data: 0.044 (0.044)
Train: 15 [ 150/391 ( 38%)]  Loss:  0.452985 (0.4878)  Time: 0.789s,  162.15/s  (0.790s,  161.98/s)  LR: 9.862e-02  Data: 0.043 (0.044)
Train: 15 [ 200/391 ( 51%)]  Loss:  0.395671 (0.4693)  Time: 0.790s,  162.11/s  (0.790s,  161.98/s)  LR: 9.862e-02  Data: 0.043 (0.044)
Train: 15 [ 250/391 ( 64%)]  Loss:  0.479671 (0.4711)  Time: 0.789s,  162.25/s  (0.790s,  161.98/s)  LR: 9.862e-02  Data: 0.042 (0.044)
Train: 15 [ 300/391 ( 77%)]  Loss:  0.555621 (0.4831)  Time: 0.788s,  162.34/s  (0.790s,  161.98/s)  LR: 9.862e-02  Data: 0.042 (0.044)
Train: 15 [ 350/391 ( 90%)]  Loss:  0.485301 (0.4834)  Time: 0.793s,  161.43/s  (0.791s,  161.88/s)  LR: 9.862e-02  Data: 0.046 (0.044)
Train: 15 [ 390/391 (100%)]  Loss:  0.587186 (0.4909)  Time: 0.510s,  156.99/s  (0.790s,  101.26/s)  LR: 9.862e-02  Data: 0.030 (0.044)
Test: [   0/99]  Time: 0.236 (0.236)  DataTime: 0.024 (0.024)  Loss:  0.5703 (0.5703)  Acc@1: 79.0000 (79.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.5825 (0.5618)  Acc@1: 77.0000 (81.0784)  Acc@5: 98.0000 (98.7843)
Test: [  99/99]  Time: 0.235 (0.234)  DataTime: 0.023 (0.022)  Loss:  0.4685 (0.5518)  Acc@1: 81.0000 (81.0200)  Acc@5: 100.0000 (98.9400)
tb
loss  0 : 3.28412109375 15
loss  1 : 0.70229248046875 15
loss  2 : 1.451376953125 15
loss  3 : 0.94638427734375 15
{'loss_different_0': 3.28412109375, 'loss_different_1': 0.70229248046875, 'loss_different_2': 1.451376953125, 'loss_different_3': 0.94638427734375}
Top1:  81.02
Train: 16 [   0/391 (  0%)]  Loss:  0.511940 (0.5119)  Time: 0.795s,  161.04/s  (0.795s,  161.04/s)  LR: 9.843e-02  Data: 0.048 (0.048)
Train: 16 [  50/391 ( 13%)]  Loss:  0.588030 (0.5500)  Time: 0.789s,  162.15/s  (0.791s,  161.78/s)  LR: 9.843e-02  Data: 0.044 (0.045)
Train: 16 [ 100/391 ( 26%)]  Loss:  0.497038 (0.5323)  Time: 0.790s,  162.02/s  (0.791s,  161.78/s)  LR: 9.843e-02  Data: 0.044 (0.045)
Train: 16 [ 150/391 ( 38%)]  Loss:  0.451101 (0.5120)  Time: 0.788s,  162.39/s  (0.791s,  161.87/s)  LR: 9.843e-02  Data: 0.043 (0.045)
Train: 16 [ 200/391 ( 51%)]  Loss:  0.404881 (0.4906)  Time: 0.788s,  162.35/s  (0.791s,  161.89/s)  LR: 9.843e-02  Data: 0.043 (0.045)
Train: 16 [ 250/391 ( 64%)]  Loss:  0.532166 (0.4975)  Time: 0.791s,  161.77/s  (0.790s,  161.93/s)  LR: 9.843e-02  Data: 0.045 (0.044)
Train: 16 [ 300/391 ( 77%)]  Loss:  0.387844 (0.4819)  Time: 0.791s,  161.80/s  (0.790s,  161.96/s)  LR: 9.843e-02  Data: 0.046 (0.044)
Train: 16 [ 350/391 ( 90%)]  Loss:  0.449037 (0.4778)  Time: 0.792s,  161.68/s  (0.790s,  161.97/s)  LR: 9.843e-02  Data: 0.046 (0.044)
Train: 16 [ 390/391 (100%)]  Loss:  0.363098 (0.4694)  Time: 0.510s,  156.95/s  (0.789s,  101.33/s)  LR: 9.843e-02  Data: 0.030 (0.044)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.023 (0.023)  Loss:  0.5435 (0.5435)  Acc@1: 81.0000 (81.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.7861 (0.7156)  Acc@1: 78.0000 (76.4902)  Acc@5: 99.0000 (98.6667)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.8540 (0.7074)  Acc@1: 73.0000 (76.7800)  Acc@5: 98.0000 (98.8200)
tb
loss  0 : 3.27837890625 16
loss  1 : 0.92818115234375 16
loss  2 : 1.3873486328125 16
loss  3 : 1.4565625 16
{'loss_different_0': 3.27837890625, 'loss_different_1': 0.92818115234375, 'loss_different_2': 1.3873486328125, 'loss_different_3': 1.4565625}
Top1:  76.78
Train: 17 [   0/391 (  0%)]  Loss:  0.467170 (0.4672)  Time: 0.792s,  161.52/s  (0.792s,  161.52/s)  LR: 9.823e-02  Data: 0.047 (0.047)
Train: 17 [  50/391 ( 13%)]  Loss:  0.368998 (0.4181)  Time: 0.790s,  162.05/s  (0.789s,  162.25/s)  LR: 9.823e-02  Data: 0.044 (0.043)
Train: 17 [ 100/391 ( 26%)]  Loss:  0.356862 (0.3977)  Time: 0.787s,  162.61/s  (0.788s,  162.34/s)  LR: 9.823e-02  Data: 0.042 (0.042)
Train: 17 [ 150/391 ( 38%)]  Loss:  0.443775 (0.4092)  Time: 0.786s,  162.95/s  (0.790s,  162.02/s)  LR: 9.823e-02  Data: 0.040 (0.042)
Train: 17 [ 200/391 ( 51%)]  Loss:  0.473173 (0.4220)  Time: 0.788s,  162.46/s  (0.790s,  162.11/s)  LR: 9.823e-02  Data: 0.042 (0.042)
Train: 17 [ 250/391 ( 64%)]  Loss:  0.552917 (0.4438)  Time: 0.788s,  162.40/s  (0.789s,  162.15/s)  LR: 9.823e-02  Data: 0.043 (0.042)
Train: 17 [ 300/391 ( 77%)]  Loss:  0.380847 (0.4348)  Time: 0.788s,  162.47/s  (0.789s,  162.18/s)  LR: 9.823e-02  Data: 0.042 (0.042)
Train: 17 [ 350/391 ( 90%)]  Loss:  0.417162 (0.4326)  Time: 0.789s,  162.18/s  (0.789s,  162.20/s)  LR: 9.823e-02  Data: 0.043 (0.042)
Train: 17 [ 390/391 (100%)]  Loss:  0.352707 (0.4268)  Time: 0.509s,  157.13/s  (0.788s,  101.47/s)  LR: 9.823e-02  Data: 0.029 (0.042)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.5903 (0.5903)  Acc@1: 79.0000 (79.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5098 (0.6065)  Acc@1: 86.0000 (80.2353)  Acc@5: 98.0000 (98.6863)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5317 (0.6100)  Acc@1: 84.0000 (79.9300)  Acc@5: 100.0000 (98.7400)
tb
loss  0 : 3.21767578125 17
loss  1 : 0.73658203125 17
loss  2 : 1.434296875 17
loss  3 : 1.1333984375 17
{'loss_different_0': 3.21767578125, 'loss_different_1': 0.73658203125, 'loss_different_2': 1.434296875, 'loss_different_3': 1.1333984375}
Top1:  79.93
Train: 18 [   0/391 (  0%)]  Loss:  0.428702 (0.4287)  Time: 0.790s,  162.02/s  (0.790s,  162.02/s)  LR: 9.801e-02  Data: 0.044 (0.044)
Train: 18 [  50/391 ( 13%)]  Loss:  0.585349 (0.5070)  Time: 0.789s,  162.30/s  (0.788s,  162.44/s)  LR: 9.801e-02  Data: 0.042 (0.042)
Train: 18 [ 100/391 ( 26%)]  Loss:  0.370467 (0.4615)  Time: 0.789s,  162.26/s  (0.788s,  162.44/s)  LR: 9.801e-02  Data: 0.043 (0.042)
Train: 18 [ 150/391 ( 38%)]  Loss:  0.415189 (0.4499)  Time: 0.789s,  162.18/s  (0.788s,  162.40/s)  LR: 9.801e-02  Data: 0.043 (0.042)
Train: 18 [ 200/391 ( 51%)]  Loss:  0.440539 (0.4480)  Time: 0.789s,  162.19/s  (0.788s,  162.37/s)  LR: 9.801e-02  Data: 0.044 (0.042)
Train: 18 [ 250/391 ( 64%)]  Loss:  0.318527 (0.4265)  Time: 0.786s,  162.90/s  (0.788s,  162.35/s)  LR: 9.801e-02  Data: 0.041 (0.042)
Train: 18 [ 300/391 ( 77%)]  Loss:  0.571820 (0.4472)  Time: 0.788s,  162.42/s  (0.788s,  162.34/s)  LR: 9.801e-02  Data: 0.042 (0.042)
Train: 18 [ 350/391 ( 90%)]  Loss:  0.377114 (0.4385)  Time: 0.788s,  162.37/s  (0.788s,  162.36/s)  LR: 9.801e-02  Data: 0.042 (0.042)
Train: 18 [ 390/391 (100%)]  Loss:  0.481142 (0.4416)  Time: 0.510s,  156.97/s  (0.788s,  101.56/s)  LR: 9.801e-02  Data: 0.030 (0.042)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.7725 (0.7725)  Acc@1: 74.0000 (74.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.8008 (0.8614)  Acc@1: 77.0000 (72.7451)  Acc@5: 99.0000 (98.3725)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  1.0713 (0.8524)  Acc@1: 70.0000 (72.7600)  Acc@5: 98.0000 (98.4700)
tb
loss  0 : 3.15724609375 18
loss  1 : 0.9341845703125 18
loss  2 : 1.904345703125 18
loss  3 : 1.4755224609375 18
{'loss_different_0': 3.15724609375, 'loss_different_1': 0.9341845703125, 'loss_different_2': 1.904345703125, 'loss_different_3': 1.4755224609375}
Top1:  72.76
Train: 19 [   0/391 (  0%)]  Loss:  0.308105 (0.3081)  Time: 0.790s,  162.11/s  (0.790s,  162.11/s)  LR: 9.779e-02  Data: 0.044 (0.044)
Train: 19 [  50/391 ( 13%)]  Loss:  0.417159 (0.3626)  Time: 0.789s,  162.27/s  (0.788s,  162.42/s)  LR: 9.779e-02  Data: 0.042 (0.042)
Train: 19 [ 100/391 ( 26%)]  Loss:  0.509051 (0.4114)  Time: 0.791s,  161.90/s  (0.790s,  162.00/s)  LR: 9.779e-02  Data: 0.045 (0.043)
Train: 19 [ 150/391 ( 38%)]  Loss:  0.466630 (0.4252)  Time: 0.790s,  162.00/s  (0.790s,  162.06/s)  LR: 9.779e-02  Data: 0.044 (0.043)
Train: 19 [ 200/391 ( 51%)]  Loss:  0.574561 (0.4551)  Time: 0.791s,  161.84/s  (0.790s,  162.10/s)  LR: 9.779e-02  Data: 0.044 (0.043)
Train: 19 [ 250/391 ( 64%)]  Loss:  0.494271 (0.4616)  Time: 0.790s,  162.06/s  (0.790s,  162.12/s)  LR: 9.779e-02  Data: 0.043 (0.043)
Train: 19 [ 300/391 ( 77%)]  Loss:  0.425807 (0.4565)  Time: 0.791s,  161.87/s  (0.790s,  162.12/s)  LR: 9.779e-02  Data: 0.045 (0.043)
Train: 19 [ 350/391 ( 90%)]  Loss:  0.410674 (0.4508)  Time: 0.789s,  162.33/s  (0.790s,  162.12/s)  LR: 9.779e-02  Data: 0.041 (0.043)
Train: 19 [ 390/391 (100%)]  Loss:  0.673983 (0.4670)  Time: 0.509s,  157.14/s  (0.789s,  101.45/s)  LR: 9.779e-02  Data: 0.030 (0.043)
Test: [   0/99]  Time: 0.231 (0.231)  DataTime: 0.020 (0.020)  Loss:  0.5371 (0.5371)  Acc@1: 82.0000 (82.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.234 (0.232)  DataTime: 0.021 (0.020)  Loss:  0.5679 (0.6359)  Acc@1: 81.0000 (80.0196)  Acc@5: 99.0000 (98.6863)
Test: [  99/99]  Time: 0.233 (0.232)  DataTime: 0.021 (0.020)  Loss:  0.5894 (0.6180)  Acc@1: 80.0000 (80.0800)  Acc@5: 98.0000 (98.7600)
tb
loss  0 : 3.14900390625 19
loss  1 : 0.7025537109375 19
loss  2 : 1.677490234375 19
loss  3 : 1.232998046875 19
{'loss_different_0': 3.14900390625, 'loss_different_1': 0.7025537109375, 'loss_different_2': 1.677490234375, 'loss_different_3': 1.232998046875}
Top1:  80.08
Train: 20 [   0/391 (  0%)]  Loss:  0.489030 (0.4890)  Time: 0.791s,  161.89/s  (0.791s,  161.89/s)  LR: 9.755e-02  Data: 0.045 (0.045)
Train: 20 [  50/391 ( 13%)]  Loss:  0.274867 (0.3819)  Time: 0.788s,  162.40/s  (0.788s,  162.43/s)  LR: 9.755e-02  Data: 0.041 (0.042)
Train: 20 [ 100/391 ( 26%)]  Loss:  0.548613 (0.4375)  Time: 0.787s,  162.57/s  (0.788s,  162.47/s)  LR: 9.755e-02  Data: 0.041 (0.042)
Train: 20 [ 150/391 ( 38%)]  Loss:  0.431231 (0.4359)  Time: 0.786s,  162.88/s  (0.788s,  162.47/s)  LR: 9.755e-02  Data: 0.040 (0.042)
Train: 20 [ 200/391 ( 51%)]  Loss:  0.479866 (0.4447)  Time: 0.786s,  162.77/s  (0.788s,  162.47/s)  LR: 9.755e-02  Data: 0.041 (0.042)
Train: 20 [ 250/391 ( 64%)]  Loss:  0.425719 (0.4416)  Time: 0.788s,  162.35/s  (0.789s,  162.25/s)  LR: 9.755e-02  Data: 0.043 (0.042)
Train: 20 [ 300/391 ( 77%)]  Loss:  0.410607 (0.4371)  Time: 0.788s,  162.49/s  (0.789s,  162.24/s)  LR: 9.755e-02  Data: 0.042 (0.042)
Train: 20 [ 350/391 ( 90%)]  Loss:  0.471761 (0.4415)  Time: 0.791s,  161.83/s  (0.789s,  162.25/s)  LR: 9.755e-02  Data: 0.044 (0.042)
Train: 20 [ 390/391 (100%)]  Loss:  0.486190 (0.4447)  Time: 0.507s,  157.67/s  (0.788s,  101.51/s)  LR: 9.755e-02  Data: 0.028 (0.042)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.5889 (0.5889)  Acc@1: 81.0000 (81.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  0.8262 (0.7553)  Acc@1: 76.0000 (76.8627)  Acc@5: 98.0000 (98.3333)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.7583 (0.7463)  Acc@1: 79.0000 (77.0300)  Acc@5: 97.0000 (98.5100)
tb
loss  0 : 3.1271875 20
loss  1 : 0.87820068359375 20
loss  2 : 1.78654296875 20
loss  3 : 1.31146484375 20
{'loss_different_0': 3.1271875, 'loss_different_1': 0.87820068359375, 'loss_different_2': 1.78654296875, 'loss_different_3': 1.31146484375}
Top1:  77.03
Train: 21 [   0/391 (  0%)]  Loss:  0.528085 (0.5281)  Time: 0.790s,  161.96/s  (0.790s,  161.96/s)  LR: 9.730e-02  Data: 0.044 (0.044)
Train: 21 [  50/391 ( 13%)]  Loss:  0.476320 (0.5022)  Time: 0.786s,  162.89/s  (0.787s,  162.57/s)  LR: 9.730e-02  Data: 0.040 (0.041)
Train: 21 [ 100/391 ( 26%)]  Loss:  0.592635 (0.5323)  Time: 0.789s,  162.28/s  (0.787s,  162.59/s)  LR: 9.730e-02  Data: 0.042 (0.041)
Train: 21 [ 150/391 ( 38%)]  Loss:  0.308536 (0.4764)  Time: 0.787s,  162.64/s  (0.788s,  162.53/s)  LR: 9.730e-02  Data: 0.041 (0.042)
Train: 21 [ 200/391 ( 51%)]  Loss:  0.425573 (0.4662)  Time: 0.787s,  162.70/s  (0.787s,  162.55/s)  LR: 9.730e-02  Data: 0.041 (0.041)
Train: 21 [ 250/391 ( 64%)]  Loss:  0.488269 (0.4699)  Time: 0.790s,  162.12/s  (0.788s,  162.54/s)  LR: 9.730e-02  Data: 0.043 (0.042)
Train: 21 [ 300/391 ( 77%)]  Loss:  0.452805 (0.4675)  Time: 0.787s,  162.56/s  (0.788s,  162.53/s)  LR: 9.730e-02  Data: 0.041 (0.042)
Train: 21 [ 350/391 ( 90%)]  Loss:  0.567451 (0.4800)  Time: 0.788s,  162.53/s  (0.788s,  162.52/s)  LR: 9.730e-02  Data: 0.041 (0.042)
Train: 21 [ 390/391 (100%)]  Loss:  0.495486 (0.4811)  Time: 0.508s,  157.46/s  (0.787s,  101.67/s)  LR: 9.730e-02  Data: 0.028 (0.042)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5825 (0.5825)  Acc@1: 81.0000 (81.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.5249 (0.5364)  Acc@1: 84.0000 (82.5294)  Acc@5: 98.0000 (99.0784)
Test: [  99/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.4883 (0.5285)  Acc@1: 84.0000 (82.5500)  Acc@5: 100.0000 (99.1800)
tb
loss  0 : 3.09787109375 21
loss  1 : 0.68199462890625 21
loss  2 : 1.3441015625 21
loss  3 : 0.9300244140625 21
{'loss_different_0': 3.09787109375, 'loss_different_1': 0.68199462890625, 'loss_different_2': 1.3441015625, 'loss_different_3': 0.9300244140625}
Top1:  82.55
Train: 22 [   0/391 (  0%)]  Loss:  0.450856 (0.4509)  Time: 0.787s,  162.56/s  (0.787s,  162.56/s)  LR: 9.704e-02  Data: 0.042 (0.042)
Train: 22 [  50/391 ( 13%)]  Loss:  0.329845 (0.3904)  Time: 0.787s,  162.64/s  (0.787s,  162.61/s)  LR: 9.704e-02  Data: 0.041 (0.041)
Train: 22 [ 100/391 ( 26%)]  Loss:  0.541858 (0.4409)  Time: 0.787s,  162.56/s  (0.787s,  162.59/s)  LR: 9.704e-02  Data: 0.042 (0.041)
Train: 22 [ 150/391 ( 38%)]  Loss:  0.477253 (0.4500)  Time: 0.786s,  162.78/s  (0.787s,  162.59/s)  LR: 9.704e-02  Data: 0.041 (0.041)
Train: 22 [ 200/391 ( 51%)]  Loss:  0.445994 (0.4492)  Time: 0.787s,  162.59/s  (0.788s,  162.50/s)  LR: 9.704e-02  Data: 0.041 (0.041)
Train: 22 [ 250/391 ( 64%)]  Loss:  0.557283 (0.4672)  Time: 0.788s,  162.53/s  (0.788s,  162.53/s)  LR: 9.704e-02  Data: 0.042 (0.041)
Train: 22 [ 300/391 ( 77%)]  Loss:  0.506236 (0.4728)  Time: 0.785s,  162.99/s  (0.787s,  162.54/s)  LR: 9.704e-02  Data: 0.040 (0.041)
Train: 22 [ 350/391 ( 90%)]  Loss:  0.522460 (0.4790)  Time: 0.787s,  162.74/s  (0.787s,  162.54/s)  LR: 9.704e-02  Data: 0.041 (0.041)
Train: 22 [ 390/391 (100%)]  Loss:  0.405150 (0.4736)  Time: 0.508s,  157.59/s  (0.787s,  101.69/s)  LR: 9.704e-02  Data: 0.028 (0.041)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.4844 (0.4844)  Acc@1: 84.0000 (84.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.233 (0.232)  DataTime: 0.021 (0.020)  Loss:  0.4084 (0.5557)  Acc@1: 90.0000 (82.3529)  Acc@5: 99.0000 (98.9412)
Test: [  99/99]  Time: 0.231 (0.232)  DataTime: 0.019 (0.020)  Loss:  0.5254 (0.5556)  Acc@1: 84.0000 (82.1400)  Acc@5: 98.0000 (98.9700)
tb
loss  0 : 3.1312890625 22
loss  1 : 0.7085205078125 22
loss  2 : 1.540908203125 22
loss  3 : 0.89288330078125 22
{'loss_different_0': 3.1312890625, 'loss_different_1': 0.7085205078125, 'loss_different_2': 1.540908203125, 'loss_different_3': 0.89288330078125}
Top1:  82.14
Train: 23 [   0/391 (  0%)]  Loss:  0.323962 (0.3240)  Time: 0.788s,  162.40/s  (0.788s,  162.40/s)  LR: 9.677e-02  Data: 0.042 (0.042)
Train: 23 [  50/391 ( 13%)]  Loss:  0.504064 (0.4140)  Time: 0.786s,  162.90/s  (0.787s,  162.59/s)  LR: 9.677e-02  Data: 0.040 (0.041)
Train: 23 [ 100/391 ( 26%)]  Loss:  0.449037 (0.4257)  Time: 0.786s,  162.78/s  (0.787s,  162.64/s)  LR: 9.677e-02  Data: 0.040 (0.041)
Train: 23 [ 150/391 ( 38%)]  Loss:  0.336443 (0.4034)  Time: 0.787s,  162.66/s  (0.787s,  162.63/s)  LR: 9.677e-02  Data: 0.040 (0.041)
Train: 23 [ 200/391 ( 51%)]  Loss:  0.467026 (0.4161)  Time: 0.788s,  162.49/s  (0.787s,  162.62/s)  LR: 9.677e-02  Data: 0.042 (0.041)
Train: 23 [ 250/391 ( 64%)]  Loss:  0.397570 (0.4130)  Time: 0.788s,  162.44/s  (0.787s,  162.57/s)  LR: 9.677e-02  Data: 0.043 (0.041)
Train: 23 [ 300/391 ( 77%)]  Loss:  0.368885 (0.4067)  Time: 0.788s,  162.53/s  (0.788s,  162.54/s)  LR: 9.677e-02  Data: 0.042 (0.042)
Train: 23 [ 350/391 ( 90%)]  Loss:  0.456052 (0.4129)  Time: 0.789s,  162.24/s  (0.788s,  162.40/s)  LR: 9.677e-02  Data: 0.042 (0.042)
Train: 23 [ 390/391 (100%)]  Loss:  0.453006 (0.4158)  Time: 0.508s,  157.55/s  (0.787s,  101.59/s)  LR: 9.677e-02  Data: 0.028 (0.042)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.7539 (0.7539)  Acc@1: 77.0000 (77.0000)  Acc@5: 97.0000 (97.0000)
Test: [  50/99]  Time: 0.233 (0.232)  DataTime: 0.021 (0.020)  Loss:  0.7012 (0.6560)  Acc@1: 80.0000 (79.3333)  Acc@5: 99.0000 (98.2353)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.6465 (0.6454)  Acc@1: 79.0000 (79.3200)  Acc@5: 100.0000 (98.4400)
tb
loss  0 : 3.01255859375 23
loss  1 : 0.863974609375 23
loss  2 : 1.4125927734375 23
loss  3 : 1.3300048828125 23
{'loss_different_0': 3.01255859375, 'loss_different_1': 0.863974609375, 'loss_different_2': 1.4125927734375, 'loss_different_3': 1.3300048828125}
Top1:  79.32
Train: 24 [   0/391 (  0%)]  Loss:  0.564640 (0.5646)  Time: 0.790s,  161.93/s  (0.790s,  161.93/s)  LR: 9.649e-02  Data: 0.044 (0.044)
Train: 24 [  50/391 ( 13%)]  Loss:  0.332916 (0.4488)  Time: 0.787s,  162.58/s  (0.787s,  162.58/s)  LR: 9.649e-02  Data: 0.041 (0.041)
Train: 24 [ 100/391 ( 26%)]  Loss:  0.427992 (0.4418)  Time: 0.788s,  162.46/s  (0.788s,  162.43/s)  LR: 9.649e-02  Data: 0.041 (0.042)
Train: 24 [ 150/391 ( 38%)]  Loss:  0.361530 (0.4218)  Time: 0.787s,  162.61/s  (0.788s,  162.44/s)  LR: 9.649e-02  Data: 0.041 (0.042)
Train: 24 [ 200/391 ( 51%)]  Loss:  0.421673 (0.4217)  Time: 0.788s,  162.36/s  (0.788s,  162.46/s)  LR: 9.649e-02  Data: 0.042 (0.042)
Train: 24 [ 250/391 ( 64%)]  Loss:  0.369390 (0.4130)  Time: 0.788s,  162.41/s  (0.788s,  162.45/s)  LR: 9.649e-02  Data: 0.042 (0.042)
Train: 24 [ 300/391 ( 77%)]  Loss:  0.525254 (0.4291)  Time: 0.786s,  162.76/s  (0.788s,  162.46/s)  LR: 9.649e-02  Data: 0.041 (0.042)
Train: 24 [ 350/391 ( 90%)]  Loss:  0.436555 (0.4300)  Time: 0.788s,  162.39/s  (0.788s,  162.47/s)  LR: 9.649e-02  Data: 0.042 (0.042)
Train: 24 [ 390/391 (100%)]  Loss:  0.418024 (0.4291)  Time: 0.508s,  157.36/s  (0.787s,  101.63/s)  LR: 9.649e-02  Data: 0.029 (0.042)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.8882 (0.8882)  Acc@1: 77.0000 (77.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.9170 (0.8381)  Acc@1: 76.0000 (76.4902)  Acc@5: 99.0000 (98.3725)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  0.7939 (0.8313)  Acc@1: 76.0000 (76.4500)  Acc@5: 99.0000 (98.4500)
tb
loss  0 : 3.1139453125 24
loss  1 : 1.1739892578125 24
loss  2 : 1.882548828125 24
loss  3 : 1.4896826171875 24
{'loss_different_0': 3.1139453125, 'loss_different_1': 1.1739892578125, 'loss_different_2': 1.882548828125, 'loss_different_3': 1.4896826171875}
Top1:  76.45
Train: 25 [   0/391 (  0%)]  Loss:  0.432390 (0.4324)  Time: 0.789s,  162.26/s  (0.789s,  162.26/s)  LR: 9.619e-02  Data: 0.043 (0.043)
Train: 25 [  50/391 ( 13%)]  Loss:  0.408545 (0.4205)  Time: 0.787s,  162.60/s  (0.788s,  162.39/s)  LR: 9.619e-02  Data: 0.041 (0.042)
Train: 25 [ 100/391 ( 26%)]  Loss:  0.446864 (0.4293)  Time: 0.787s,  162.73/s  (0.788s,  162.44/s)  LR: 9.619e-02  Data: 0.041 (0.042)
Train: 25 [ 150/391 ( 38%)]  Loss:  0.365229 (0.4133)  Time: 0.789s,  162.20/s  (0.788s,  162.45/s)  LR: 9.619e-02  Data: 0.042 (0.042)
Train: 25 [ 200/391 ( 51%)]  Loss:  0.479747 (0.4266)  Time: 0.788s,  162.43/s  (0.788s,  162.45/s)  LR: 9.619e-02  Data: 0.042 (0.042)
Train: 25 [ 250/391 ( 64%)]  Loss:  0.535892 (0.4448)  Time: 0.786s,  162.77/s  (0.788s,  162.45/s)  LR: 9.619e-02  Data: 0.041 (0.042)
Train: 25 [ 300/391 ( 77%)]  Loss:  0.388541 (0.4367)  Time: 0.787s,  162.65/s  (0.788s,  162.36/s)  LR: 9.619e-02  Data: 0.041 (0.042)
Train: 25 [ 350/391 ( 90%)]  Loss:  0.405040 (0.4328)  Time: 0.789s,  162.16/s  (0.788s,  162.34/s)  LR: 9.619e-02  Data: 0.043 (0.042)
Train: 25 [ 390/391 (100%)]  Loss:  0.818526 (0.4607)  Time: 0.509s,  157.12/s  (0.788s,  101.54/s)  LR: 9.619e-02  Data: 0.030 (0.042)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.7178 (0.7178)  Acc@1: 78.0000 (78.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.5737 (0.8318)  Acc@1: 82.0000 (74.7255)  Acc@5: 99.0000 (97.7647)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.7427 (0.8429)  Acc@1: 76.0000 (74.1900)  Acc@5: 97.0000 (97.8300)
tb
loss  0 : 3.08533203125 25
loss  1 : 1.0341845703125 25
loss  2 : 1.759267578125 25
loss  3 : 1.708408203125 25
{'loss_different_0': 3.08533203125, 'loss_different_1': 1.0341845703125, 'loss_different_2': 1.759267578125, 'loss_different_3': 1.708408203125}
Top1:  74.19
Train: 26 [   0/391 (  0%)]  Loss:  0.378960 (0.3790)  Time: 0.793s,  161.47/s  (0.793s,  161.47/s)  LR: 9.589e-02  Data: 0.047 (0.047)
Train: 26 [  50/391 ( 13%)]  Loss:  0.393452 (0.3862)  Time: 0.791s,  161.78/s  (0.790s,  162.01/s)  LR: 9.589e-02  Data: 0.045 (0.044)
Train: 26 [ 100/391 ( 26%)]  Loss:  0.356014 (0.3761)  Time: 0.789s,  162.28/s  (0.790s,  162.02/s)  LR: 9.589e-02  Data: 0.044 (0.044)
Train: 26 [ 150/391 ( 38%)]  Loss:  0.324061 (0.3631)  Time: 0.789s,  162.19/s  (0.790s,  162.04/s)  LR: 9.589e-02  Data: 0.044 (0.044)
Train: 26 [ 200/391 ( 51%)]  Loss:  0.470128 (0.3845)  Time: 0.790s,  162.01/s  (0.790s,  162.04/s)  LR: 9.589e-02  Data: 0.044 (0.044)
Train: 26 [ 250/391 ( 64%)]  Loss:  0.375379 (0.3830)  Time: 0.791s,  161.92/s  (0.790s,  162.04/s)  LR: 9.589e-02  Data: 0.044 (0.044)
Train: 26 [ 300/391 ( 77%)]  Loss:  0.388069 (0.3837)  Time: 0.790s,  162.00/s  (0.790s,  162.04/s)  LR: 9.589e-02  Data: 0.044 (0.044)
Train: 26 [ 350/391 ( 90%)]  Loss:  0.562235 (0.4060)  Time: 0.789s,  162.32/s  (0.790s,  162.05/s)  LR: 9.589e-02  Data: 0.042 (0.044)
Train: 26 [ 390/391 (100%)]  Loss:  0.472505 (0.4109)  Time: 0.510s,  156.85/s  (0.789s,  101.37/s)  LR: 9.589e-02  Data: 0.031 (0.044)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.023 (0.023)  Loss:  0.5747 (0.5747)  Acc@1: 76.0000 (76.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.021)  Loss:  0.6514 (0.6899)  Acc@1: 78.0000 (77.5686)  Acc@5: 99.0000 (98.8235)
Test: [  99/99]  Time: 0.236 (0.234)  DataTime: 0.024 (0.022)  Loss:  0.6743 (0.6886)  Acc@1: 80.0000 (77.8800)  Acc@5: 97.0000 (98.7500)
tb
loss  0 : 3.00775390625 26
loss  1 : 0.85143798828125 26
loss  2 : 1.643837890625 26
loss  3 : 1.3629345703125 26
{'loss_different_0': 3.00775390625, 'loss_different_1': 0.85143798828125, 'loss_different_2': 1.643837890625, 'loss_different_3': 1.3629345703125}
Top1:  77.88
Train: 27 [   0/391 (  0%)]  Loss:  0.416493 (0.4165)  Time: 0.797s,  160.65/s  (0.797s,  160.65/s)  LR: 9.557e-02  Data: 0.050 (0.050)
Train: 27 [  50/391 ( 13%)]  Loss:  0.367676 (0.3921)  Time: 0.791s,  161.92/s  (0.795s,  161.05/s)  LR: 9.557e-02  Data: 0.044 (0.044)
Train: 27 [ 100/391 ( 26%)]  Loss:  0.357226 (0.3805)  Time: 0.788s,  162.42/s  (0.792s,  161.52/s)  LR: 9.557e-02  Data: 0.042 (0.044)
Train: 27 [ 150/391 ( 38%)]  Loss:  0.253722 (0.3488)  Time: 0.789s,  162.18/s  (0.792s,  161.69/s)  LR: 9.557e-02  Data: 0.044 (0.044)
Train: 27 [ 200/391 ( 51%)]  Loss:  0.388450 (0.3567)  Time: 0.794s,  161.26/s  (0.791s,  161.76/s)  LR: 9.557e-02  Data: 0.048 (0.044)
Train: 27 [ 250/391 ( 64%)]  Loss:  0.418538 (0.3670)  Time: 0.791s,  161.78/s  (0.791s,  161.80/s)  LR: 9.557e-02  Data: 0.045 (0.044)
Train: 27 [ 300/391 ( 77%)]  Loss:  0.353462 (0.3651)  Time: 0.789s,  162.26/s  (0.791s,  161.85/s)  LR: 9.557e-02  Data: 0.043 (0.044)
Train: 27 [ 350/391 ( 90%)]  Loss:  0.452493 (0.3760)  Time: 0.789s,  162.15/s  (0.791s,  161.88/s)  LR: 9.557e-02  Data: 0.044 (0.044)
Train: 27 [ 390/391 (100%)]  Loss:  0.476290 (0.3833)  Time: 0.510s,  156.84/s  (0.790s,  101.28/s)  LR: 9.557e-02  Data: 0.031 (0.044)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.7095 (0.7095)  Acc@1: 74.0000 (74.0000)  Acc@5: 98.0000 (98.0000)
Test: [  50/99]  Time: 0.233 (0.234)  DataTime: 0.020 (0.021)  Loss:  0.7275 (0.7405)  Acc@1: 80.0000 (76.3137)  Acc@5: 100.0000 (98.1176)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.6699 (0.7296)  Acc@1: 79.0000 (76.4300)  Acc@5: 98.0000 (98.4800)
tb
loss  0 : 3.04146484375 27
loss  1 : 0.8950634765625 27
loss  2 : 1.7520703125 27
loss  3 : 1.2369091796875 27
{'loss_different_0': 3.04146484375, 'loss_different_1': 0.8950634765625, 'loss_different_2': 1.7520703125, 'loss_different_3': 1.2369091796875}
Top1:  76.43
Train: 28 [   0/391 (  0%)]  Loss:  0.398012 (0.3980)  Time: 0.793s,  161.38/s  (0.793s,  161.38/s)  LR: 9.524e-02  Data: 0.047 (0.047)
Train: 28 [  50/391 ( 13%)]  Loss:  0.318940 (0.3585)  Time: 0.790s,  162.03/s  (0.790s,  162.10/s)  LR: 9.524e-02  Data: 0.044 (0.044)
Train: 28 [ 100/391 ( 26%)]  Loss:  0.395396 (0.3708)  Time: 0.790s,  162.06/s  (0.790s,  162.03/s)  LR: 9.524e-02  Data: 0.044 (0.044)
Train: 28 [ 150/391 ( 38%)]  Loss:  0.440785 (0.3883)  Time: 0.790s,  162.08/s  (0.790s,  162.02/s)  LR: 9.524e-02  Data: 0.044 (0.044)
Train: 28 [ 200/391 ( 51%)]  Loss:  0.454053 (0.4014)  Time: 0.790s,  162.07/s  (0.790s,  162.00/s)  LR: 9.524e-02  Data: 0.044 (0.044)
Train: 28 [ 250/391 ( 64%)]  Loss:  0.377268 (0.3974)  Time: 0.790s,  162.09/s  (0.790s,  162.00/s)  LR: 9.524e-02  Data: 0.044 (0.044)
Train: 28 [ 300/391 ( 77%)]  Loss:  0.367445 (0.3931)  Time: 0.790s,  162.01/s  (0.790s,  162.01/s)  LR: 9.524e-02  Data: 0.044 (0.044)
Train: 28 [ 350/391 ( 90%)]  Loss:  0.402028 (0.3942)  Time: 0.790s,  162.11/s  (0.790s,  162.04/s)  LR: 9.524e-02  Data: 0.043 (0.044)
Train: 28 [ 390/391 (100%)]  Loss:  0.446011 (0.3980)  Time: 0.510s,  156.78/s  (0.789s,  101.34/s)  LR: 9.524e-02  Data: 0.031 (0.044)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.5547 (0.5547)  Acc@1: 84.0000 (84.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  0.6128 (0.5813)  Acc@1: 77.0000 (81.0588)  Acc@5: 99.0000 (98.9804)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5698 (0.5725)  Acc@1: 83.0000 (81.2400)  Acc@5: 98.0000 (99.1400)
tb
loss  0 : 2.89953125 28
loss  1 : 0.73586181640625 28
loss  2 : 1.2354345703125 28
loss  3 : 1.15035400390625 28
{'loss_different_0': 2.89953125, 'loss_different_1': 0.73586181640625, 'loss_different_2': 1.2354345703125, 'loss_different_3': 1.15035400390625}
Top1:  81.24
Train: 29 [   0/391 (  0%)]  Loss:  0.386495 (0.3865)  Time: 0.791s,  161.92/s  (0.791s,  161.92/s)  LR: 9.490e-02  Data: 0.045 (0.045)
Train: 29 [  50/391 ( 13%)]  Loss:  0.478023 (0.4323)  Time: 0.788s,  162.39/s  (0.789s,  162.25/s)  LR: 9.490e-02  Data: 0.043 (0.043)
Train: 29 [ 100/391 ( 26%)]  Loss:  0.404002 (0.4228)  Time: 0.789s,  162.15/s  (0.789s,  162.13/s)  LR: 9.490e-02  Data: 0.043 (0.043)
Train: 29 [ 150/391 ( 38%)]  Loss:  0.526478 (0.4487)  Time: 0.791s,  161.79/s  (0.790s,  162.12/s)  LR: 9.490e-02  Data: 0.045 (0.044)
Train: 29 [ 200/391 ( 51%)]  Loss:  0.404998 (0.4400)  Time: 0.791s,  161.91/s  (0.790s,  162.08/s)  LR: 9.490e-02  Data: 0.045 (0.044)
Train: 29 [ 250/391 ( 64%)]  Loss:  0.401692 (0.4336)  Time: 0.789s,  162.21/s  (0.790s,  162.04/s)  LR: 9.490e-02  Data: 0.043 (0.044)
Train: 29 [ 300/391 ( 77%)]  Loss:  0.439641 (0.4345)  Time: 0.788s,  162.46/s  (0.790s,  162.09/s)  LR: 9.490e-02  Data: 0.042 (0.044)
Train: 29 [ 350/391 ( 90%)]  Loss:  0.459518 (0.4376)  Time: 0.788s,  162.45/s  (0.789s,  162.14/s)  LR: 9.490e-02  Data: 0.042 (0.043)
Train: 29 [ 390/391 (100%)]  Loss:  0.274541 (0.4258)  Time: 0.511s,  156.57/s  (0.789s,  101.44/s)  LR: 9.490e-02  Data: 0.031 (0.043)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5923 (0.5923)  Acc@1: 80.0000 (80.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.8408 (0.8124)  Acc@1: 76.0000 (75.1961)  Acc@5: 98.0000 (98.2941)
Test: [  99/99]  Time: 0.234 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.9204 (0.8051)  Acc@1: 71.0000 (75.1600)  Acc@5: 95.0000 (98.3100)
tb
loss  0 : 2.993515625 29
loss  1 : 0.94564208984375 29
loss  2 : 1.89806640625 29
loss  3 : 1.4286669921875 29
{'loss_different_0': 2.993515625, 'loss_different_1': 0.94564208984375, 'loss_different_2': 1.89806640625, 'loss_different_3': 1.4286669921875}
Top1:  75.16
Train: 30 [   0/391 (  0%)]  Loss:  0.326882 (0.3269)  Time: 0.793s,  161.48/s  (0.793s,  161.48/s)  LR: 9.455e-02  Data: 0.046 (0.046)
Train: 30 [  50/391 ( 13%)]  Loss:  0.363158 (0.3450)  Time: 0.789s,  162.17/s  (0.790s,  162.07/s)  LR: 9.455e-02  Data: 0.044 (0.044)
Train: 30 [ 100/391 ( 26%)]  Loss:  0.431843 (0.3740)  Time: 0.787s,  162.54/s  (0.789s,  162.15/s)  LR: 9.455e-02  Data: 0.042 (0.043)
Train: 30 [ 150/391 ( 38%)]  Loss:  0.464355 (0.3966)  Time: 0.790s,  162.09/s  (0.790s,  162.12/s)  LR: 9.455e-02  Data: 0.043 (0.043)
Train: 30 [ 200/391 ( 51%)]  Loss:  0.469219 (0.4111)  Time: 0.789s,  162.22/s  (0.790s,  162.13/s)  LR: 9.455e-02  Data: 0.044 (0.043)
Train: 30 [ 250/391 ( 64%)]  Loss:  0.475601 (0.4218)  Time: 0.786s,  162.75/s  (0.790s,  162.12/s)  LR: 9.455e-02  Data: 0.041 (0.043)
Train: 30 [ 300/391 ( 77%)]  Loss:  0.525417 (0.4366)  Time: 0.791s,  161.85/s  (0.790s,  162.12/s)  LR: 9.455e-02  Data: 0.046 (0.043)
Train: 30 [ 350/391 ( 90%)]  Loss:  0.463908 (0.4400)  Time: 0.794s,  161.23/s  (0.790s,  162.05/s)  LR: 9.455e-02  Data: 0.047 (0.044)
Train: 30 [ 390/391 (100%)]  Loss:  0.355630 (0.4339)  Time: 0.509s,  157.15/s  (0.789s,  101.37/s)  LR: 9.455e-02  Data: 0.030 (0.043)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.3489 (0.3489)  Acc@1: 88.0000 (88.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.235 (0.234)  DataTime: 0.023 (0.022)  Loss:  0.4492 (0.4737)  Acc@1: 85.0000 (84.2549)  Acc@5: 98.0000 (99.1765)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.4922 (0.4684)  Acc@1: 83.0000 (84.2800)  Acc@5: 98.0000 (99.2800)
tb
loss  0 : 2.573359375 30
loss  1 : 0.620343017578125 30
loss  2 : 1.2088037109375 30
loss  3 : 0.78947021484375 30
{'loss_different_0': 2.573359375, 'loss_different_1': 0.620343017578125, 'loss_different_2': 1.2088037109375, 'loss_different_3': 0.78947021484375}
Top1:  84.28
Train: 31 [   0/391 (  0%)]  Loss:  0.338209 (0.3382)  Time: 0.791s,  161.89/s  (0.791s,  161.89/s)  LR: 9.419e-02  Data: 0.045 (0.045)
Train: 31 [  50/391 ( 13%)]  Loss:  0.335630 (0.3369)  Time: 0.788s,  162.45/s  (0.790s,  162.07/s)  LR: 9.419e-02  Data: 0.042 (0.044)
Train: 31 [ 100/391 ( 26%)]  Loss:  0.368812 (0.3476)  Time: 0.791s,  161.88/s  (0.790s,  162.06/s)  LR: 9.419e-02  Data: 0.045 (0.044)
Train: 31 [ 150/391 ( 38%)]  Loss:  0.541791 (0.3961)  Time: 0.788s,  162.41/s  (0.790s,  162.07/s)  LR: 9.419e-02  Data: 0.043 (0.044)
Train: 31 [ 200/391 ( 51%)]  Loss:  0.316028 (0.3801)  Time: 0.789s,  162.31/s  (0.790s,  162.09/s)  LR: 9.419e-02  Data: 0.043 (0.044)
Train: 31 [ 250/391 ( 64%)]  Loss:  0.368427 (0.3781)  Time: 0.788s,  162.34/s  (0.790s,  162.08/s)  LR: 9.419e-02  Data: 0.043 (0.044)
Train: 31 [ 300/391 ( 77%)]  Loss:  0.506117 (0.3964)  Time: 0.787s,  162.59/s  (0.790s,  162.08/s)  LR: 9.419e-02  Data: 0.042 (0.044)
Train: 31 [ 350/391 ( 90%)]  Loss:  0.356830 (0.3915)  Time: 0.790s,  162.04/s  (0.790s,  162.10/s)  LR: 9.419e-02  Data: 0.043 (0.044)
Train: 31 [ 390/391 (100%)]  Loss:  0.576979 (0.4049)  Time: 0.511s,  156.66/s  (0.789s,  101.41/s)  LR: 9.419e-02  Data: 0.031 (0.044)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5444 (0.5444)  Acc@1: 81.0000 (81.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.021)  Loss:  0.5186 (0.5898)  Acc@1: 82.0000 (80.4314)  Acc@5: 99.0000 (98.9412)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.021)  Loss:  0.6597 (0.5737)  Acc@1: 79.0000 (80.8200)  Acc@5: 98.0000 (99.0600)
tb
loss  0 : 2.803046875 31
loss  1 : 0.6801171875 31
loss  2 : 1.3147021484375 31
loss  3 : 1.0409619140625 31
{'loss_different_0': 2.803046875, 'loss_different_1': 0.6801171875, 'loss_different_2': 1.3147021484375, 'loss_different_3': 1.0409619140625}
Top1:  80.82
Train: 32 [   0/391 (  0%)]  Loss:  0.413577 (0.4136)  Time: 0.792s,  161.60/s  (0.792s,  161.60/s)  LR: 9.382e-02  Data: 0.046 (0.046)
Train: 32 [  50/391 ( 13%)]  Loss:  0.504914 (0.4592)  Time: 0.787s,  162.65/s  (0.789s,  162.24/s)  LR: 9.382e-02  Data: 0.041 (0.043)
Train: 32 [ 100/391 ( 26%)]  Loss:  0.182144 (0.3669)  Time: 0.790s,  162.06/s  (0.792s,  161.66/s)  LR: 9.382e-02  Data: 0.044 (0.044)
Train: 32 [ 150/391 ( 38%)]  Loss:  0.238471 (0.3348)  Time: 0.792s,  161.71/s  (0.791s,  161.81/s)  LR: 9.382e-02  Data: 0.046 (0.044)
Train: 32 [ 200/391 ( 51%)]  Loss:  0.449224 (0.3577)  Time: 0.788s,  162.43/s  (0.791s,  161.90/s)  LR: 9.382e-02  Data: 0.042 (0.043)
Train: 32 [ 250/391 ( 64%)]  Loss:  0.406115 (0.3657)  Time: 0.791s,  161.74/s  (0.790s,  161.94/s)  LR: 9.382e-02  Data: 0.046 (0.044)
Train: 32 [ 300/391 ( 77%)]  Loss:  0.540510 (0.3907)  Time: 0.787s,  162.67/s  (0.790s,  161.97/s)  LR: 9.382e-02  Data: 0.042 (0.044)
Train: 32 [ 350/391 ( 90%)]  Loss:  0.515293 (0.4063)  Time: 0.789s,  162.21/s  (0.790s,  161.99/s)  LR: 9.382e-02  Data: 0.043 (0.044)
Train: 32 [ 390/391 (100%)]  Loss:  0.418538 (0.4072)  Time: 0.508s,  157.41/s  (0.789s,  101.34/s)  LR: 9.382e-02  Data: 0.029 (0.044)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.6074 (0.6074)  Acc@1: 82.0000 (82.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.021)  Loss:  0.5537 (0.5404)  Acc@1: 78.0000 (82.7451)  Acc@5: 99.0000 (99.2353)
Test: [  99/99]  Time: 0.237 (0.234)  DataTime: 0.024 (0.021)  Loss:  0.4319 (0.5374)  Acc@1: 86.0000 (82.7300)  Acc@5: 98.0000 (99.1700)
tb
loss  0 : 2.8130859375 32
loss  1 : 0.7009423828125 32
loss  2 : 1.4680078125 32
loss  3 : 0.95524658203125 32
{'loss_different_0': 2.8130859375, 'loss_different_1': 0.7009423828125, 'loss_different_2': 1.4680078125, 'loss_different_3': 0.95524658203125}
Top1:  82.73
Train: 33 [   0/391 (  0%)]  Loss:  0.315114 (0.3151)  Time: 0.793s,  161.32/s  (0.793s,  161.32/s)  LR: 9.343e-02  Data: 0.047 (0.047)
Train: 33 [  50/391 ( 13%)]  Loss:  0.320026 (0.3176)  Time: 0.790s,  162.07/s  (0.790s,  162.02/s)  LR: 9.343e-02  Data: 0.044 (0.044)
Train: 33 [ 100/391 ( 26%)]  Loss:  0.367921 (0.3344)  Time: 0.795s,  161.00/s  (0.790s,  162.07/s)  LR: 9.343e-02  Data: 0.049 (0.044)
Train: 33 [ 150/391 ( 38%)]  Loss:  0.565153 (0.3921)  Time: 0.787s,  162.59/s  (0.790s,  162.05/s)  LR: 9.343e-02  Data: 0.042 (0.044)
Train: 33 [ 200/391 ( 51%)]  Loss:  0.431777 (0.4000)  Time: 0.789s,  162.31/s  (0.790s,  162.03/s)  LR: 9.343e-02  Data: 0.042 (0.044)
Train: 33 [ 250/391 ( 64%)]  Loss:  0.518255 (0.4197)  Time: 0.789s,  162.21/s  (0.790s,  162.05/s)  LR: 9.343e-02  Data: 0.043 (0.044)
Train: 33 [ 300/391 ( 77%)]  Loss:  0.442973 (0.4230)  Time: 0.787s,  162.58/s  (0.790s,  162.08/s)  LR: 9.343e-02  Data: 0.042 (0.044)
Train: 33 [ 350/391 ( 90%)]  Loss:  0.345813 (0.4134)  Time: 0.787s,  162.60/s  (0.790s,  162.09/s)  LR: 9.343e-02  Data: 0.042 (0.044)
Train: 33 [ 390/391 (100%)]  Loss:  0.477777 (0.4180)  Time: 0.509s,  157.32/s  (0.789s,  101.40/s)  LR: 9.343e-02  Data: 0.029 (0.044)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.022 (0.022)  Loss:  0.4700 (0.4700)  Acc@1: 85.0000 (85.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.4971 (0.4993)  Acc@1: 83.0000 (83.7451)  Acc@5: 98.0000 (99.2157)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.021)  Loss:  0.5894 (0.5009)  Acc@1: 82.0000 (83.3700)  Acc@5: 99.0000 (99.3300)
tb
loss  0 : 2.53380859375 33
loss  1 : 0.643055419921875 33
loss  2 : 1.281376953125 33
loss  3 : 0.86760009765625 33
{'loss_different_0': 2.53380859375, 'loss_different_1': 0.643055419921875, 'loss_different_2': 1.281376953125, 'loss_different_3': 0.86760009765625}
Top1:  83.37
Train: 34 [   0/391 (  0%)]  Loss:  0.353347 (0.3533)  Time: 0.791s,  161.84/s  (0.791s,  161.84/s)  LR: 9.304e-02  Data: 0.044 (0.044)
Train: 34 [  50/391 ( 13%)]  Loss:  0.374083 (0.3637)  Time: 0.789s,  162.15/s  (0.792s,  161.60/s)  LR: 9.304e-02  Data: 0.043 (0.043)
Train: 34 [ 100/391 ( 26%)]  Loss:  0.261684 (0.3297)  Time: 0.790s,  162.13/s  (0.790s,  161.95/s)  LR: 9.304e-02  Data: 0.043 (0.043)
Train: 34 [ 150/391 ( 38%)]  Loss:  0.375584 (0.3412)  Time: 0.787s,  162.73/s  (0.790s,  162.12/s)  LR: 9.304e-02  Data: 0.041 (0.043)
Train: 34 [ 200/391 ( 51%)]  Loss:  0.451046 (0.3631)  Time: 0.787s,  162.68/s  (0.789s,  162.24/s)  LR: 9.304e-02  Data: 0.041 (0.042)
Train: 34 [ 250/391 ( 64%)]  Loss:  0.396748 (0.3687)  Time: 0.786s,  162.80/s  (0.789s,  162.30/s)  LR: 9.304e-02  Data: 0.040 (0.042)
Train: 34 [ 300/391 ( 77%)]  Loss:  0.366022 (0.3684)  Time: 0.789s,  162.20/s  (0.788s,  162.35/s)  LR: 9.304e-02  Data: 0.043 (0.042)
Train: 34 [ 350/391 ( 90%)]  Loss:  0.334747 (0.3642)  Time: 0.786s,  162.92/s  (0.788s,  162.37/s)  LR: 9.304e-02  Data: 0.040 (0.042)
Train: 34 [ 390/391 (100%)]  Loss:  0.331331 (0.3618)  Time: 0.508s,  157.36/s  (0.788s,  101.59/s)  LR: 9.304e-02  Data: 0.029 (0.042)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.5200 (0.5200)  Acc@1: 84.0000 (84.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.020)  Loss:  0.5327 (0.4881)  Acc@1: 81.0000 (84.4510)  Acc@5: 98.0000 (99.1569)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.019 (0.021)  Loss:  0.6328 (0.4848)  Acc@1: 78.0000 (84.3100)  Acc@5: 98.0000 (99.1900)
tb
loss  0 : 2.7578125 34
loss  1 : 0.6838671875 34
loss  2 : 1.3398486328125 34
loss  3 : 0.85776123046875 34
{'loss_different_0': 2.7578125, 'loss_different_1': 0.6838671875, 'loss_different_2': 1.3398486328125, 'loss_different_3': 0.85776123046875}
Top1:  84.31
Train: 35 [   0/391 (  0%)]  Loss:  0.240870 (0.2409)  Time: 0.789s,  162.32/s  (0.789s,  162.32/s)  LR: 9.263e-02  Data: 0.042 (0.042)
Train: 35 [  50/391 ( 13%)]  Loss:  0.269278 (0.2551)  Time: 0.787s,  162.65/s  (0.788s,  162.50/s)  LR: 9.263e-02  Data: 0.041 (0.042)
Train: 35 [ 100/391 ( 26%)]  Loss:  0.369750 (0.2933)  Time: 0.787s,  162.63/s  (0.788s,  162.49/s)  LR: 9.263e-02  Data: 0.040 (0.042)
Train: 35 [ 150/391 ( 38%)]  Loss:  0.369902 (0.3125)  Time: 0.786s,  162.87/s  (0.788s,  162.50/s)  LR: 9.263e-02  Data: 0.041 (0.042)
Train: 35 [ 200/391 ( 51%)]  Loss:  0.345027 (0.3190)  Time: 0.785s,  162.96/s  (0.789s,  162.31/s)  LR: 9.263e-02  Data: 0.041 (0.042)
Train: 35 [ 250/391 ( 64%)]  Loss:  0.354961 (0.3250)  Time: 0.787s,  162.65/s  (0.788s,  162.35/s)  LR: 9.263e-02  Data: 0.042 (0.042)
Train: 35 [ 300/391 ( 77%)]  Loss:  0.580237 (0.3614)  Time: 0.787s,  162.66/s  (0.788s,  162.39/s)  LR: 9.263e-02  Data: 0.041 (0.042)
Train: 35 [ 350/391 ( 90%)]  Loss:  0.392869 (0.3654)  Time: 0.787s,  162.73/s  (0.788s,  162.41/s)  LR: 9.263e-02  Data: 0.041 (0.041)
Train: 35 [ 390/391 (100%)]  Loss:  0.304775 (0.3610)  Time: 0.510s,  156.92/s  (0.787s,  101.60/s)  LR: 9.263e-02  Data: 0.031 (0.041)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.5488 (0.5488)  Acc@1: 79.0000 (79.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  0.5547 (0.5905)  Acc@1: 82.0000 (80.6078)  Acc@5: 100.0000 (98.9412)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5947 (0.5866)  Acc@1: 82.0000 (80.7800)  Acc@5: 100.0000 (99.0900)
tb
loss  0 : 2.489765625 35
loss  1 : 0.81469482421875 35
loss  2 : 1.332666015625 35
loss  3 : 1.0349267578125 35
{'loss_different_0': 2.489765625, 'loss_different_1': 0.81469482421875, 'loss_different_2': 1.332666015625, 'loss_different_3': 1.0349267578125}
Top1:  80.78
Train: 36 [   0/391 (  0%)]  Loss:  0.316424 (0.3164)  Time: 0.789s,  162.19/s  (0.789s,  162.19/s)  LR: 9.222e-02  Data: 0.043 (0.043)
Train: 36 [  50/391 ( 13%)]  Loss:  0.300802 (0.3086)  Time: 0.787s,  162.71/s  (0.788s,  162.50/s)  LR: 9.222e-02  Data: 0.041 (0.042)
Train: 36 [ 100/391 ( 26%)]  Loss:  0.303585 (0.3069)  Time: 0.792s,  161.70/s  (0.788s,  162.51/s)  LR: 9.222e-02  Data: 0.046 (0.042)
Train: 36 [ 150/391 ( 38%)]  Loss:  0.287179 (0.3020)  Time: 0.786s,  162.83/s  (0.788s,  162.53/s)  LR: 9.222e-02  Data: 0.041 (0.042)
Train: 36 [ 200/391 ( 51%)]  Loss:  0.386705 (0.3189)  Time: 0.787s,  162.66/s  (0.788s,  162.53/s)  LR: 9.222e-02  Data: 0.041 (0.042)
Train: 36 [ 250/391 ( 64%)]  Loss:  0.504110 (0.3498)  Time: 0.786s,  162.90/s  (0.787s,  162.54/s)  LR: 9.222e-02  Data: 0.040 (0.042)
Train: 36 [ 300/391 ( 77%)]  Loss:  0.358970 (0.3511)  Time: 0.787s,  162.59/s  (0.787s,  162.54/s)  LR: 9.222e-02  Data: 0.042 (0.042)
Train: 36 [ 350/391 ( 90%)]  Loss:  0.431353 (0.3611)  Time: 0.786s,  162.79/s  (0.787s,  162.55/s)  LR: 9.222e-02  Data: 0.040 (0.042)
Train: 36 [ 390/391 (100%)]  Loss:  0.339803 (0.3596)  Time: 0.515s,  155.28/s  (0.787s,  101.67/s)  LR: 9.222e-02  Data: 0.036 (0.042)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.5420 (0.5420)  Acc@1: 81.0000 (81.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  0.4539 (0.5897)  Acc@1: 84.0000 (81.4314)  Acc@5: 99.0000 (98.7451)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5850 (0.5803)  Acc@1: 82.0000 (81.4600)  Acc@5: 99.0000 (98.9500)
tb
loss  0 : 2.55673828125 36
loss  1 : 0.70154296875 36
loss  2 : 1.4611181640625 36
loss  3 : 1.073671875 36
{'loss_different_0': 2.55673828125, 'loss_different_1': 0.70154296875, 'loss_different_2': 1.4611181640625, 'loss_different_3': 1.073671875}
Top1:  81.46
Train: 37 [   0/391 (  0%)]  Loss:  0.405766 (0.4058)  Time: 0.791s,  161.77/s  (0.791s,  161.77/s)  LR: 9.179e-02  Data: 0.045 (0.045)
Train: 37 [  50/391 ( 13%)]  Loss:  0.400674 (0.4032)  Time: 0.787s,  162.55/s  (0.788s,  162.52/s)  LR: 9.179e-02  Data: 0.041 (0.041)
Train: 37 [ 100/391 ( 26%)]  Loss:  0.369944 (0.3921)  Time: 0.787s,  162.57/s  (0.788s,  162.53/s)  LR: 9.179e-02  Data: 0.042 (0.041)
Train: 37 [ 150/391 ( 38%)]  Loss:  0.280261 (0.3642)  Time: 0.787s,  162.63/s  (0.788s,  162.37/s)  LR: 9.179e-02  Data: 0.041 (0.041)
Train: 37 [ 200/391 ( 51%)]  Loss:  0.269129 (0.3452)  Time: 0.792s,  161.66/s  (0.788s,  162.43/s)  LR: 9.179e-02  Data: 0.046 (0.041)
Train: 37 [ 250/391 ( 64%)]  Loss:  0.422939 (0.3581)  Time: 0.787s,  162.63/s  (0.788s,  162.46/s)  LR: 9.179e-02  Data: 0.041 (0.041)
Train: 37 [ 300/391 ( 77%)]  Loss:  0.477036 (0.3751)  Time: 0.788s,  162.35/s  (0.788s,  162.44/s)  LR: 9.179e-02  Data: 0.043 (0.042)
Train: 37 [ 350/391 ( 90%)]  Loss:  0.418639 (0.3805)  Time: 0.789s,  162.23/s  (0.788s,  162.41/s)  LR: 9.179e-02  Data: 0.042 (0.042)
Train: 37 [ 390/391 (100%)]  Loss:  0.501696 (0.3893)  Time: 0.508s,  157.49/s  (0.788s,  101.59/s)  LR: 9.179e-02  Data: 0.029 (0.042)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.7534 (0.7534)  Acc@1: 77.0000 (77.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5537 (0.6151)  Acc@1: 78.0000 (80.6078)  Acc@5: 99.0000 (98.5686)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  0.6504 (0.6230)  Acc@1: 79.0000 (80.3400)  Acc@5: 99.0000 (98.6500)
tb
loss  0 : 2.4415234375 37
loss  1 : 0.7287158203125 37
loss  2 : 1.3116162109375 37
loss  3 : 1.71240234375 37
{'loss_different_0': 2.4415234375, 'loss_different_1': 0.7287158203125, 'loss_different_2': 1.3116162109375, 'loss_different_3': 1.71240234375}
Top1:  80.34
Train: 38 [   0/391 (  0%)]  Loss:  0.281310 (0.2813)  Time: 0.791s,  161.75/s  (0.791s,  161.75/s)  LR: 9.135e-02  Data: 0.045 (0.045)
Train: 38 [  50/391 ( 13%)]  Loss:  0.365957 (0.3236)  Time: 0.788s,  162.45/s  (0.789s,  162.26/s)  LR: 9.135e-02  Data: 0.041 (0.043)
Train: 38 [ 100/391 ( 26%)]  Loss:  0.336756 (0.3280)  Time: 0.790s,  162.03/s  (0.789s,  162.21/s)  LR: 9.135e-02  Data: 0.045 (0.043)
Train: 38 [ 150/391 ( 38%)]  Loss:  0.286577 (0.3176)  Time: 0.790s,  161.94/s  (0.789s,  162.23/s)  LR: 9.135e-02  Data: 0.045 (0.043)
Train: 38 [ 200/391 ( 51%)]  Loss:  0.343952 (0.3229)  Time: 0.788s,  162.36/s  (0.789s,  162.23/s)  LR: 9.135e-02  Data: 0.043 (0.043)
Train: 38 [ 250/391 ( 64%)]  Loss:  0.356463 (0.3285)  Time: 0.788s,  162.41/s  (0.789s,  162.22/s)  LR: 9.135e-02  Data: 0.043 (0.043)
Train: 38 [ 300/391 ( 77%)]  Loss:  0.440285 (0.3445)  Time: 0.790s,  162.04/s  (0.789s,  162.21/s)  LR: 9.135e-02  Data: 0.044 (0.043)
Train: 38 [ 350/391 ( 90%)]  Loss:  0.351924 (0.3454)  Time: 0.800s,  159.99/s  (0.790s,  162.07/s)  LR: 9.135e-02  Data: 0.053 (0.043)
Train: 38 [ 390/391 (100%)]  Loss:  0.620892 (0.3654)  Time: 0.508s,  157.36/s  (0.789s,  101.40/s)  LR: 9.135e-02  Data: 0.029 (0.043)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.5210 (0.5210)  Acc@1: 82.0000 (82.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.4238 (0.4904)  Acc@1: 88.0000 (84.4314)  Acc@5: 98.0000 (99.2745)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.4929 (0.5003)  Acc@1: 83.0000 (83.7100)  Acc@5: 100.0000 (99.3000)
tb
loss  0 : 2.46853515625 38
loss  1 : 0.67833251953125 38
loss  2 : 1.265986328125 38
loss  3 : 0.833759765625 38
{'loss_different_0': 2.46853515625, 'loss_different_1': 0.67833251953125, 'loss_different_2': 1.265986328125, 'loss_different_3': 0.833759765625}
Top1:  83.71
Train: 39 [   0/391 (  0%)]  Loss:  0.494437 (0.4944)  Time: 0.794s,  161.30/s  (0.794s,  161.30/s)  LR: 9.091e-02  Data: 0.047 (0.047)
Train: 39 [  50/391 ( 13%)]  Loss:  0.282427 (0.3884)  Time: 0.790s,  161.98/s  (0.789s,  162.31/s)  LR: 9.091e-02  Data: 0.044 (0.043)
Train: 39 [ 100/391 ( 26%)]  Loss:  0.513047 (0.4300)  Time: 0.789s,  162.31/s  (0.789s,  162.29/s)  LR: 9.091e-02  Data: 0.043 (0.043)
Train: 39 [ 150/391 ( 38%)]  Loss:  0.294633 (0.3961)  Time: 0.787s,  162.54/s  (0.789s,  162.28/s)  LR: 9.091e-02  Data: 0.042 (0.043)
Train: 39 [ 200/391 ( 51%)]  Loss:  0.326623 (0.3822)  Time: 0.789s,  162.18/s  (0.789s,  162.24/s)  LR: 9.091e-02  Data: 0.044 (0.043)
Train: 39 [ 250/391 ( 64%)]  Loss:  0.384419 (0.3826)  Time: 0.789s,  162.20/s  (0.789s,  162.26/s)  LR: 9.091e-02  Data: 0.043 (0.043)
Train: 39 [ 300/391 ( 77%)]  Loss:  0.439496 (0.3907)  Time: 0.788s,  162.34/s  (0.789s,  162.24/s)  LR: 9.091e-02  Data: 0.043 (0.043)
Train: 39 [ 350/391 ( 90%)]  Loss:  0.388921 (0.3905)  Time: 0.789s,  162.27/s  (0.789s,  162.25/s)  LR: 9.091e-02  Data: 0.043 (0.043)
Train: 39 [ 390/391 (100%)]  Loss:  0.425254 (0.3930)  Time: 0.511s,  156.70/s  (0.788s,  101.49/s)  LR: 9.091e-02  Data: 0.031 (0.043)
Test: [   0/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.6606 (0.6606)  Acc@1: 78.0000 (78.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.234 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.7197 (0.6279)  Acc@1: 82.0000 (80.0196)  Acc@5: 98.0000 (98.9608)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.021 (0.021)  Loss:  0.5317 (0.6149)  Acc@1: 80.0000 (80.3400)  Acc@5: 99.0000 (99.0600)
tb
loss  0 : 2.471484375 39
loss  1 : 0.773115234375 39
loss  2 : 1.4276220703125 39
loss  3 : 1.2072021484375 39
{'loss_different_0': 2.471484375, 'loss_different_1': 0.773115234375, 'loss_different_2': 1.4276220703125, 'loss_different_3': 1.2072021484375}
Top1:  80.34
Train: 40 [   0/391 (  0%)]  Loss:  0.286545 (0.2865)  Time: 0.791s,  161.79/s  (0.791s,  161.79/s)  LR: 9.045e-02  Data: 0.045 (0.045)
Train: 40 [  50/391 ( 13%)]  Loss:  0.374375 (0.3305)  Time: 0.787s,  162.55/s  (0.789s,  162.28/s)  LR: 9.045e-02  Data: 0.042 (0.043)
Train: 40 [ 100/391 ( 26%)]  Loss:  0.262345 (0.3078)  Time: 0.787s,  162.74/s  (0.789s,  162.29/s)  LR: 9.045e-02  Data: 0.041 (0.043)
Train: 40 [ 150/391 ( 38%)]  Loss:  0.499811 (0.3558)  Time: 0.789s,  162.31/s  (0.789s,  162.31/s)  LR: 9.045e-02  Data: 0.043 (0.043)
Train: 40 [ 200/391 ( 51%)]  Loss:  0.297442 (0.3441)  Time: 0.789s,  162.28/s  (0.789s,  162.33/s)  LR: 9.045e-02  Data: 0.043 (0.043)
Train: 40 [ 250/391 ( 64%)]  Loss:  0.310696 (0.3385)  Time: 0.788s,  162.43/s  (0.789s,  162.25/s)  LR: 9.045e-02  Data: 0.041 (0.043)
Train: 40 [ 300/391 ( 77%)]  Loss:  0.374367 (0.3437)  Time: 0.788s,  162.53/s  (0.789s,  162.26/s)  LR: 9.045e-02  Data: 0.042 (0.043)
Train: 40 [ 350/391 ( 90%)]  Loss:  0.547843 (0.3692)  Time: 0.792s,  161.56/s  (0.789s,  162.25/s)  LR: 9.045e-02  Data: 0.046 (0.043)
Train: 40 [ 390/391 (100%)]  Loss:  0.525911 (0.3805)  Time: 0.509s,  157.12/s  (0.788s,  101.46/s)  LR: 9.045e-02  Data: 0.030 (0.043)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.023 (0.023)  Loss:  0.6587 (0.6587)  Acc@1: 79.0000 (79.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.5869 (0.6939)  Acc@1: 85.0000 (77.8431)  Acc@5: 98.0000 (98.8431)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.7134 (0.6963)  Acc@1: 78.0000 (77.6900)  Acc@5: 98.0000 (98.9800)
tb
loss  0 : 2.474765625 40
loss  1 : 0.87430419921875 40
loss  2 : 1.488681640625 40
loss  3 : 1.2710546875 40
{'loss_different_0': 2.474765625, 'loss_different_1': 0.87430419921875, 'loss_different_2': 1.488681640625, 'loss_different_3': 1.2710546875}
Top1:  77.69
Train: 41 [   0/391 (  0%)]  Loss:  0.461728 (0.4617)  Time: 0.794s,  161.29/s  (0.794s,  161.29/s)  LR: 8.998e-02  Data: 0.047 (0.047)
Train: 41 [  50/391 ( 13%)]  Loss:  0.347205 (0.4045)  Time: 0.790s,  161.95/s  (0.791s,  161.87/s)  LR: 8.998e-02  Data: 0.044 (0.045)
Train: 41 [ 200/391 ( 51%)]  Loss:  0.398709 (0.3963)  Time: 0.788s,  162.53/s  (0.789s,  162.13/s)  LR: 8.998e-02  Data: 0.042 (0.044)
Train: 41 [ 250/391 ( 64%)]  Loss:  0.288556 (0.3783)  Time: 0.790s,  162.02/s  (0.789s,  162.14/s)  LR: 8.998e-02  Data: 0.044 (0.044)
Train: 41 [ 300/391 ( 77%)]  Loss:  0.373936 (0.3777)  Time: 0.791s,  161.73/s  (0.790s,  162.13/s)  LR: 8.998e-02  Data: 0.046 (0.044)
Train: 41 [ 350/391 ( 90%)]  Loss:  0.342855 (0.3733)  Time: 0.788s,  162.39/s  (0.789s,  162.16/s)  LR: 8.998e-02  Data: 0.042 (0.043)
Train: 41 [ 390/391 (100%)]  Loss:  0.277319 (0.3664)  Time: 0.508s,  157.43/s  (0.788s,  101.46/s)  LR: 8.998e-02  Data: 0.029 (0.043)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.4844 (0.4844)  Acc@1: 82.0000 (82.0000)  Acc@5: 98.0000 (98.0000)
Test: [  50/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.4819 (0.5534)  Acc@1: 87.0000 (82.6667)  Acc@5: 98.0000 (98.5490)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.020)  Loss:  0.4038 (0.5453)  Acc@1: 84.0000 (82.4500)  Acc@5: 100.0000 (98.7200)
tb
loss  0 : 2.49375 41
loss  1 : 0.7200244140625 41
loss  2 : 1.50947265625 41
loss  3 : 0.84885986328125 41
{'loss_different_0': 2.49375, 'loss_different_1': 0.7200244140625, 'loss_different_2': 1.50947265625, 'loss_different_3': 0.84885986328125}
Top1:  82.45
Train: 42 [   0/391 (  0%)]  Loss:  0.379879 (0.3799)  Time: 0.791s,  161.85/s  (0.791s,  161.85/s)  LR: 8.951e-02  Data: 0.045 (0.045)
Train: 42 [  50/391 ( 13%)]  Loss:  0.307772 (0.3438)  Time: 0.797s,  160.61/s  (0.792s,  161.60/s)  LR: 8.951e-02  Data: 0.051 (0.042)
Train: 42 [ 100/391 ( 26%)]  Loss:  0.288447 (0.3254)  Time: 0.787s,  162.72/s  (0.790s,  162.05/s)  LR: 8.951e-02  Data: 0.041 (0.042)
Train: 42 [ 150/391 ( 38%)]  Loss:  0.339397 (0.3289)  Time: 0.787s,  162.71/s  (0.789s,  162.22/s)  LR: 8.951e-02  Data: 0.040 (0.042)
Train: 42 [ 200/391 ( 51%)]  Loss:  0.385833 (0.3403)  Time: 0.789s,  162.13/s  (0.789s,  162.32/s)  LR: 8.951e-02  Data: 0.043 (0.042)
Train: 42 [ 250/391 ( 64%)]  Loss:  0.308640 (0.3350)  Time: 0.787s,  162.74/s  (0.788s,  162.38/s)  LR: 8.951e-02  Data: 0.040 (0.042)
Train: 42 [ 300/391 ( 77%)]  Loss:  0.351607 (0.3374)  Time: 0.787s,  162.71/s  (0.788s,  162.39/s)  LR: 8.951e-02  Data: 0.041 (0.042)
Train: 42 [ 350/391 ( 90%)]  Loss:  0.368527 (0.3413)  Time: 0.788s,  162.48/s  (0.788s,  162.41/s)  LR: 8.951e-02  Data: 0.041 (0.042)
Train: 42 [ 390/391 (100%)]  Loss:  0.442512 (0.3486)  Time: 0.508s,  157.37/s  (0.787s,  101.60/s)  LR: 8.951e-02  Data: 0.029 (0.042)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.4695 (0.4695)  Acc@1: 85.0000 (85.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.240 (0.233)  DataTime: 0.028 (0.021)  Loss:  0.3943 (0.4825)  Acc@1: 88.0000 (84.4510)  Acc@5: 100.0000 (99.1176)
Test: [  99/99]  Time: 0.232 (0.233)  DataTime: 0.020 (0.021)  Loss:  0.5396 (0.4835)  Acc@1: 79.0000 (84.2300)  Acc@5: 99.0000 (99.1500)
tb
loss  0 : 2.43751953125 42
loss  1 : 0.61427490234375 42
loss  2 : 1.198193359375 42
loss  3 : 0.90352783203125 42
{'loss_different_0': 2.43751953125, 'loss_different_1': 0.61427490234375, 'loss_different_2': 1.198193359375, 'loss_different_3': 0.90352783203125}
Top1:  84.23
Train: 43 [   0/391 (  0%)]  Loss:  0.405460 (0.4055)  Time: 0.790s,  162.09/s  (0.790s,  162.09/s)  LR: 8.902e-02  Data: 0.044 (0.044)
Train: 43 [  50/391 ( 13%)]  Loss:  0.462262 (0.4339)  Time: 0.788s,  162.34/s  (0.788s,  162.41/s)  LR: 8.902e-02  Data: 0.042 (0.042)
Train: 43 [ 100/391 ( 26%)]  Loss:  0.439284 (0.4357)  Time: 0.788s,  162.50/s  (0.788s,  162.50/s)  LR: 8.902e-02  Data: 0.042 (0.042)
Train: 43 [ 150/391 ( 38%)]  Loss:  0.317160 (0.4060)  Time: 0.792s,  161.56/s  (0.788s,  162.50/s)  LR: 8.902e-02  Data: 0.046 (0.042)
Train: 43 [ 200/391 ( 51%)]  Loss:  0.295251 (0.3839)  Time: 0.794s,  161.29/s  (0.788s,  162.44/s)  LR: 8.902e-02  Data: 0.048 (0.042)
Train: 43 [ 250/391 ( 64%)]  Loss:  0.387973 (0.3846)  Time: 0.789s,  162.23/s  (0.788s,  162.44/s)  LR: 8.902e-02  Data: 0.042 (0.042)
Train: 43 [ 300/391 ( 77%)]  Loss:  0.361299 (0.3812)  Time: 0.790s,  162.12/s  (0.788s,  162.42/s)  LR: 8.902e-02  Data: 0.043 (0.042)
Train: 43 [ 350/391 ( 90%)]  Loss:  0.377472 (0.3808)  Time: 0.787s,  162.55/s  (0.788s,  162.41/s)  LR: 8.902e-02  Data: 0.041 (0.042)
Train: 43 [ 390/391 (100%)]  Loss:  0.455707 (0.3862)  Time: 0.507s,  157.68/s  (0.788s,  101.58/s)  LR: 8.902e-02  Data: 0.028 (0.042)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.5469 (0.5469)  Acc@1: 79.0000 (79.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.231 (0.231)  DataTime: 0.019 (0.020)  Loss:  0.4211 (0.4833)  Acc@1: 85.0000 (84.2549)  Acc@5: 99.0000 (99.2157)
Test: [  99/99]  Time: 0.231 (0.232)  DataTime: 0.019 (0.020)  Loss:  0.4478 (0.4854)  Acc@1: 86.0000 (84.1600)  Acc@5: 100.0000 (99.3200)
tb
loss  0 : 2.47498046875 43
loss  1 : 0.712197265625 43
loss  2 : 1.2539453125 43
loss  3 : 0.889892578125 43
{'loss_different_0': 2.47498046875, 'loss_different_1': 0.712197265625, 'loss_different_2': 1.2539453125, 'loss_different_3': 0.889892578125}
Top1:  84.16
Train: 44 [   0/391 (  0%)]  Loss:  0.425358 (0.4254)  Time: 0.789s,  162.18/s  (0.789s,  162.18/s)  LR: 8.853e-02  Data: 0.043 (0.043)
Train: 44 [  50/391 ( 13%)]  Loss:  0.255964 (0.3407)  Time: 0.786s,  162.84/s  (0.788s,  162.42/s)  LR: 8.853e-02  Data: 0.040 (0.042)
Train: 44 [ 200/391 ( 51%)]  Loss:  0.482606 (0.3769)  Time: 0.786s,  162.80/s  (0.788s,  162.44/s)  LR: 8.853e-02  Data: 0.041 (0.042)
Train: 44 [ 250/391 ( 64%)]  Loss:  0.370982 (0.3759)  Time: 0.787s,  162.75/s  (0.788s,  162.42/s)  LR: 8.853e-02  Data: 0.040 (0.042)
Train: 44 [ 300/391 ( 77%)]  Loss:  0.406025 (0.3802)  Time: 0.785s,  162.95/s  (0.788s,  162.46/s)  LR: 8.853e-02  Data: 0.040 (0.042)
Train: 44 [ 350/391 ( 90%)]  Loss:  0.365406 (0.3784)  Time: 0.788s,  162.37/s  (0.788s,  162.47/s)  LR: 8.853e-02  Data: 0.043 (0.042)
Train: 44 [ 390/391 (100%)]  Loss:  0.417473 (0.3812)  Time: 0.512s,  156.25/s  (0.787s,  101.61/s)  LR: 8.853e-02  Data: 0.032 (0.042)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.5518 (0.5518)  Acc@1: 79.0000 (79.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.6270 (0.6221)  Acc@1: 84.0000 (80.6471)  Acc@5: 99.0000 (98.7647)
Test: [  99/99]  Time: 0.233 (0.233)  DataTime: 0.020 (0.021)  Loss:  0.6445 (0.6287)  Acc@1: 76.0000 (80.2800)  Acc@5: 98.0000 (98.8900)
tb
loss  0 : 2.49013671875 44
loss  1 : 0.86509521484375 44
loss  2 : 1.62775390625 44
loss  3 : 1.0545458984375 44
{'loss_different_0': 2.49013671875, 'loss_different_1': 0.86509521484375, 'loss_different_2': 1.62775390625, 'loss_different_3': 1.0545458984375}
Top1:  80.28
Train: 45 [   0/391 (  0%)]  Loss:  0.574488 (0.5745)  Time: 0.790s,  162.10/s  (0.790s,  162.10/s)  LR: 8.802e-02  Data: 0.044 (0.044)
Train: 45 [  50/391 ( 13%)]  Loss:  0.398045 (0.4863)  Time: 0.787s,  162.69/s  (0.788s,  162.36/s)  LR: 8.802e-02  Data: 0.041 (0.042)
Train: 45 [ 100/391 ( 26%)]  Loss:  0.260153 (0.4109)  Time: 0.786s,  162.82/s  (0.788s,  162.40/s)  LR: 8.802e-02  Data: 0.040 (0.042)
Train: 45 [ 150/391 ( 38%)]  Loss:  0.465378 (0.4245)  Time: 0.788s,  162.34/s  (0.788s,  162.41/s)  LR: 8.802e-02  Data: 0.043 (0.042)
Train: 45 [ 200/391 ( 51%)]  Loss:  0.321920 (0.4040)  Time: 0.787s,  162.68/s  (0.788s,  162.44/s)  LR: 8.802e-02  Data: 0.041 (0.042)
Train: 45 [ 250/391 ( 64%)]  Loss:  0.333056 (0.3922)  Time: 0.785s,  162.99/s  (0.788s,  162.46/s)  LR: 8.802e-02  Data: 0.040 (0.042)
Train: 45 [ 300/391 ( 77%)]  Loss:  0.380212 (0.3905)  Time: 0.787s,  162.73/s  (0.788s,  162.40/s)  LR: 8.802e-02  Data: 0.042 (0.042)
Train: 45 [ 350/391 ( 90%)]  Loss:  0.350320 (0.3854)  Time: 0.797s,  160.56/s  (0.788s,  162.42/s)  LR: 8.802e-02  Data: 0.052 (0.042)
Train: 45 [ 390/391 (100%)]  Loss:  0.360240 (0.3836)  Time: 0.507s,  157.80/s  (0.787s,  101.62/s)  LR: 8.802e-02  Data: 0.028 (0.042)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.4424 (0.4424)  Acc@1: 85.0000 (85.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.3701 (0.4879)  Acc@1: 88.0000 (84.2745)  Acc@5: 100.0000 (99.1373)
Test: [  99/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.4443 (0.4867)  Acc@1: 86.0000 (84.2000)  Acc@5: 100.0000 (99.1900)
tb
loss  0 : 2.48576171875 45
loss  1 : 0.6472802734375 45
loss  2 : 1.0638134765625 45
loss  3 : 1.0082666015625 45
{'loss_different_0': 2.48576171875, 'loss_different_1': 0.6472802734375, 'loss_different_2': 1.0638134765625, 'loss_different_3': 1.0082666015625}
Top1:  84.2
Train: 46 [   0/391 (  0%)]  Loss:  0.246980 (0.2470)  Time: 0.790s,  161.95/s  (0.790s,  161.95/s)  LR: 8.751e-02  Data: 0.044 (0.044)
Train: 46 [  50/391 ( 13%)]  Loss:  0.245870 (0.2464)  Time: 0.785s,  163.01/s  (0.787s,  162.65/s)  LR: 8.751e-02  Data: 0.040 (0.041)
Train: 46 [ 100/391 ( 26%)]  Loss:  0.232341 (0.2417)  Time: 0.787s,  162.56/s  (0.787s,  162.57/s)  LR: 8.751e-02  Data: 0.041 (0.042)
Train: 46 [ 150/391 ( 38%)]  Loss:  0.253392 (0.2446)  Time: 0.787s,  162.56/s  (0.787s,  162.61/s)  LR: 8.751e-02  Data: 0.041 (0.041)
Train: 46 [ 200/391 ( 51%)]  Loss:  0.334010 (0.2625)  Time: 0.789s,  162.27/s  (0.787s,  162.60/s)  LR: 8.751e-02  Data: 0.043 (0.041)
Train: 46 [ 250/391 ( 64%)]  Loss:  0.483473 (0.2993)  Time: 0.787s,  162.64/s  (0.787s,  162.59/s)  LR: 8.751e-02  Data: 0.041 (0.041)
Train: 46 [ 300/391 ( 77%)]  Loss:  0.463919 (0.3229)  Time: 0.787s,  162.69/s  (0.787s,  162.62/s)  LR: 8.751e-02  Data: 0.041 (0.041)
Train: 46 [ 350/391 ( 90%)]  Loss:  0.326555 (0.3233)  Time: 0.787s,  162.58/s  (0.787s,  162.62/s)  LR: 8.751e-02  Data: 0.042 (0.041)
Train: 46 [ 390/391 (100%)]  Loss:  0.427833 (0.3309)  Time: 0.507s,  157.70/s  (0.786s,  101.73/s)  LR: 8.751e-02  Data: 0.028 (0.041)
Test: [   0/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.4834 (0.4834)  Acc@1: 84.0000 (84.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.233 (0.232)  DataTime: 0.021 (0.020)  Loss:  0.3850 (0.4476)  Acc@1: 86.0000 (85.3922)  Acc@5: 99.0000 (99.1176)
Test: [  99/99]  Time: 0.232 (0.232)  DataTime: 0.020 (0.020)  Loss:  0.4897 (0.4500)  Acc@1: 82.0000 (85.1000)  Acc@5: 100.0000 (99.2500)
tb
loss  0 : 2.4872265625 46
loss  1 : 0.6253076171875 46
loss  2 : 1.2073876953125 46
loss  3 : 0.76126953125 46
{'loss_different_0': 2.4872265625, 'loss_different_1': 0.6253076171875, 'loss_different_2': 1.2073876953125, 'loss_different_3': 0.76126953125}
Top1:  85.1
Train: 47 [   0/391 (  0%)]  Loss:  0.328198 (0.3282)  Time: 0.789s,  162.26/s  (0.789s,  162.26/s)  LR: 8.698e-02  Data: 0.043 (0.043)
Train: 47 [  50/391 ( 13%)]  Loss:  0.308210 (0.3182)  Time: 0.790s,  162.10/s  (0.787s,  162.63/s)  LR: 8.698e-02  Data: 0.043 (0.041)
Train: 47 [ 100/391 ( 26%)]  Loss:  0.293883 (0.3101)  Time: 0.787s,  162.62/s  (0.790s,  162.07/s)  LR: 8.698e-02  Data: 0.042 (0.042)
Train: 47 [ 150/391 ( 38%)]  Loss:  0.297327 (0.3069)  Time: 0.788s,  162.45/s  (0.789s,  162.19/s)  LR: 8.698e-02  Data: 0.043 (0.042)
Train: 47 [ 200/391 ( 51%)]  Loss:  0.382523 (0.3220)  Time: 0.786s,  162.76/s  (0.789s,  162.24/s)  LR: 8.698e-02  Data: 0.041 (0.042)
Train: 47 [ 250/391 ( 64%)]  Loss:  0.367267 (0.3296)  Time: 0.789s,  162.24/s  (0.789s,  162.22/s)  LR: 8.698e-02  Data: 0.043 (0.042)
Train: 47 [ 300/391 ( 77%)]  Loss:  0.286058 (0.3234)  Time: 0.789s,  162.15/s  (0.789s,  162.20/s)  LR: 8.698e-02  Data: 0.043 (0.042)
Train: 47 [ 350/391 ( 90%)]  Loss:  0.333435 (0.3246)  Time: 0.790s,  162.10/s  (0.789s,  162.18/s)  LR: 8.698e-02  Data: 0.043 (0.043)
Train: 47 [ 390/391 (100%)]  Loss:  0.279557 (0.3213)  Time: 0.512s,  156.40/s  (0.789s,  101.45/s)  LR: 8.698e-02  Data: 0.032 (0.043)
Test: [   0/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.5356 (0.5356)  Acc@1: 83.0000 (83.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.4165 (0.5240)  Acc@1: 87.0000 (83.0784)  Acc@5: 100.0000 (99.3333)
Test: [  99/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.3936 (0.5317)  Acc@1: 89.0000 (82.9800)  Acc@5: 100.0000 (99.2200)
tb
loss  0 : 2.43240234375 47
loss  1 : 0.69119140625 47
loss  2 : 1.272490234375 47
loss  3 : 1.0050439453125 47
{'loss_different_0': 2.43240234375, 'loss_different_1': 0.69119140625, 'loss_different_2': 1.272490234375, 'loss_different_3': 1.0050439453125}
Top1:  82.98
Train: 48 [   0/391 (  0%)]  Loss:  0.407808 (0.4078)  Time: 0.793s,  161.37/s  (0.793s,  161.37/s)  LR: 8.645e-02  Data: 0.047 (0.047)
Train: 48 [  50/391 ( 13%)]  Loss:  0.304425 (0.3561)  Time: 0.790s,  161.95/s  (0.791s,  161.84/s)  LR: 8.645e-02  Data: 0.044 (0.045)
Train: 48 [ 100/391 ( 26%)]  Loss:  0.266573 (0.3263)  Time: 0.790s,  162.11/s  (0.791s,  161.86/s)  LR: 8.645e-02  Data: 0.043 (0.045)
Train: 48 [ 150/391 ( 38%)]  Loss:  0.375800 (0.3387)  Time: 0.790s,  162.09/s  (0.791s,  161.89/s)  LR: 8.645e-02  Data: 0.043 (0.045)
Train: 48 [ 200/391 ( 51%)]  Loss:  0.371084 (0.3451)  Time: 0.788s,  162.34/s  (0.790s,  161.94/s)  LR: 8.645e-02  Data: 0.043 (0.044)
Train: 48 [ 250/391 ( 64%)]  Loss:  0.449167 (0.3625)  Time: 0.789s,  162.26/s  (0.790s,  161.94/s)  LR: 8.645e-02  Data: 0.042 (0.044)
Train: 48 [ 300/391 ( 77%)]  Loss:  0.587160 (0.3946)  Time: 0.790s,  162.01/s  (0.790s,  161.94/s)  LR: 8.645e-02  Data: 0.044 (0.044)
Train: 48 [ 350/391 ( 90%)]  Loss:  0.236059 (0.3748)  Time: 0.790s,  161.94/s  (0.791s,  161.91/s)  LR: 8.645e-02  Data: 0.043 (0.044)
Train: 48 [ 390/391 (100%)]  Loss:  0.382953 (0.3754)  Time: 0.511s,  156.56/s  (0.790s,  101.28/s)  LR: 8.645e-02  Data: 0.031 (0.044)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.023 (0.023)  Loss:  0.3149 (0.3149)  Acc@1: 88.0000 (88.0000)  Acc@5: 99.0000 (99.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.022 (0.022)  Loss:  0.4412 (0.4598)  Acc@1: 87.0000 (84.2745)  Acc@5: 99.0000 (99.2745)
Test: [  99/99]  Time: 0.237 (0.234)  DataTime: 0.025 (0.022)  Loss:  0.4124 (0.4555)  Acc@1: 85.0000 (84.3700)  Acc@5: 100.0000 (99.4400)
tb
loss  0 : 2.45595703125 48
loss  1 : 0.64828369140625 48
loss  2 : 1.1481494140625 48
loss  3 : 0.77439208984375 48
{'loss_different_0': 2.45595703125, 'loss_different_1': 0.64828369140625, 'loss_different_2': 1.1481494140625, 'loss_different_3': 0.77439208984375}
Top1:  84.37
Train: 49 [   0/391 (  0%)]  Loss:  0.253599 (0.2536)  Time: 0.793s,  161.50/s  (0.793s,  161.50/s)  LR: 8.591e-02  Data: 0.046 (0.046)
Train: 49 [  50/391 ( 13%)]  Loss:  0.253369 (0.2535)  Time: 0.799s,  160.13/s  (0.795s,  161.01/s)  LR: 8.591e-02  Data: 0.052 (0.045)
Train: 49 [ 100/391 ( 26%)]  Loss:  0.300775 (0.2692)  Time: 0.791s,  161.82/s  (0.793s,  161.38/s)  LR: 8.591e-02  Data: 0.045 (0.045)
Train: 49 [ 150/391 ( 38%)]  Loss:  0.451536 (0.3148)  Time: 0.790s,  162.12/s  (0.793s,  161.46/s)  LR: 8.591e-02  Data: 0.044 (0.045)
Train: 49 [ 200/391 ( 51%)]  Loss:  0.249224 (0.3017)  Time: 0.790s,  162.00/s  (0.793s,  161.51/s)  LR: 8.591e-02  Data: 0.044 (0.045)
Train: 49 [ 250/391 ( 64%)]  Loss:  0.320035 (0.3048)  Time: 0.794s,  161.27/s  (0.792s,  161.54/s)  LR: 8.591e-02  Data: 0.047 (0.045)
Train: 49 [ 300/391 ( 77%)]  Loss:  0.403880 (0.3189)  Time: 0.794s,  161.21/s  (0.793s,  161.49/s)  LR: 8.591e-02  Data: 0.047 (0.045)
Train: 49 [ 350/391 ( 90%)]  Loss:  0.384787 (0.3272)  Time: 0.791s,  161.79/s  (0.793s,  161.50/s)  LR: 8.591e-02  Data: 0.045 (0.046)
Train: 49 [ 390/391 (100%)]  Loss:  0.133514 (0.3131)  Time: 0.511s,  156.53/s  (0.792s,  101.03/s)  LR: 8.591e-02  Data: 0.031 (0.045)
Test: [   0/99]  Time: 0.236 (0.236)  DataTime: 0.024 (0.024)  Loss:  0.5981 (0.5981)  Acc@1: 82.0000 (82.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.235 (0.235)  DataTime: 0.023 (0.023)  Loss:  0.4443 (0.5028)  Acc@1: 86.0000 (83.9804)  Acc@5: 99.0000 (99.0980)
Test: [  99/99]  Time: 0.235 (0.235)  DataTime: 0.023 (0.023)  Loss:  0.4460 (0.4911)  Acc@1: 88.0000 (84.1600)  Acc@5: 99.0000 (99.2700)
tb
loss  0 : 2.46333984375 49
loss  1 : 0.6043310546875 49
loss  2 : 1.2264111328125 49
loss  3 : 1.011259765625 49
{'loss_different_0': 2.46333984375, 'loss_different_1': 0.6043310546875, 'loss_different_2': 1.2264111328125, 'loss_different_3': 1.011259765625}
Top1:  84.16
Train: 50 [   0/391 (  0%)]  Loss:  0.185785 (0.1858)  Time: 0.796s,  160.83/s  (0.796s,  160.83/s)  LR: 8.536e-02  Data: 0.049 (0.049)
Train: 50 [  50/391 ( 13%)]  Loss:  0.294434 (0.2401)  Time: 0.789s,  162.28/s  (0.791s,  161.80/s)  LR: 8.536e-02  Data: 0.043 (0.045)
Train: 50 [ 100/391 ( 26%)]  Loss:  0.368801 (0.2830)  Time: 0.789s,  162.26/s  (0.791s,  161.92/s)  LR: 8.536e-02  Data: 0.043 (0.044)
Train: 50 [ 150/391 ( 38%)]  Loss:  0.312974 (0.2905)  Time: 0.791s,  161.80/s  (0.790s,  161.97/s)  LR: 8.536e-02  Data: 0.045 (0.044)
Train: 50 [ 200/391 ( 51%)]  Loss:  0.315145 (0.2954)  Time: 0.788s,  162.52/s  (0.792s,  161.56/s)  LR: 8.536e-02  Data: 0.042 (0.045)
Train: 50 [ 250/391 ( 64%)]  Loss:  0.324447 (0.3003)  Time: 0.792s,  161.71/s  (0.792s,  161.60/s)  LR: 8.536e-02  Data: 0.045 (0.045)
Train: 50 [ 300/391 ( 77%)]  Loss:  0.377894 (0.3114)  Time: 0.792s,  161.63/s  (0.792s,  161.61/s)  LR: 8.536e-02  Data: 0.046 (0.045)
Train: 50 [ 350/391 ( 90%)]  Loss:  0.289462 (0.3086)  Time: 0.793s,  161.35/s  (0.792s,  161.64/s)  LR: 8.536e-02  Data: 0.048 (0.045)
Train: 50 [ 390/391 (100%)]  Loss:  0.222614 (0.3024)  Time: 0.511s,  156.48/s  (0.791s,  101.12/s)  LR: 8.536e-02  Data: 0.032 (0.045)
Test: [   0/99]  Time: 0.235 (0.235)  DataTime: 0.022 (0.022)  Loss:  0.4121 (0.4121)  Acc@1: 85.0000 (85.0000)  Acc@5: 100.0000 (100.0000)
Test: [  50/99]  Time: 0.234 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.3357 (0.4298)  Acc@1: 92.0000 (85.8431)  Acc@5: 100.0000 (99.3725)
Test: [  99/99]  Time: 0.233 (0.234)  DataTime: 0.021 (0.022)  Loss:  0.3091 (0.4302)  Acc@1: 89.0000 (85.8900)  Acc@5: 100.0000 (99.4400)
tb
loss  0 : 2.46515625 50
loss  1 : 0.59773193359375 50
loss  2 : 1.1028173828125 50
loss  3 : 0.877158203125 50
{'loss_different_0': 2.46515625, 'loss_different_1': 0.59773193359375, 'loss_different_2': 1.1028173828125, 'loss_different_3': 0.877158203125}
Top1:  85.89
Saving model to /home/guodong/runhua/cifar_finetune/spe_snn_finetune_cifar/output/train/20250828-064313-pretrain_cifar_10/pretrain_cifar_10_50.pt
Train: 51 [   0/391 (  0%)]  Loss:  0.235673 (0.2357)  Time: 0.796s,  160.75/s  (0.796s,  160.75/s)  LR: 8.480e-02  Data: 0.049 (0.049)
Train: 51 [  50/391 ( 13%)]  Loss:  0.432993 (0.3343)  Time: 0.791s,  161.81/s  (0.791s,  161.88/s)  LR: 8.480e-02  Data: 0.045 (0.045)
Train: 51 [ 100/391 ( 26%)]  Loss:  0.283455 (0.3174)  Time: 0.790s,  162.12/s  (0.791s,  161.90/s)  LR: 8.480e-02  Data: 0.044 (0.045)
Train: 51 [ 150/391 ( 38%)]  Loss:  0.374847 (0.3317)  Time: 0.790s,  162.09/s  (0.791s,  161.92/s)  LR: 8.480e-02  Data: 0.043 (0.044)
^C
Traceback (most recent call last):
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 233, in <module>
    main()
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 69, in main
    train_metrics = train_epoch(
                    ^^^^^^^^^^^^
  File "/kaggle/working/AML-Project/finetune_hyperparameter/finetune.py", line 127, in train_epoch
    loss_scaler(loss, optimizer)
  File "/kaggle/working/AML-Project/finetune_hyperparameter/utils/tools/utility.py", line 95, in __call__
    self._scaler.scale(loss).backward(create_graph=create_graph)
  File "/usr/local/lib/python3.11/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt